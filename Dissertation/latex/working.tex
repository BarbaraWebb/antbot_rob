
\documentclass[a4paper,12pt]{article}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{subcaption}
\geometry{margin=1in}
\graphicspath{ {Images/} }
\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TITLEPAGE
\pagenumbering{gobble}
\centering
\vspace*{5cm}
{\huge Developing AntBot: \par Visual Navigation based on the insect brain \par}
\vspace{1cm}
{\itshape Robert Mitchell}

\vspace{2cm}

{\large Master of Informatics \par}
{\large Informatics \par}
{\large School of Informatics \par}
{\large The University of Edinburgh \par}
\large \today \par

\vfill
Supervised by\par
Dr. Barbara Webb

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ACKNOWLEDGEMENTS
\pagenumbering{roman}
\centering
{\LARGE \textbf{Acknowledgements}}
\begin{flushleft}
 {\small
  I would like to take the opportunity to thank my supervisor, Dr. Barbara Webb, for her
  guidance, and invaluable insight on the subject matter. My gratitude also extends to
  Zhaoyu Zhang and Leonard Eberding, two of my predecessors on this project; both have been
  extremely useful in explaining the existing codebase and operations of the robot where they
  were not always clear. Finally I should like to thank my parents, for their unwavering support
  throughout my education; I could not have made it here without them. }
\end{flushleft}  

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ABSTRACT
\centering
{\LARGE\textbf{Abstract}}
\begin{flushleft}
  {\small \textit{The abstract will be written as one of the last parts of the report to allow
    an accurate summary of the project as a whole.}}
\end{flushleft}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% DECLARATION
\centering
{\LARGE\textbf{Declaration}}
\begin{flushleft}
  {\small
    I declare that this disseration was composed by myself, the work
    contained herein is my own except where explicitly stated otherwise
    in the text, and that this work has not been submitted for any other
    degree or professional qualification except as specified.
    \par 

    \textit{Robert Mitchell}}

\end{flushleft}

\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% CONTENTS


\tableofcontents

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% FIGURES
\listoffigures
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TABLES
\listoftables
\newpage
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BODY
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INTRODUCTION
\pagenumbering{arabic}

\raggedright
\section{ Introduction }
Desert ants (\textit{Cataglyphis velox}) have the
remarkable ability to navigate through complex natural environments, using only low-resolution
visual information and limited computational power. It is well documented that many species of
ant, and other hymenoptra are capable of very robust visual navigation; however, it is as yet
unclear how the insects perform this seemingly complex task with such little brainpower. In this
paper, we will focus on using and extending an existing model for visual navigation in ants using
the Mushroom Body circuit, an artificial neural network which emulates the Mushroom Body neuropils
in the ant brain. We will also discuss biologically plausible methods of visual Collision Avoidance
using Optical Flow. A robot (AntBot) has been constructed \cite{Eberding2016} to allow us a testing
platform on which to implement, and experiment with, the algorithms in the
\textit{Ant Navigational Toolkit} \cite{Wehner2009}.

\subsection{ Motivation }
Though we are able to observe and mimic algorithmically the visual navigational capabilities
of insects, we still do not understand the precise methods by which this process takes place. The
model we will look at was propsed by \textit{Ardin et al.} \cite{Ardin2016}, which takes
the Mushroom Body (whose function was thought to be primarily for olfactory learning), and shows
that this provides a plausible neural model for encoding visual memories.
\newline

The MB circuit has been implemented and tested on AntBot by Eberding and Zhang respectively, however
the existing MB circuit is fairly simple. It uses binary weightings for the connections between
the visual projection neurons and the Kenyon Cells, and a single boolean Extrinsic Neuron denoting
image recognition. A modification was made by Zhang, whereby eight ENs were used, one for each of
the cardinal directions in the Central Complex model. This will be discussed further in
\ref{MBBackground}. The reader should note that the Central Complex (CX) model is primarily used
to model the task of Path Integration and will not be discussed further (see \cite{Scimeca2017}).
\newline

We would also like to look at methods for collision avoidance (CA) which do not involve specialised
sensors such as a LIDAR or SONAR, the luxury of which, ants do not have. Models have been proposed
which use Optical Flow (OF) properties to determine whether or not a collision is imminent. These
models have been propsed both in purely robotic contexts \cite{Souhila2007},
and biological ones \cite{Low2005}. 
  
\subsection { Goals }
% EDIT: The reverse route may not be worth talking about, or perhaps integrated path integration?
% I'm not sure there is enough time for this

The project aims for the following experimental scenario to be possible: We want to send the robot
on a run through an obstacle course, allowing it to navigate however it chooses through the
environment. From here, we want the robot to be able to replicate this route using only visual
memories, which it should store on that initial run. Finally, we would like the robot to be able to 
navigate home following the reverse of this route. It should be noted that this final step is not
strictly accurate to the behaviour of the desert ant. As noted by \cite{Baddeley2012} in their
familiarity-driven study of ant route navigation, \textit{Wehner et al.} \cite{Wehner2006}
demonstrated that the remembered routes have a distinct polarity, so knowledge of a route from
nest to food, does not imply that the ant has knowledge of a route from food to nest. In this case,
we make the outward and homeward route the same.
\newline

The first stage of the project will focus upon obtaining a working collision avoidance system as a
pre-requisite to gathering the route information. This CA system should be based on visual
information readily available to AntBot with no additional/specialist sensors. For this paper, we
assume that CA is a low-level reactionary behaviour, in that, we do not use any further processing
of the detected motion; we react based on the immediate stimulus of the flow
field. We will look at two different optical flow techniques used to build CA systems. We will also
discuss the effects of using different types of flow field, how the different flow
techniques behave in the same situation, and different methods of response.
\newline

We then move to the Mushroom Body circuit; this model for visual navigation was demonstrated by
\cite{Ardin2016} in simulation. Previous iterations of this project which focussed on the Mushroom
Body circuit have struggled to achieve similar performance on a robot using the methods originally
presented by \textit{Ardin et al.}; certainly, no previous iteration
has dealt with a non-deterministic route through a cluttered environment. We aim to get the original
\textit{basic} model from \cite{Ardin2016} working on the robot in a robust fashion and hope to
establish a baseline agains which future iterations can work while modifying the model. We will use
a modified scanning behaviour to establish this baseline; ants have demonstrated use of scanning in
visual navigation but it is generally accepted that this
is not the primary method they use to determine a direction after having recognised a scene, rather,
this scanning behaviour only occurs in certain scenarios (e.g. when the ant becomes lost)
\cite{Kodzhabashev2015}. This will require debugging of the existing model and any interacting
factors; for example, \textit{Zhang} managed to achieve good results with the MB model using
klinokinesis as the route following behaviour while a scanning struggled to produce robust routes.
It should also be noted that the scanning behaviour performed poorly using a Perfect Memory model
(the highest performing model used by \textit{Ardin et al.}) which may suggest that the model is
not all that contributes to the accuracy of recapitulated routes.
\newline

Finally, we will report the results of the experiments and testing performed at different stages during, and
post development; we will compare these to relevant results from previous iterations of
this project. We will end with a conclusion of our findings and contributions to the project,
as well as discussing technical limitations and potential for future developments.
\newpage

\subsection { Results }
This work is based on work done previously by Leonard Eberding, Luca Scimeca, and Zhaoyu Zhang
\cite{Eberding2016, Scimeca2017, Zhang2017}.
\newline

Significant contributions:
\begin{enumerate}
  \item{An optical flow based system for Collision Avoidance,}
  \item{Results indicating the impracticality of a time-to-contact based system for Collision Avoidance,}
  \item{Implemented hardware upgrades to AntBot to make the platform more robust,}
  \item{Implemented a new type of scanning behaviour for AntBot,}
  \item{Successful replication of a route through a cluttered environment using Visual Navigation,}
  \item{Implemented a basic but extensible statistical logging utility on AntBot with a complementary parsing and plotting utility written in Python.} %AntBotStats
\end{enumerate}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% BACKGROUND
\section{ Background}
\subsection{ Optical Flow }
\textit{Image flow} is defined as being the 3D velocity vector of an object, projected onto a
2D image plane\cite{ODonovan2005}. Optical flow is an approximation of this, working from a series
of images to compute the projected velocity vector for a pixel. A single \textit{flow vector}
shows the displacement of a single pixel from one image to the next. A set of these flow vectors
creates a \textit{flow field}, a series of vectors which describe the motion in the complete image.
\newline

Broadly, there are two types of flow field: dense and sparse (also known as differential and
feature-based repectively\cite{Low2005}). A \textit{dense} flow field tracks
the motion of every pixel in the image. A \textit{sparse} flow field tracks the motion of a subset
of pixels in the image. The sparse field may track a uniform subset of pixels, such as a grid, or a
set of important points in the image, such as the prominent features in the image (object corners).
See \ref{sec:methods} for implementation.


\subsection{ Optical flow models for Collision Avoidance } \label{sec:ofca}
Collision avoidance is an important component in navigation. Ants
do not have dedicated sensory systems or the ability to create a visual 3D map of their environment
using stereoscopic vision or motion paralax. It has been demonstrated that optical flow is used by
honeybees in performing visual navigation \cite{Dittmar2010}, so it is not unreasonable to
think that bees and other hymenoptera may use this information for other purposes. Indeed, optical
flow has been shown to be a viable model for collision avoidance in \textit{Drosophila}
\cite{Stewart2010}. We visit two models in this paper which we will term the
\textit{time-to-collision} model and the \textit{filtering} model.

\subsubsection{ Time-to-Collision }
This model, proposed by \textit{Low and Wyeth}\cite{Low2005}, aims to replicate the CA capabilities
of larger animals such as birds and humans. The model relies on accurately computing the
\textit{time-to-collision} or \textit{time-to-contact} (TTC), which is the computed time until the
robot collides with an obstacle. The TTC is computed as follows:

\begin{equation}
  \label{eqn:lowttc}
TTC = \frac{d}{v}
\end{equation}

Where $d$ is the distance from a point object on a collision course with the robot and $v$ is
constant speed at which the robot and object are closing.
\newline
\textit{Low and Wyeth} then alter this equation by taking the direction of the velocity vector $v$
and the direction to a point on the object and computing the angle between these two vectors $\phi$.
Their TTC equation then becomes:

\begin{equation}
 TTC = \frac{cos\phi \times sin\phi}{\phi}
\end{equation}

The TTC is then used to generate range information (image depth), which can be used to generate an
appropriate reaction.
\newline

An alternative, yet equivalent, method for computing the TTC is given by \textit{Souhila and Karim}
wherein the time-to-contact is given in terms of the distance of all pixels from the \textit{focus of expansion}
(FOE); the point from which all flow vectors originate. \textit{Souhila and Karim} do
not give a method of explicitly computing the FOE of a flow frame
however, they do describe their method of estimating it. All flow vectors are in two dimensions with a
horizontal ($x$) and vertical ($y$) component. Focussing on the horizontal case; flow vectors to the
right of the FOE will have a positive $x$ component, and flow vectors to the left of the FOE will have a negative
$x$ component. They then tally the signs of the horizontal components of the flow vectors computing a
difference between the horizontal components on the left of the FOE, and the horizontal components on the
right of the FOE. Intuitively, at the point where this difference is minimised, the divergence of the
horizontal components is maximised; this gives us the horizontal component of the FOE. The vertical
component is computed in a similar manner.
\newline

Finally, they compute the TTC as:
\begin{equation}
TTC = \frac{\Delta_i}{|\vec{V}_t|} \dots
\end{equation}
Where $\Delta_i$ is the distance of a point $p_i = (x,y)$ from the FOE, and $|\vec{V}_t|$ is the
translational velocity of the camera computed from optical flow\cite{Souhila2007}.
\newline

This time-to-contact is then appropriately thresholded and a reaction is generated based on the
magnitudes of the vectors surrounding the FOE. A ``balance strategy'' is then used as the control law by which
the robot is directed through the environment; this strategy will be discussed in Section \ref{sec:methods}
as a modified version was used during development.


\subsubsection{ Filtering }
The filtering method asks the following question: Given my current motion, what visual changes do
I expect to see? Much of the following explanation was provided by \cite{Stewart2010}.
A model proposed by \textit{Stewart et al.} for CA in simulated fruit flies takes
advantage of the fact that expanding patterns will trigger an avoidance manouver away from the focus
of expansion\cite{Stewart2010}. Their model uses two offset flow filters (the \textit{expected}
flow). Each filter is constructed as a frontally centered expansion pattern with the same spatial
extent to either side of the expansion pole (the central vertical axis of the pattern). These filters
are then offset by $+3^{\circ}$ for the right and $-3^{\circ}$ for the left. The left and right
filters feed into leaky accumulators, and if the accumulator exceeds a given threshold, a saccade
in the opposite direction is triggered. The model from \cite{Stewart2010} is shown in Figure
\ref{fig:stewartca}. We discuss a slightly modified version of this model in section
\ref{sec:methods}.

\begin{figure}[h]
 
  \centering
  \includegraphics[width=0.9\textwidth]{Stewart2010CAModel}
  \caption{
    \label{fig:stewartca} The OF filter model: (Caption from \textit{Stewart et al.} Figure 7):
    Collision avoidance (CA). (A)The CA filters used in the
    model. Each covers ~105 deg. of azimuth but they are centred
    at $\pm 3$ deg. (elevation = 0 deg.). (B) Control diagram for collision
    avoidance. Only the half of the system that triggers rightward
    saccades is shown for clarity; the other half has an identical
    configuration. The dark blue box represents the blue wide-field
    filter in A. The reset operation also applies to the other half of
    the system, i.e. a saccade in one direction sets both
    accumulators to 0. Thresh., threshold; ang. vel., angular velocity.
  }
 
\end{figure}

\subsection{ The Mushroom Body for Visual Navigation } \label{MBBackground}
The Mushroom Body neuropils are structures present in the brains of all insects though they are
largest in the brains of hymenoptera. They are known to play a critical role in olfactory
learning, and have been thought to play a role in visual memory in hymenoptera since 1982
at the latest\cite{Mobbs1982}. In 2016 a Mushroom Body circuit was proposed by \textit{Ardin et al.}
to allow emulation of the structure in a simulated desert ant \cite{Ardin2016}. The simulated
ant's view is taken from 1cm above the ground and has a field of view of $296^{\circ}$ azimuth
by $76^{\circ}$ elevation. A ratio of $4^{\circ}/pixel$ is used to give a $19\times74$ pixel image.
This image is then downsampled to 10$\times$36 pixels to give a realistic resolution for ant vision.
A $1\times360$ vector is used for further processing.
\newline

The generalised MB circuit is a three layer neural network: The first layer consists of a set of
visual Projection Neurons (vPNs), these connect to the second layer of standard artificial neurons
referred to as Kenyon Cells (KCs), and finally these KCs connect to a set of Extrinsic Neurons (ENs).
The reader should note that any reference to the weight of a Kenyon Cell herein is specifically referring
to the weight of the connection between that KC and the Extrinsic Neuron; this abstraction is made for
ease of reference.
\newline

The model by \textit{Ardin et al.} (shown in Figure \ref{fig:ardinmb}) consisted of 350 vPNs
(one for each pixel in the downsampled image). In the second layer we have 20,000 KCs each of
which receives input from 10 randomly selected vPNs; each KC requires coincident input from multiple
vPNs to fire. Every KC is then connected to a single EN which sums the number of KCs which are
activated by the input image. The network is trained by providing a reward signal at regular
intervals. If KC activation coincides with a reward signal, the connection strength to the EN is
greatly reduced. The single EN simply gives a familiarity measure for the image seen. The agent
decides on its next action by scanning to find the direction of greatest familiarity.
\newline

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{Ardin2010MBModel}
  \caption{
    \label{fig:ardinmb} The Mushroom Body circuit: (Caption from \textit{Ardin et al.} Figure 2):
    Images (see Fig 1) activate the visual projection neurons (vPNs). Each Kenyon cell (KC) receives
    input from 10 (random) vPNs and exceeds firing threshold only for coincident activation from
    several vPNs, thus images are encoded as a sparse pattern of KC activation. All KCs converge on
    a single extrinsic neuron (EN) and if activation coincides with a reward signal, the connection
    strength is decreased. After training the EN output to previously rewarded (familiar) images is
    few or no spikes.
  }
\end{figure}

This version of the MB circuit has demonstrated the capacity to learn scene information, as well
as recapitulate routes by using the scanning technique \cite{Ardin2016}. In 2016, \textit{Eberding}
implemented the Willshaw Network (WN) on AntBot, which resembles the Mushroom Body neuropils (see \ref{sec:wn}); he
demonstrated that the network allowed the agent to perform visual navigation through a sparse testing
environment\cite{Eberding2016}. The agent navigates by scanning, computing unfamiliarity, and finally
choosing the direction of minimum unfamiliarity. In order to save on computational resources, the
KCs are binary rather than spiking (we hope to explore a spiking model in the future). While this does
generate a correct route, it is not as continuous as those performed by real ants.
\newline

In 2017, \textit{Zhang} implemented a route following strategy originally proposed by
\textit{Koszhabashev and Mangan} which employed klinokinesis in place of scanning. Klinokinesis
is one of two main forms of kinesis - the movement of an organism in response to stimulus - in which
the turning rate is proportional to stimulus intensity. In \cite{Zhang2017}, the
stimulus is given by the unfamiliarity metric generated by the MB model. The step size between each
turning point as well as the turning angle depends on the unfamiliarity of the current view. The same
algorithm from \cite{Kodzhabashev2015} is used to perform klinokinesis on the robot.
\newline

A separate model, also implemented by \textit{Zhang}, added seven extrinsic neurons. Each EN is then
used to represent one of eight directions relative to the robot's current heading; if we take 0$^\circ$
to be the robot's forward direction then the other seven directions correspond to $+45^\circ$,
$+90^\circ$, $+135^\circ$, $\pm180^\circ$, $-135^\circ$, $-90^\circ$, and $-45^\circ$. This model was implemented as a way of
combining the MB model for visual navigation and the CX model for path integration. While, path
integration and the CX model are not explored in this project, the model is still worth discussing,
as it may still be used to encode a desired response to specific stimuli.

% Should the Klinokinesis Algorithm be given here?

\subsection{The Willshaw Network} \label{sec:wn}
The Willshaw network is a specific style of neural network with an interesting background; capable of
instantaneous learning, high capacity, and importantly, resembling the structure of the Mushroom Body
inputs, storage layer, and outputs.
\newline

The idea behind the network stems from the effect of passing a pattern of light beams through two
patterned pinhole cards and a lens to create a pattern of refraction on a screen. The pattern output
on the screen is said to be a correlogram of the two patterns. Intuitively, the light pattern output
at $A$ can be passed through filter pattern $B$ to produce pattern $C$. This idea is then refined by
\textit{Willshaw et al.} into a simple, three-layer neural network. As an aside, though the terminology of artificial
neural networks is not used in the original paper, the model presented is designed to be interpreted as
a biological neural network, so interpretation as an ANN is a case of semantics. The three layers of the
Willshaw network are quite simple: a set of input neurons $N_B$ are represented as parallel horizontal
lines; a set of output neurons $N_A$ are represented as parallel vertical lines; and finally the storage
layer is a map of all of the intersections between the inputs and outputs. The storage layer is made up of
artificial synapses which are initially inactive (weight zero). The synapses are activated (weight set to one)
when the input pattern and output pattern activate the same synapse. More succinctly, synapse $c_{i,j}$ is set
to 1 when $a_i$ and $b_j$ are activated simultaneously.

\begin{figure}[h]
 
  \centering
  \includegraphics[width=0.5\textwidth]{Willshaw1969Model}
  \caption{
    \label{fig:willshawnet} The original network figure (Fig. 4) from \textit{Willshaw et al.} \cite{Willshaw1969}.
  }
 
\end{figure}

This style of network can be transformed into something which models the MB circuit presented by \cite{Ardin2016}.
The input neurons $N_b$ become the vPNs with multiple inputs going to each synapse. The synapses themselves become the
KCs, though the weighting convention is reversed with zero being active, and one being inactive. We reduce the set of
outputs $N_a$ to a single line which represents our reward signal. Coincidence of a pattern of KC activation and a reward
signal results in instant learning of that pattern by the learning rule set up in \cite{Willshaw1969},
just as in the MB circuit. Finally we must add the Extrinsic Neuron to
the model as there is no parallel in the original Willshaw Net. As before the EN simply sums the weights of the KCs activated
on presentation of an input pattern. Thus, the Willshaw net can be used to model the Mushroom Body circuit.
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% PLATFORM
\section{ Platform } \label{sec:platform}
In order to test the hypothesised models for ant navigation we use a simple robot autonomous - AntBot.
AntBot was originally developed by \textit{Eberding} in 2016; here we will discuss his design and
implementation, upon which we develop our algorithms.

\subsection{ Hardware }
AntBot's predecessor, Roboant, was originally designed by \textit{Kodzhabashev} \cite{Kodzhabashev2014}
as a compact Android robot. The robot required only four components: A sufficiently powerful Android
phone (A Google Nexus 5 was used) as the brain, the Zumo Robot shield by Pololu as the chassis,
an Arduino microcontroller to allow them to communicate, and finally a 360$^{\circ}$ camera attachment.
AntBot uses the same basic structure, however, a Dangu 5 Rover chassis is used as the base, and
therefore an alternate motor controller board had to be used.
\newline

The Android phone was chosen as the control module for the robot for a number of reasons. Firstly,
the hardware; a modern smartphone allows a compact, powerful platform on which to build the
software system as well as providing built in sensory systems and the libraries to use them (e.g. the
camera). The Google Nexus 5 is more than capable of running image processing software, analysing
optical flow patterns, and simulating the required artifical neural networks required for this project.
Using an Android platform also allows for modular software design (See section \ref{subsubsec:droid}).
In order to mimic the near 360$^{\circ}$ field of view (FOV) given by the ant's compound eyes, we use a
panoramic lense (the Kogeto Dot), which uses a convex mirror to give a full 360$^{\circ}$ FOV. This
lense is attached to the front camera and requires some pre-processing to retrieve the desired
$360\times40$ image. As with Roboant, the Android phone is connected to an Arduino using a serial
interface. Commands are sent from the phone to the Arduino which then executes the relevant commands
on the motor board to provide motion control.

\begin{figure}[t!]
  \centering
  \begin{minipage}[t!]{0.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{KogetoDot}
    \caption{The Kogeto Dot 360$^\circ$ panoramic lens.}
  \end{minipage}
  \hfill
  \begin{minipage}[t!]{0.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{PanoramicView}
    \caption{A sample of the view given by the lense before any processing.}
  \end{minipage}

\end{figure}

\subsection{ Software }
\subsubsection{ Android } \label{subsubsec:droid}
The architecture of the Anrdoid operating system is such that applications can (subject to certain
constraints) run in parallel while broadcasting important information to one another. This allows
for a modular, ROS-like\footnote{Robot Operating System - ROS: http://www.ros.org/}
system in which we can have a dedicated application for each navigational subsystem employed by AntBot.
\newline

In this fashion, \textit{Eberding} implemented an Android Application Network (AAN) consisting of
five applications:
\newline

\begin{enumerate}
\item{
    The \textit{AntEye} application - This application is the main application in the network and
    provides the user interface, along with all camera interaction and visual processing. To summarise;
    the visual processing system takes the 360$^{\circ}$ panoramic image from the camera, extracts
    the blue channel information, crops out the ring which contains the actual image and reshapes
    this into a $360\times40px$ image, and finally downsamples this to a $90\times10px$ image. This
    final image is then used by any application which requires visual information.
  }

\item{
    The \textit{Path Integration} application - This application is responsible for performing all
    tasks related to Path Integration (PI). PI is the process of computing displacement
    based on a series of consecutive moves. In \textit{Eberding's} original implementation this
    application did not perform PI, but was instead used as a utility application to record orientation
    and distance travelled. \textit{Scimeca} extended this application to implement PI, using both a
    mathematical and neural approach.
  }

\item{
    The \textit{Visual Navigation} application - Similar to the PI application, the VN application
    houses the necessary components for performing visual navigation tasks. \textit{Eberding}
    implemented both the Willshaw (Mushroom Body) network, and also a Perfect Memory (PM) module both
    of which were extended by \textit{Zhang}. There also exists a super-class for visual navigation
    algorithms.
    }

\item{
    The \textit{Combiner} application - This application is used to combine the output from the

    VN and PI applications in order to compute what action the robot should take. This application
    governs the movement of the robot based on the two primary navigational systems.
  }

\item{
    The \textit{Serial Communications} application - This application governs all communication from
    the Android phone to the Arduino and server-interface. Android forbids multiple applications from
    using a single serial or Wifi port, so this application was developed as an intermediary to allow
    the other applications in the network to communicate through a single application.
    }
\end{enumerate}


\textit{Eberding's} implementation included a server interface which was used to control the robot
remotely
using the phone's Wifi hotspot and Serial Communicatin App, however, this interface has not been used
since the original implementation. For more information, please see \cite{Eberding2016}. It should
also be noted that, due to work conducted during previous iterations of this project, it may not
be possible to follow this exact structure.


\subsubsection{Arduino}
The Arduino software can be split into two sections: the \textit{parser} and the \textit{executioner}.
The parser will receive commands from the serial port and convert them into a series of movement
commands. These movement commands are then sent to the motor board by the executioner. Encoder
information may be gathered by the Arduino and sent back to the phone for processing.
For this project,
the Arduino code has not been modified. We only required the use of two commands for this project;
\textit{go}, which allows the robot to move indefinitely at a set speed, and \textit{turn}, which
allows the robot to turn to a desired (relative) angle. The \textit{go} command also allows the
operator to specify a speed for the left and right sides allowing the robot to move in smooth arcs.
A full list of commands can be seen in Table \ref{tab:commands}.

\begin{center}
  \begin{table}
    \begin{tabular}{ | l | l | p{7cm} | }

      \hline
      \textbf{Command} & \textbf{Message} & \textbf{Action} \\ \hline
      
      Heartbeat & x \textit{seconds} n &   Feedback sent to verify a stable connection between the
                                           phone and the Arduino. A signal is sent every second with a
                                           timestamp and checked. \\ \hline

      Move & t 0 \textit{distance} n & Travel a set distance in metres. \\  \hline
      Turn & t 0 \textit{angle} m 0 n & Turn a set angle (in degrees). \\ \hline
      Turn and Move & t \textit{angle} m \textit{distance} n & Turn by a specified angle then move the
                                                               specified distance. \\ \hline
      Turn left & \textit{l} & Turn left indefinitely. \\ \hline
      Turn right & \textit{r} & Turn right indefinitely. \\ \hline
      Halt & \textit{h} & Stop any command in progress and stop the robot. \\ \hline
      Go & g \textit{leftSpeed} \textit{rightSpeed} n & Move indefinitely with specified left and
                                                        right speeds. \\ \hline
                               
      
      \hline
      


      
    \end{tabular}
    \caption{The available commands on the Arduino and the messages sent to invoke them. Those
      values which are changeable are shown in italics. The \textit{go} command was added by
      \textit{Scimeca}. This table was adapted from \cite{Eberding2016, Scimeca2017}.
    }
    \label{tab:commands}
  \end{table}
\end{center}

Previous works have mentioned a message of the format \textit{e e1 e2 e3 e4} n. This message was used
to send wheel encoder information from the Arduino to the phone, however, this message was only sent
during the execution of a particular function and it has since been removed from the Arduino code.
There are currently no utilities available to retrieve and reset encoder values on-demand from the
phone, though such utilities should not be difficult to implement.

\subsection{Modifications}
Some hardware modifications have been made to the robot. Upon the uptake of this project, AntBot
was powered by six 1.2V AA NiMH batteries wired in series (using a simple power pack). This power
pack was connected to the motor board (and subsequently the motors) by a pair of 9V connectors. The robot
had to be opened up in order to connect or disconnect the batteries from the motors. The batteries themselves
had to be extracted from the power pack and charged individually. These actions, repeated often by the current
project and predecessors had caused wear on the power pack and the internal wiring. The wires themselves
needed re-soldered multiple times. In order to have a more robust platform for testing, the on-board
power system was modified. The aim of the modification was simply to avoid opening up the robot at any cost.
As such a new power source which could be charged as required without removal was added, along with external
charging ports and a switch. The internal wiring was re-done such that power could either flow from the
external ports to the battery, or from the battery to the motors controlled by the external switch. The new
power source is a 9.6V NiMH power pack which can be charged without removal from the robot. Motor speeds
and systems which depended on them required re-tuning after completion of the modifications.
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% METHODS
\section{ Methods } \label{sec:methods}
\subsection{Optical flow models for Collision Avoidance}
\subsubsection{Time-to-Collision}
We attempted to follow the outline presented by \textit{Low and Wyeth} as, at first glance, it seems
a straightforward and neat solution. However, this method of computation for the TTC relies on the
assumption that the camera view only ever moves in the forward direction which is not appropriate
for AntBot (See Section \ref{sec:platform}). So we look instead at the paper by \textit{Souhila and Karim}
for an alternative; for this method, we must compute the \textit{focus of expansion} or FOE, however,
their method of computing the FOE again relied on forward motion of the camera and did not translate
well into AntBot. A more general method for computing the FOE is given by \cite{ODonovan2005}:

%Copy of description from old backround so I don't have to type out the maths again.
%\textit{Included for skeleton purposes.}
%However, this method of computation relies on the assumption that the robot only ever moves in the
%forward camera direction which is not appropriate for AntBot (See section \ref{sec:platform}).
%We must look for an alternate way to compute TTC. Fortuntely, such information is easily
%computable from the \textit{focus of expansion} (FOE), the point from which all flow vectors
%originate.
%\newline

%A general method for computing the FOE is given by \cite{ODonovan2005}:

\begin{equation}
  FOE = (A^TA)^{-1}A^T\mathbf{b}
\end{equation}

\begin{equation*}
  \begin{split}
 A = 
\begin{bmatrix}
  a_{00} & a_{01}\\
  \dots  & \dots \\
  a_{n0} &  a_{n1}
\end{bmatrix}
\qquad
\end{split}
\begin{split}
\mathbf{b} =
\begin{bmatrix}
  b_0 \\
  \dots \\
  b_n
\end{bmatrix}
\end{split}
\end{equation*}
\newline

Where, for each pixel $p_i = (x, y)$, the associated flow vector is given by $\mathbf{v} = (u, v)$.
We then set $a_{i0} = u, a_{i1} = v$ and finally $b_i = xv - yu$. The TTC can then be computed as:

\begin{equation}
  TTC = \frac{d}{v} = \frac{y}{\frac{\partial y}{\partial t}}
\end{equation}

Where $y$ is the vertical distance of some point $p = (x,y,z)$ from the
FOE, $\frac{\partial y}{\partial t}$ is the velocity of translational motion of $y$, and
$d$ and $v$ are as given in (\ref{eqn:lowttc}).
A full derivation is given by \textit{O'Donovan} from whom we have adapted this equation.
\newline

Finally, we can simplify this to the desired equation from \cite{Souhila2007}:
\begin{equation}
TTC = \frac{\Delta_i}{|\vec{V}_t|} \dots
\end{equation}
Where $\Delta_i$ is the distance of a point $p_i = (x,y)$ from the FOE, and $|\vec{V}_t|$ is the
translational velocity of the camera computed from optical flow\cite{Souhila2007}. 
\newline
This time-to-contact is then appropriately thresholded and a reaction is generated based on the
position of the focus of expansion which provided a simple method of replicating the balance strategy
used by \cite{Souhila2007} as the FOE will be drawn to one side based on the motion parallax observed.
Instability in the computation of these properties was dealt with by averaging the properties
across a series of frames to reduce potential noise.
\newline

The generated responses were to be halt on detection (for debugging purposes), and triggering of a
smooth turn away from the obstacle. This technique was designed to be used with a sparse optic flow
field, however, a version using a dense flow field was attempted. The computation across a dense
flow field was found to be impractical. 


\subsubsection{Filtering}
An optical flow filter was previously implemented by \textit{Scimeca} for the purpose of speed retrieval.
This same filter was re-purposed for use in collision avoidance; the creation of the filter (largely taken from
\cite{Scimeca2017}) is as follows:

An $Nx3$

\subsection{Debugging the Mushroom Body}


\subsection{AntBotStats}
\newpage

\section{ Results and Evalutation } \label{sec:results}
\textit{Included for skeleton purposes.}

\newpage

\section{ Discussion } \label{sec:discussion}
\textit{Included for skeleton purposes.} 

\newpage

\bibliographystyle{plain}
\bibliography{working}

\end{document}





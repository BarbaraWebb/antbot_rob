Notes file for dissertation 1st draft:
This file is mostly for content planning purposes. All formatting will be
handled during the write-up phase (hopefully in LaTeX)

Title Page:
            
      Developing AntBot: A mobile-phone powered autnomous robot based on
      the insect brain.
      -
      Visual Route Memory

      Robert Mitchell

      Master of Informatics
      Informatics
      School of Informatics
      The University of Edinburgh
      2018
      
	


Abstract:

Acknowledgements:
	- Dr. B. Webb
	- Zhaoyu Zhang
	- Leonard Eberding
	- Jan? 
	- Parents

Declaration:
	I declare that this disseration was composed by myself, the work
	contained herein is my own except where explicitly stated otherwise
	in the text, and that this work has not been submitted for any other
	degree or professional qualification except as specified.

	       Robert Mitchell

Table of Contents:
      - Self explanatory
      - Add a document structure foreword?
List of Figures:
      - Self explanatory
List of Tables:
      - Self explanatory
Section 1 - Introduction:

	{NOTE: need to briefly define Path Integration somewhere 

	Path Integration is the ability of the ant to determine its
	displacement from the nest using odometric and directional
	information, the ant uses the integrated path to learn the a straight
	route from the nest to a food source.}
	

	Cataglyphis, herein referred to as ants, or desert ants have the
	amazing ability to navigate through complex natural environments using
	only low-resolution visual information and minimal computational
	complexity. The "Ant Navigational Toolkit", proposed in 2009 by
	Wehner describes the two most prominant components in ant navigation:
	Path Integration, and Visual Navigation. In this paper, we focus on
	the latter alongside a biologically plausible method for performing
	collision avoidance. A robot, based on a mobile phone, has been
	constructed as a development platform in order to implement the
	algorithms in the Ant Navigational Toolkit.

	Cite: [Eberding, Wehner, and Scimeca]
	
	Section 1.1 - Motivation:
	
	Insects are remarkably good at using visual cues to perform navigation.
	A model for visual memory and learning has been proposed by Ardin et
	al. which shows that the Mushroom Body neuropils (traditionally
	considered a mechanism for purely olfactory learning) provides a
	plausible neural circuit for encoding visual memories.

	The Mushroom Body circuit has been implemented and tested on AntBot prior.
	However, the existing model is quite simple, using binary weightings for
	the neural connections, and a single boolean Extrinsic Neuron denoting
	whether, or not, an image was recognised. A method was implemented by
	Zhang in 2017 using eight ENs each of which denoted one of the eight
	cardinal directions (N, NE, E, SE, S, SW, W, NW). This will be discussed in
	section <<SECTION>>.

	We also look at biologically inspired methods of Collision Avoidance (CA)
	using optical flow techniques.

	============================================================================

	Section 1.2 - Project Goals:
	
	This paper shall discuss methods of improving upon the existing Mushroom
	Body model, as well as experiment with different methods of biologically
	inspired Collision Avoidance. The final goal being, to send the robot on
	a journey through an obstacle course and then see if it could reproduce the
	same route using visual memories.

	The first stage of the project focuses on the Collision Avoidance, as a
	pre-requisite to collecting our route information. The traditional methods
	of performing CA in robots tend to use SONAR or LIDAR sensors, however, ants
	do not posess these sensory capabilities so far as we know.
	It has been shown that Optical Flow (OF) provides a biologically feasible way
	of performing CA [Drosophila], and even	some navigational tasks [Honeybees].

	A few OF techniques are available which allow us to perform CA, and so, we will
	look at a selection. We will look at using the effects of using different types
	of flow field, and then different the different methods of detecting obstacles.
	In the context of this project this is assumed to be low level, reactionary
	behaviour (i.e. we do not use a neural model or perform any processing, rather
	we react based on immediate stimulus). 

	We will then begin to experiment with the Mushroom Body network, first
	establishing a measure of baseline performance by using the original "basic"
	network. The robustness of the network will be tested similarly to [Ardin],
	wherein the robot may deviate from the memorised route by so much that it can
	never find the path again. These instances of the robot getting "lost" will be
	counted as errors. After establishing a baseline for performance, we will
	compare and contrast different versions of the MB circuit to see if we
	can improve upon the performance presented by our predecessors while still
	remaining biologically feasible.

	Finally, we will report the results of the tests performed during the
	different stages of development, as well as comparing results with relevant
	results from previous iterations of this project. We will end with a conclusion
	noting contributions to the project, as well as discussing limitations and
	possible future developments.  

	1.3 Results:
	This work was based on work done previously by Leanord Matthias Eberding [Diss.],
	Luca Scimeca, and Zhaoyu Zhang.

	Significant contributions:
	1. An optical flow filtering system for Collision Avoidance
	2. Achieved an understanding of why an expansion based system would not work
	3. [Future] Hopefully, implemented a working visual navigation system and provided
	   results for different visual models.
	=================================================================================
Section 2 - Background:
	2.1 Optical flow models for Collision Avoidance:
	Ants do not have any dedicated collision sensory systems, nor are they believed to
	have the ability to form a 3D picture using stereoscopic vision (as, for example,
	humans do). In order to perform depth analysis on images, they must use some
	characteristics of a sequence of 2D images such as paralax or optical flow. Optical
	flow is the term used to describe the motion fields in a series of images; how a pixel,
	or object, has moved since the prior image. For this paper, we used both sparse,
	and dense optical flow. The former describes the motion of distinctive points (corners)
	through between two images, and the latter describes the motion of every pixel between
	two images. For a more detailed explanation of optical flow, and the different
	properties it can be used to compute, please see [OF PAPER].

   	2.1.1: Expansion:
	One model, proposed by Low and Wyeth, computed a number of different properties of
	the flow field, and then used these to determine whether or not an obstacle had been
	detected. The model relies on accurately computing the Focus of Expansion (FOE) of
	a sparse optical flow field; that is, the point from which all flow vectors originate.
	The Time to Contact (TTC), the time until collision, is then computed by using the
	variance of all relevant points in the image from the FOE. The reader should note
	that the method used here to compute the image properties is different to those used
	in [LOW/WYETH/OA1], instead we use the following methods to compute the image properties.

	The Focus of Expansion is computed as follows[FLOW]:

		     Focus of Expansion (2.1.1) drawn from Flow paper

        The Time to Contact is then computed by[OA2]:
		     Time to Contact (2.1.2) drawn from obstacle avoidance paper 2

	The TTC is then appropriately thresholded and response direction is determined by
	the current FOE position.

	This model, while proven to work for different experimental set-ups[OA1][OA2], and
	used to provide an explanation for collision avoidance in animals, is not particularly
	intuitive in a biological sense. The model also struggles to perform at all in the
	case of the ant due to the restricted image detail. A more biologically sound model
	follows.
		     
	2.1.2: Filtering:
	Here, we drew inspiration from [Drosophila paper], which used an optical flow filter
	to determine any immediate danger of collision and trigger action in a simulated
	fruit fly. Instead of computing properties from the flow field observed, we instead
	ask the question: If I make this move, what visual changes do I expect to see?
	Stewart, Baker, and Webb compute expected flow fields for the left and right sides
	of the fly's visual input. The actual input is then compared, or filtered, with the
	expected input to see how it differs. For the fly based model the expected fields
	and processing are as follows:

			Drosophila expected OF fields (Fig 2.1)

	We adapt this model in Section 4.
	
	2.2: Mushroom Body models for Visual Navigation:
	Ants are highly capable when it comes to visual navigation, especially considering
	their limited brainpower and the limited detail they can extract from a scene. Though
	low in detail, the eyes of the ant cover an impressive field of view, allowing it
	to see almost 280 degrees at any one time (upper estimate from [Ardin]).
	
	Previous papers have discussed various algorithmic models for performing the task of
	visual navigation. In this paper we focus more on the proposed neural model of
	the Mushroom Body circuit. The precise neural mechanisms used for visual route
	navigation are still unknown.
	

	
	

Section 3 - Platform:

Section 4 - Methods:

Section 5 - Results and Evaluation: //Conisder splitting into two sections

Section 6 - Conclusion and Discussion:

References:

Visual Navigation: 
[Ardin et al.] [Mushroom Body]
Ardin P, Peng F, Mangan M, Lagogiannis K, Webb B (2016) Using an Insect Mushroom Body Circuit to Encode Route Memory in Complex Natural Environments.
PLoS Comput Biol 12(2): e1004683. https://doi.org/10.1371/journal.pcbi.1004683

[Baddeley et al.] [Infomax]
Baddeley B, Graham P, Husbands P, Philippides A (2012) A Model of Ant Route Navigation Driven by Scene Familiarity. PLoS Comput Biol 8(1): e1002336. https://doi.org/10.1371/journal.pcbi.1002336

Optical Flow:
Kahlouche Souhila, Achour Karim (2007) Optical Flow Based Robot Obstacle Avoidance


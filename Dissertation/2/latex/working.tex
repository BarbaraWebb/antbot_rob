\documentclass[a4paper,11pt,twoside,openright]{article}

\usepackage[toc,page]{appendix}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage[section]{placeins}
\usepackage{array}
\usepackage[nottoc,numbib]{tocbibind}
\usepackage{multicol}
\usepackage{wrapfig}
%\usepackage{titlesec}

\geometry{margin=1in}
\graphicspath{ {Images/} }

\newcolumntype{?}[1]{!{\vrule width #1}}
\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}

\let\oldsection\section
\def\section{\cleardoublepage\oldsection}

%\setcounter{tocdepth}{4}

%\titleformat{\paragraph}
%{\normalfont\normalsize\bfseries}{\theparagraph}{1em}{}
%\titlespacing*{\paragraph}
%{0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TITLEPAGE                                                                   %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{gobble}
\centering
\vspace*{6cm}
        {\huge Developing AntBot: \par A navigational system inspired by \par the
          insect brain \par}
\vspace{1cm}
{\Large \textit{Robert E. F. Mitchell}}

\vspace{3cm}

{\large Master of Informatics \par}
{\large Informatics \par}
{\large School of Informatics \par}
{\large The University of Edinburgh \par}
\large \today \par

\vfill
Supervised by\par
Dr. Barbara Webb

\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ACKNOWLEDGEMENTS                                                            %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{roman}
\centering
{\LARGE \textbf{Acknowledgements}}
\begin{flushleft}
 {\small \textit{Pending . . .} }
\end{flushleft}

\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DECLARATION                                                                 %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
{\LARGE\textbf{Declaration}}
\begin{flushleft}
  {\small
    I declare that this dissertation was composed by myself, the work
    contained herein is my own except where explicitly stated otherwise
    in the text, and that this work has not been submitted for any other
    degree or professional qualification except as specified.
    \par

    \textit{Robert Mitchell}}

\end{flushleft}

\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT                                                                    %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\centering
{\LARGE\textbf{Abstract}}
\begin{flushleft}
  {\small \textit{Pending . . .}}
\end{flushleft}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CONTENTS                                                                    %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\tableofcontents
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FIGURES                                                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoffigures
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TABLES                                                                      %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\listoftables
\newpage
\thispagestyle{empty}
\mbox{}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCTION                                                                %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{arabic}

\raggedright

\section{ Introduction }
Navigation is a complex task. Determining a sequence of actions to reach a
known location based on a combination of sensory inputs requires a lot of
computational power. Desert ants, are capable of performing such a task
over comparitively huge distances with limited, low resolution sensory
information and remarkable efficiency. While the exact method by which
the ants perform this task is still unknown, a reasonably complete navigational
model can be constructed from existing physiologically plausible components,
which may mimic the insect behaviour.
\newline
\par

In this paper we introduce a theoretical combined model, the Extended
Central Complex (ECX) model for insect navigation. To be clear, there
is no (known) physiological basis for such a model; however, is not
biologically infeasible and may provide insight into the operation of
the real insect brain. The ECX model combines the tasks of Visual
Navigation, Path Integration, and Collision Avoidance; using, the
Mushroom Body Circuit (MB) \cite{Ardin2016}, the Central Complex model
(CX) \cite{Stone2017}, and Optical Flow Collision Avoidance (OFCA)
\cite{Mitchell2018, Stewart2010} for each task at a low level, then
combining their outputs to get a form of higher navigational processing
(similar to the weighted ``base model'' described in \cite{Webb2019}).
The ECX model is a modified Central Complex model, named simply to
ensure distinction between the two models. The individual components
are all biologically plausible and two of three are known to hava a
physiological basis. \cite{Ardin2016, Stone2017, Mitchell2018,
  Stewart2010, Julien2017}.
\newline
\par

This project primarily extends the work done in
\cite{Mitchell2018}. As such we continue using the AntBot platform; a
robot constructed for the express purpose of experimenting with the
algorithms in the \textit{Ant Navigational Toolkit}
\cite{Eberding2016, Wehner2009}. Through our experience in
\cite{Mitchell2018} and in constructing this work, we have concluded
that, while the AntBot could still be of use for specific tests, it is
generally no longer fit for purpose (see Discussion).

\subsection{ Motivation }
Currently, a full base model for insect navigation does not exist
\cite{Webb2019}. We here aim to take the abstraction presented by
\textit{Webb} and create a biologically plausible implementation using
our three-system approach. Both the MB and CX models have been
implemented and tested on AntBot previously \cite{Scimeca2017,
  Mitchell2018, Eberding2016, Zhang2017}, and a model combining the
two has also been constructed by \textit{Zhang} \cite{Zhang2017}.
This combined model is used as a basis for our own and will be
discussed further in Section \ref{CXMBBackground}.
\newline
\par

The previous AntBot implementations have demonstrated good performance
of the CX and MB models individually \cite{Scimeca2017,
  Mitchell2018}. Performance of a combined system has also been shown
to be reasonable, however, it is less consistent than we would desire
\cite{Zhang2017}. In the combined model tests from \cite{Zhang2017} we
note two key methodological problems: A fixed outbound route, and
fixed component weightings; the fixed outbound route was also present
in the CX experiments conducted by \textit{Scimeca}. We address the
former by adding the OFCA component to our model; as in
\cite{Mitchell2018}, the AntBot will follow a non-deterministic
outbound route through an arena with obejcts present (due to the level
of implementation achieved these objects will be placed
strategically). The latter methodilogical brings up the
more complicated question of plausible synaptic plasticity which,
while undoubtedly interesting, lies outside the scope of this project
(though we will discuss the idea to a small extent). It is worth
noting that here may also have been unknown tecnical issues with the
robot which affected the results of \cite{Zhang2017}, making them less
consistent than they should be in reality (see \cite{Mitchell2018} and
later in this work).
\newline
\par

While \cite{Mitchell2018} provides a reasonable collision avoidance system
based on optical flow, it does not fit so neatly into the ECX model. We therefore
aim to explore an alternative, yet still biologically plausible collision
avoidance system which will fit into the ECX model.
\newline
\par

Our ultimate hope, is to provide some insight into the precise biological
systems in play during a point-to-point navigational task.

\subsection { Practical Goals }
The original aim of this project was to build on the experimental
scenario from \cite{Mitchell2018}. The robot would be tested by
allowing it to navigate through a cluttered environment using a
collision avoidance system. The navigational systems would then be
tasked with bringing the robot home through the cluttered environment
using a combination of visual information, a path integration vector,
and collision avoidance.
\newline
\par

In order to achieve this experimental goal, we break the project down
into four stages:

\begin{enumerate}
\item{The first stage will involve solving some choice technical issues
  picked up by \cite{Mitchell2018}; making any hardware/software adjustments
  required to provide a solid foundation on which to develop.}

\item{We can then begin investigating existing systems and establishing
  experimental metrics. This stage will involve research and review of new topics
  (the main one being the Central Complex model for Path Integration), and their
implementations on the robot (if they exist). This stage will look to establish
appropriate metrics by testing the existing CX model in a non-deterministic
navigational task (see Section \ref{sec:methods}).}

\item{This stage will involve the set up and testing of the individual components
  of the ECX model. Building the modified optical flow system, using the work
  from \textit{Zhang} to combine the MB model with the CX, and finally, putting
  the three pieces together.}

\item{Finally, the collection and compilation of results from the combined system
and the individual systems.}
\end{enumerate}

As will be discussed later, we did not reach our goal. We will discuss
our progress and testing up to the end of stage three of the project, we did not
manage to complete stage four.

\subsection { Results }
This work is based on work done previously by Leonard Eberding, Luca Scimeca,
Zhaoyu Zhang, and Robert Mitchell.
\cite{Eberding2016, Scimeca2017, Zhang2017, Mitchell2018}.
\newline

Significant contributions of this project:
\begin{enumerate}
\item{
  Research and installation of a new compass sensor for the AntBot control
  systems.
}

\item{
  A major code refactor has taken place to make AntBot more usable for this
  project, and make it more accessible for future students.
}

\item{
  Addition of a calibration system to allow the user to auto-detect the position
  of the camera lens attachment (see Section \ref{sec:mod}).
}

\item{
  Results gathered for the Central Complex model using a non-deterministic
  outbound route.
}

\item{
  Construction of a video recording tool to allow processed
  image frames to be recorded from the robot for later viewing and
  offline analysis.
}

\item{ Results definitively showing the impracticality of a focus of
    expansion based system for collision avoidance and justifying
    continued use of matched filters.
}

\item{ Identified and fixed a bug in the visual preprocessing system
  to raise the framerate from 2fps to approximately 10fps under normal usage.}

\item{ Construction of a neural collision avoidance model based on the
  matched-filter system from \cite{Mitchell2018, Stewart2010}. }


\item{
  Implementation of a biologically plausible ``base model'' for
  insect navigation, the Extended Central Complex (ECX) model.
}

\item{
 Justification for the recommendation that the AntBot be retired at
 the earliest opportunity.
}
  
\end{enumerate}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% BACKGROUND                                                                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Brief summaries of OF and MB, more detailed discussion of CX
\section{ Background }
This project builds directly upon \cite{Mitchell2018}. We first provide a review
of the relevant background topics from that paper, before developing the relevant
ideas further for this project. This will be a very brief summary of the work
that took place and any relevant results or new perspectives. For more detail,
please consult \cite{Mitchell2018}.


\subsection{ The Focus of Expansion for Collision Avoidance } \label{OFBackground}
Optical flow is a large and diverse area of study. As such, we will
not provide a complete background on the fundamental
principles. Relevant terms will be explained, however, a succinct
background of the concepts necessary for this paper can be found in
\cite{Mitchell2018}. A comprehensive introduction is given by
\textit{O'Donovan} in \cite{ODonovan2005}.
\newline
\par

The main driving point in this paper is the integration of multiple
navigational systems into a single model, namely, the Central Complex.
Optical flow filtering worked well for a standalone collision
avoidance system, however, it does not fit so intuitively into the CX
model. We therefore investigate an alternate approach. The relevant
background revisits the concept of the \textit{focus of expansion}
(FOE) from \cite{Mitchell2018, ODonovan2005}.  The FOE is defined as
the point from which all optical flow vectors originate. The location
of the FOE, and its relationship with the rest of the flow field, can
tell us things about the motion and depth of the image.
\newline
\par

In \cite{Mitchell2018}, the FOE was used only to compute
time-to-contact with an obstacle. In this work, we instead attempt to
use it directly to determine the potential location of an
obstacle. Following \cite{Mitchell2018} we will be using a dense optic
flow field (tracking motion for every pixel in the image) as the
computation of sparse fields was shown to be unreliable on the
AntBot. This potentially makes the problem more difficult; the basic
method for computing the FOE given by \cite{ODonovan2005} is
computationally complex for a dense flow field
\cite{Mitchell2018}. The time taken to compute may become prohibitive,
though as will be seen, this computation did not become the main
barrier to efficiency in implementation. A common, reliable and
intuitive method of computing the FOE is the Least Squared-Error (LSE)
method \cite{Tistarelli1991, ODonovan2005, Vanderstap2012}. This
method also appears to be in use in \cite{Souhila2007}, a major
inspiration for the optical flow methods presented in
\cite{Mitchell2018}, though it is presented in a different manner

\subsubsection{The Least Squared-Error method}
This is the method given by \textit{O'Donovan} and discussed in
\cite{Mitchell2018}. The FOE is computed simply as:

\begin{equation}
  \label{eq:foe}
  FOE = (A^TA)^{-1}A^T\mathbf{b}
\end{equation}

\begin{equation*}
  \begin{split}
 A =
\begin{bmatrix}
  a_{00} & a_{01}\\
  \dots  & \dots \\
  a_{n0} &  a_{n1}
\end{bmatrix}
\qquad
\end{split}
\begin{split}
\mathbf{b} =
\begin{bmatrix}
  b_0 \\
  \dots \\
  b_n
\end{bmatrix}
\end{split}
\end{equation*}
\newline

Where, each pixel $p_i = (x, y)$ has associated flow vector $\mathbf{v} = (u,v)$.
Finally, set $a_{i0} = u$, $a_{i1} = v$ and $b{i} = xv - yu$. Note that this
computation and explanation have been more or less copied from
\cite{Mitchell2018} for the benefit of the reader. For more details, the reader
should consult \cite{Mitchell2018}, or \cite{ODonovan2005}.
\newline
\par

This method for estimating the focus of expansion was originally given by
\textit{Tistarelli et al.} in their paper
\textit{Dynamic Stereo in Visual Navigation}\cite{Tistarelli1991, ODonovan2005},
and it serves as an excellent example for the reason the FOE is so difficult to
compute. In theory, we should be able to take any two vectors $\mathbf{u}$,
$\mathbf{v}$ from the flow field, and compute the point at which lines
running along them intersect. This point of intersection would give us the
FOE \cite{ODonovan2005}. While wonderfully simple, this method only works for
a perfect flow field. In reality, flow fields are imperfect; for example,
visual noise can cause disruptions to areas of the field. Therefore picking two
arbitrary vectors could lead to an erroneous FOE. Unravelling the matrix notation
of Equation \ref{eq:foe}, shows this to be a least squares technique;
essentially, we compute the best FOE for the flow field given.

%% \subsubsection{The fuzzy method}
%% This method was not used for this paper but it was researched as an
%% alternative to the LSE method; we found this interesting as most
%% papers seemed to default to the LSE method or a variation. We present
%% this briefly for the sake of curiosity, we would encourage the reader
%% to investigate the work presented in \cite{Burger1989}.
%% \newline\par

%% Instead of attempting to estimate an exact point which is likely to be
%% the FOE, \textit{Burger and Bhanu} propose a method for computing a
%% small region which is likely to contain the FOE. This region is
%% referred to as the \textit{fuzzy FOE}\cite{Burger1989}.
%% \newline
%% \par

%% Their motivation for using a region as opposed to a point is the rotational
%% aspect of motion. If we have a wheeled robot with a fixed camera, the camera has
%% (broadly) two aspects to its motion: Translational and Rotational. Translation is
%% experienced when the robot moves forward, or backward in a straight line
%% (i.e. the camera is moving directly into the image or away from it). Rotation
%% occurs when the robot turns. If the robot moves in a straight line, the rotation
%% component can be ignored; similarly, if the robot turns on the spot, the
%% translation component can be ignored. Many papers on using the FOE will ignore
%% rotation. In many applications (including our own) this is completely acceptable;
%% for example, if the robot only ever moves in straight lines with stationary
%% turns. Their discussion takes place in the context of an Autonomous Land Vehicle
%% (ALV) which is capable of arcing movements, so both components come into play.
%% \newline
%% \par

%% Perhaps counter-intuitively, their computation still requires testing and
%% working with a single FOE.

%% The computation takes two steps:
%% \begin{enumerate}
%% \item{
%%  Search for a location (they call it $\mathbf{x}_{min}$) of local minimum error
%%  starting from an arbitrary location $\mathbf{x}_0$.
%% }

%% \item{
%%  \textit{Grow} a connected region around this $\mathbf{x}_{min}$ until some
%%  criterion is met.
%% }

%% \end{enumerate}

%% The criterion they choose is an accumulated error volume threshold; i.e. setting
%% a limit on the amount of error that can be contained in the fuzzy FOE;
%% intuitively, the smaller the error, the more likely the region is to contain the
%% FOE.\newline\par

%% The difference between the two methods could be described by the
%% concept of local/global minima. Usually we wish to look for global
%% minima and local minima are a trap which must be keenly sidestepped,
%% however, in this case the global minimum may be the eroneous result.
%% \newline\par

%% While in theory, the LSE method can find the exact FOE with only two
%% flow vectors, in practice flow fields are often incredibly noisy. The
%% LSE method searches for a minimum error across the whole vector field
%% simultaneously, if the field is noisy, then there is a high chance
%% that the computed FOE will be incorrect, despite the fact it minimises
%% error across the whole flow field.
%% \newline\par

%% The fuzzy method, on the other hand, looks at local error; in essence,
%% we want to know how accurate the computed FOE is for the vectors in a
%% specific region. The region which minimises this error is highly
%% likely to contain the true FOE (though it may not be the one under test).

%

\subsection{Matched Filters for Collision Avoidance}\label{sec:offbg}
In light of the results we gather with regards to the FOE, we will
also revisit the matched-filter collision avoidance system from
\cite{Mitchell2018}, though the flow field computation, filter
generation and comparision are unchanged. We present a brief
description of the intuition here as we will go on to develop neural
representations of this model later. For details of the filter and
matching process, please see \cite{Mitchell2018}.
\newline\par

In brief, we compare the observed optical flow field to a precomputed
filter. This filter represents the motion we expect to observe in the
image frame given some self (camera) motion. This method for collision
avoidance has been an active area of research in the biorobotics (and
wider robotics) community for some time though there are different
hypotheses about how the information is actually used by insects
\cite{Julien2017}.\newline\par

The difference between the expected flow field (the filter) and the
actual flow field computed from consecutive images was used by
\textit{Scimeca} for speed retrieval \cite{Scimeca2017}. In
\cite{Mitchell2018} we observe that objects in different 3D positions
will appear to move at different speeds in the image frame;
specifically objects which are on a collision course will appear to
loom (grow quickly) in the agent's field of view. Therefore, by
computing the speed from optical flow on both sides of the field of
view, we can determine if an object is looming on a particular side by
comparing the observed speed on both sides; this can then be used to
generate an avoidance saccade\footnote{This is subtly different from
  the balance strategy presented in \cite{Julien2017}, though we adapt
  the system into a balance strategy as part of the neural
  implementation.}. This method was also used by \textit{Stewart et
  al.} in their simulations of Drosophila \cite{Stewart2010};
interestingly they use the matched filter to detect a collision, then
use the position of the FOE to determine a steering direction
\cite{Stewart2010}. An example of the flow disturbance caused by a
looming object can be seen in Figure \ref{fig:flowfield}.
\newline\par

\subsection{ The Mushroom Body for Visual Navigation } \label{MBBackground}
The Mushroom Body model is an artificial neural network which models
the mushroom body structures present in the insect brain
\cite{Ardin2016}. It consists of three layers: Projection Neurons
(PNs), Kenyon Cells (KCs) and Mushroom Body Output Neurons
(MBONs). These MBONs are also referred to as Extrinsic Neurons (ENs)
by older works, we use MBON herein. The original MB model proposed by
\textit{Ardin et al.} for navigation in \cite{Ardin2016} contained 360
visual PNs (vPNs), 20,000 KCs, and a single MBON. Every KC connects to
the MBON, and each KC also connects to 10 vPNs chosen uniform
randomly. The KC-MBON connections all start with weight $w=1$. The
figure and caption from \textit{Ardin et al.} is given here in Figure
\ref{fig:mb}. In their implementation and previous AntBot
implementations, captured images were converted to greyscale and
downsampled \cite{Ardin2016, Eberding2016, Zhang2017,
  Mitchell2018}. As such, each pixel can be described by its
brightness (greyscale value) and each KC has a brightness threshold.
\newline
\par

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{Ardin2010MBModel}
  \caption{\label{fig:mb} The Mushroom Body circuit: (Caption from
    \textit{Ardin et al.}, Figure 2; note, their description and figure uses
    ``EN'' instead of ``MBON''): Images (see Fig 1) activate the visual
    projection neurons (vPNs). Each Kenyon cell (KC) receives input from 10
    (random) vPNs and exceeds firing threshold only for coincident activation
    from several vPNs, thus images are encoded as a sparse pattern of KC
    activation. All KCs converge on a single extrinsic neuron (EN) and if
    activation coincides with a reward signal, the connection strength is
    decreased. After training the EN output to previously rewarded (familiar)
    images is few or no spikes.}
\end{figure}

Learning occurs by showing patterns (images) to the vPNs. Each KC sums the
brightness of all connected vPNs and compares that sum to the activation
threshold. If the total brightness is greater than the threshold then the KC is
\textit{activated}. As the connections between the vPN and KC layers are random,
a given image will generate some sparse pattern of KC activation; to learn this
pattern, the weight of the KC-MBON connection is lowered from 1 to 0 for the
active KCs.
\newline
\par

To determine image familiarity during the recapitulation process, a pattern is
projected onto the vPNs (again giving a sparse pattern of KC activation).
The MBON then sums the synaptic (KC-MBON) weights of all active KCs to obtain a
\textit{familiarity} measure; recall, these weights are decayed as part of the
learing process; thus, the lower the MBON output, the more familiar the image
(and vice versa). Different route following strategies have been implemented
(scanning \cite{Ardin2016, Eberding2016, Zhang2017}, Klinokinesis
\cite{Zhang2017}, combination with the CX model \cite{Zhang2017}(see below), and
visual scanning \cite{Mitchell2018}); but, most commonly, some form of scanning
is used whereby the agent scans an arc for the most familiar direction.
\newline
\par

This has been a brief explanation for context. Part 1 gives slightly deeper
insight \cite{Mitchell2018} (particularly in relation to AntBot projects), and of
course the work of \textit{Ardin et al.} can give the full details
\cite{Ardin2016}.
\newline
\par

The MB model described above has been tested on the AntBot in three
different works (albeit using different methods and metrics); the
network as presented in \cite{Eberding2016, Zhang2017, Mitchell2018}
differs only in the number of PNs present (900 vPNs are used in all
three works), and the KC modelling (\textit{Ardin et al.} use a
spiking model\footnote{The AntBot projects do not explicitly model
  membrane potentials and realistic neural responses.  They use a
  simple abstraction for the sake of complexity. The network function
  is the same.} for the KCs \cite{Ardin2016}). More
relevant to this work, is the proposed modification given by
\textit{Zhang}, whereby, 8 MBONs are present as opposed to 1 (see
Section \ref{CXMBBackground}).
\newline
\par

\subsection{ The Central Complex for Path Integration } \label{CXBackground}
The Central Complex (CX) is a highly conserved structure present in the insect
brain \cite{Pfeiffer2014, Stone2017}. Though the finer structural details and
component position may vary, the basic composition is more or less the same
across species \cite{Pfeiffer2014}. Organization and function of the
various parts of the CX are given by \textit{Pfeiffer and Homberg} in
\cite{Pfeiffer2014}.
\newline
\par

We instead direct our attention to the CX model presented by
\textit{Stone et al.}  which is the first neural model for path
integration in the insect brain with structure drawn purely from
physiology. Interestingly, this model is a more advanced version of an
earlier model presented by \textit{Haferlach et al.}, which was
\textit{evolved} using a Genetic Algorithm \cite{Haferlach2007}. We
present this also, for interest.

\subsubsection{The Evolved Model for Path Integration}
Prior to the CX model described below, there were several candidate neural
networks proposed for PI in ants \cite{Haferlach2007}. \textit{Haferlach et al.}
report the
two best-known candidates being hand-designed, with more recent research
into an evolutionary approach to network design \cite{Haferlach2007}. The
approach presented in \cite{Haferlach2007} follows this trend and employs
a Genetic Algorithm (GA) (see Appendix \ref{ap:ga} for a brief
overview of GAs) to
evolve a neural model which is suitable for PI with biologically plausible
inputs.
\newline
\par

\textit{Haferlach et al.} encode solutions as lists of integers. In these lists
there are \textit{start markers} and \textit{end markers} which delimit
the definitions of the actual neurons or sensors present in the model. The
complete model (topography, connection weights, sensors, and effectors) is
limited to a fixed-size encoding (genome limited to 500 parameters; ~$50$ neurons
maximum) \cite{Haferlach2007}. An example encoding from \cite{Haferlach2007} can
be seen in Figure \ref{fig:markerbasedencoding}.
\newline
\par


\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{MarkerBasedEncoding}
  \caption{\label{fig:markerbasedencoding} (Figure 2 from \cite{Haferlach2007})
    The Marker-Based encoding scheme used by \textit{Haferlach et al.},
    demonstrating how a simple neural model would be encoded as a sequence of
    integers.}
\end{figure}

Localised \textit{tournament selection} is used
to pick solution for transition into the next generation. Each
solution is assigned a location in 1-dimensional space; a solution is
picked randomly, then another solution within distance $k$ of the
first is picked for the tournament ($k$ usually between 5 and 15)
\cite{Haferlach2007}. The solution with higher fitness wins the
tournament and progresses to mutation and
\textit{crossover}\footnote{Crossover - A particular method for
  breeding two solutions whereby their genomes are cut at a particular
  point and the ends swapped.} This localised selection allows the
algorithm to find multiple \textit{good} solutions which need not
share the same genetic information.
\newline
\par

The fitness of each solution is evaluated by having the agent navigate an
outbound path through two randomly chosen points, then testing its ability
to navigate back to the start point. During the outbound phase, information is
input into the path integration network via the sensors; the network output is
ignored. During the inbound route, the agent is steered by the network output.
An individual is evaluated until some maximum time limit $m$ is reached, and the
distane to the start point $dist_t$ is computed at each time step. The fitness is
then the inverse sum of squared distances \cite{Haferlach2007}:

\begin{equation}
 f = \frac{1}{\sqrt{\int_{0}^m dist_t^2 dt}}
\end{equation}

The fitness is then augmented to penalise homeward routes which
spiral (circling the start point, getting closer with each pass), and
also to reward networks which are simple in structure; shown in
Equations \ref{eq:fithead} and \ref{eq:fitsimple} respectively.

\begin{equation}
  \label{eq:fithead}
  f = \frac{1}{\sqrt{\int_{0}^m dist_t^2 |\theta - \omega | dt}}
\end{equation}
\begin{equation}
  \label{eq:fitsimple}
  f = \frac{1}{(1 + k_n)^{c_n}(1 + k_c)^{c_c}
    \sqrt{\int_{0}^m dist_t^2 |\theta - \omega | dt}}
\end{equation}

where $\omega$ is the nest heading, $\theta$ is the agent's heading,
$k_n$ is a neuron penalty constant, $k_c$ is a connection penalty
constant, $c_n$ is the number of neurons, and $c_c$ is the number of
connections (i.e.  the network is heavily penalised for the number of
connections and neurons it has, forcing simpler networks)
\cite{Haferlach2007}. These are applied as a two-stage evolutionary
process; evolve a solution first using Equation \ref{eq:fithead} to
compute the fitness, then use this solution to seed a new population
and evolve a solution from here using Equation \ref{eq:fitsimple}. The
two-stage evolutionary process alone did not find acceptable
solutions. The search space was constrained by providing the following
four constraints on network topology: Each direction cell had to be
connected to at least one memory neuron; each direction cell had to be
connected to at least one sigmoid neuron; sigmoid neurons had to be
connected to turn effectors; a maximum of two turn effectors (left and
right) were allowed \cite{Haferlach2007}.
\newline
\par

\textit{Haferlach et al.} present the model shown in Figure
\ref{fig:evolvednetwork}; notice the evolved structure bears a striking
resemblance to the CX model presented by \textit{Stone et al.} (Figure
\ref{fig:cx}). Indeed, this network operates on the same principle; encoding
a heading vector across multiple neurons with directional preferences. This
network provides a compact, elegant, robust model capable of performing
path integration with reasonable errors. The network also demonstrated
some capability for PI with obstacles present (though errors were generally much
greater) \cite{Haferlach2007}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{EvolvedNetwork}
  \caption{\label{fig:evolvednetwork} (Figure 3 from \cite{Haferlach2007})
    The high-fitness network, consistently evolved using the constrained
    two-stage evolution process illustrated by \textit{Haferlach et al.}
  }
\end{figure}

\subsubsection{The Central Complex Model}
The Central Complex model is a six layer artificial neural network presented by
\textit{Stone et al.} which has been shown to provide a plausible neural
substrate for Path Integration (PI) both in simulation and on the AntBot platform
\cite{Scimeca2017, Stone2017}. The model presented is shown in Figure
\ref{fig:cx}. Splitting the model into its six layers, we get a breakdown of
functionality:
\newline
\par

\begin{itemize}
\item{Layer 1: Heading preprocessing (TL), Speed (TN)}
\item{Layer 2: Heading preprocessing (CL1)}
\item{Layer 3: Heading (TB1)}
\item{Layer 4: Memory (CPU4)}
\item{Layer 5: Normalisation and Inhibition (Pontine Neurons)}
\item{Layer 6: Steering/output (CPU1)}
\end{itemize}

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{StoneCXModel}
  \caption{\label{fig:cx} The Central Complex model presented by
    \textit{Stone et al.}. (Left) This graph demonstrates the basic structure of
    the CX model (Figure 5G from \cite{Stone2017}). Pontine neurons have been
    excluded for clarity.
    (Right) This graph shows how signals propogate through the network where the
    current heading lies to the left of the desired heading, i.e. a right turn
    should be generated (Figure 5I from \cite{Stone2017}). The numbers given at
    each layer on the right correspond to the numbers given for each neuron
    in the graph on the left.
  }
\end{figure}

Figure \ref{fig:cx} shows four types of neuron: TN (Tangential Neuron,
brown), TB1 (green), CPU4 (yellow and orange), and CPU1 (dark blue,
light blue, and purple).
\newline
\par

While Figure \ref{fig:cx} (Left) shows a distinction between CPU1a (blue) and
CPU1b (purple) neurons, we will ignore this distinction; for clarity, the
physiological mapping between these CPU1 subtypes in the Upper Central Body (CBU)
and the Protocerebral Bridge (PB) is different, but the function they serve in
the Central Complex is the same \cite{Stone2017}; thus, the distinction makes
no difference in the model. Similarly the normalisation and inhibition function
of the Pontine Neurons only has an effect when the agent experiences holonomic
motion (motion where the view direction does not match the direction of travel);
as AntBot is incapable of such motion, we can safely ignore the function of
the Pontine Neurons also; in our case, the Pontine Neurons would have the
same activity patterns as the CPU4 neurons \cite{Stone2017}. Figure \ref{fig:cx}
(Right) shows how the Pontine inhibition is structured.
\newline
\par

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{Figure5F}
  \caption{
    \label{fig:cxlayer} Here we can see the layers of the CX model and
    how they fit together. A heading signal is input to the TL neurons,
    propogating through the CL layer to TB1 (heading ring-attractor) and
    CPU4 (memory). TN neurons (speed sensitivity) input directly to CPU4.
    So, the combination of heading and speed inputs to CPU4 gives a measure
    of distance travelled in a particular direction; this facilitates generation
    of a steering command in CPU1 providing a mechanism for path integration.
  }
\end{figure}

We now break down the different neuronal types and their proposed functions.
Citation is given, but for the avoidance of any doubt, the following descriptions
are adapted from \textit{Stone et al.} (see STAR Methods) \cite{Stone2017}.
We also add implementation information for the AntBot where appropriate;
occassionally the AntBot implementations are slightly different.
\newline
\par

\textbf{Simulated Neurons:}
Each neuron is described by its firing rate, where the firing rate $r$ is
a sigmoid function of the input $I$:

\begin{equation}
r = \frac{1}{1 + e^{-(aI - b)}}
\end{equation}

where $a$ and $b$ are parameters which control the slope and offset of
the sigmoid function \cite{Stone2017}. Optionally, Gaussian noise may
be added to the output (no noise is applied in AntBot
implementations). The input to each neuron is a weighted sum of the
activity of each neuron that synapses onto it; say neuron $j$, the
input is:

\begin{equation}
  I_j = \sum_{i} w_{ij} \cdot r_i
\end{equation}

The weights used by \textit{Stone et al.} can only be 0, 1, or -1 for
no-connection, excitatory, or inhibitory respectively \cite{Stone2017}.
\newline
\par

% Function of CL & TL
\textbf{TL \& CL1:}
The TL neurons are the input point for heading information. In the ant brain
this heading information comes from a \textit{sky-compass}; the ant eye contains
cells sensitive to polarized light which allows the ant to infer an accurate
allothetic heading from vision alone. Interestingly, there is also evidence that ants
have the capability to infer a direction without a view of celestial cues,
suggesting they may have access to some other signal, the candidate signal being
the geomagnetic field \cite{Fleischmann2018, Grob2017}. The TL neurons have been
shown to be polarisation sensiive across multiple insect species \cite{Stone2017}
(see STAR Methods). Each TL neuron has a preferred direction (i.e. a specific
direction of poliarisation sensitivity) $\theta_{TL} $, and there are 16 such
neurons representing the 8 directions around the agent (i.e.
$\theta_{TL} \in \{ 0^{\circ}, 45^{\circ}, 90^{\circ}, 135^{\circ}, 180^{\circ},
225^{\circ}, 270^{\circ}, 315^{\circ}\} $) \cite{Stone2017}. Together, the 16
TL neurons encode the heading of the agent in a single timestep; each neuron
receives input activation as:

\begin{equation}
  I_{TL} = \cos ( \theta_{TL} - \theta_{h} )
\end{equation}

where $\theta_{TL} $ is the preferred heading of the neuron as above, and
$\theta_{h} $ is the current heading of the agent. In the next heading layer,
there are 16 CL1 neurons which use inhibition to invert the polarisation
response \cite{Stone2017}. \textit{Stone et al.} comment that these neurons
effectively make no difference to the model and are included for completeness.
They are also included in previous AntBot projects which make use of the CX model
\cite{Zhang2017, Scimeca2017}. On AntBot, the heading is derived from the onboard
compass on the mobile phone (see Section \ref{sec:platform}).
\newline
\par

% Function of TN
\textbf{TN:}
There are 4 TN neurons which act as an input for speed information. The TN
neurons are sensitive to optical flow and can be split into two subtypes:
TN1, and TN2. \textit{Stone et al.} showed that TN1 neurons are inhibited by
simulated forward flight and excited by simulated backward flight, while TN2
neurons are inhibited by simulated backward flight and excited by simulated
forward flight \cite{Stone2017}. Each of the four TN neurons has a tuning
preference; these tuning preferences were measured in bees as approximately
$+45^{\circ}$/$-45^{\circ}$ for TN2, and $+135^{\circ}$/$-135^{\circ}$ for TN1
(where $0^{\circ}$ is straight ahead). In summary, we have TN$1_{left}$,
TN1$_{right}$, TN$2_{left}$, and TN$2_{right}$. It is thought that these neurons
provide a (or part of a) mechanism for odometry by allowing the model to
integrate speed with respect to time giving a distance measure \cite{Stone2017}.
\textit{Stone et al.} give the speed calculation from the TN neurons
as:

\begin{equation}
  I_{TN_{L}} =
  [ \sin (\theta_{h} + \phi_{TN}) \quad \cos (\theta_{h} + \phi_{TN}) ]\mathbf{v}
\end{equation}

\begin{equation}
  I_{TN_{R}} =
  [ \sin (\theta_{h} - \phi_{TN}) \quad \cos (\theta_{h} - \phi_{TN}) ]\mathbf{v}
\end{equation}

where $\mathbf{v}$ is the velocity of the agent in Cartesian coordinates,
$\theta_h \in [0^{\circ}, 360^{\circ})$ is the current heading of the agent, and
$\phi_{TN}$ is the preferred angle of that TN neuron \cite{Stone2017}.
\newline
\par

The TN neurons are not modelled in the \textit{basic} CX model on AntBot. There
is, however, a \textit{holonomic} implementation which models both the TN1 and
TN2 neurons is present on the robot. The formulae above are implemented on the
robot but they are never used. Instead, the raw left and right speeds computed
from the optical flow are passed in (see \textit{Scimeca} \cite{Scimeca2017} for
the details of speed computation from optic flow). The TN2 neurons will simply
clip these speeds to make sure they lie in $[0,1]$; the TN1 neurons will perform
$(1 - s) / 2$ for both the left and right speeds $s$, then clip the output to lie
in $[0,1]$. The output is returned directly, rather than being returned as a
sigmoid (as for all other neuron types).
\newline
\par

% Function of TB1
\textbf{TB1:}
There are eight TB1 neurons, each with a directional preference $\theta_{TB1}$,
which correspond to the eight cardinal directions in the model. Each TB1 neuron
receives excitatory input from the pair of CL1 neurons that have the same
directional preference. The TB1 layer contains inhibitory connections between
peer neurons where each TB1 neuron strongly inhibits other TB1 neurons with
opposite directional preferences (see Figure \ref{fig:cx}) forming a
\textit{ring attractor}\cite{Stone2017}. The weighting for an arbitrary
inhibitory connection from neuron $i$ to neuron $j$ is given by:

\begin{equation}
  w_{ij} =
  \frac{\cos (\theta_{TB1,i} - \theta_{TB1,j}) - 1}{2}
\end{equation}

where $\theta_{TB1,i}$ is the direction preference of TB1 neuron $i$ (similarly
for $\theta_{TB1,j}$). The total input for each TB1 neuron from the CL1 layer
at timestep $t$ is:

\begin{equation}
  I_{TB1_{j}^{(t)}} =
  (1 - c) \cdot r_{CL1_j}^{(t)} + c \cdot \sum_{i = 1}^{8} w_{ij}
  \cdot r_{CL1_j}^{(t - 1)}
\end{equation}

where $c = 0.33$ is a scaling factor which determines the relative strength
of excitation from the CL1 layer and inhibition from other TB1 neurons;
the AntBot implementation of the TB1 neurons is the same. This
network layer produces a stable heading encoding which provides accurate
input for the CPU4 layer, underpinning accurate path integration.
\newline
\par

% Function of CPU4
\textbf{CPU4:}
The 16 CPU4 neurons receive input in the form of an accumulation of
heading $\theta_h^{(t)}$ of the agent, along with a modulated speed response
from the TN2 neurons \cite{Stone2017}. The CPU4 neurons accumulate distance with
direction. The input the CPU4 neurons is given by:

\begin{equation}
  \label{eq:cpu4update}
I_{CPU4}^{(t)} = I_{CPU4}^{(t - 1)} + h \cdot (r_{TN2}^{(t)} - r_{TB1}^{(t)} - k)
\end{equation}

where $h = 0.0025$ determines the rate of memory accumulation, and $k = 0.1$ is
a uniform rate of memory decay \cite{Stone2017}. All memory cells are initialised
to $I_{CPU4}^{(0)} = 0.5$ and are clipped at each timestep to fall between 0 and
1 \cite{Stone2017}. As shown in Figure \ref{fig:cx}, each TB1 provides input to
two CPU4 neuron, each of which receives input from the TN2 cell in the opposite
hemisphere. As these neurons accumulate distance with respect to a direction,
they provide a population encoding of the \textit{home vector} (the integrated
path back to the nest) \cite{Stone2017}. Interestingly, \textit{Zhang} showed
that the network can be initialised to an arbitrary state, allowing the agent
to navigate along arbitrary vectors \cite{Zhang2017}. While this seems intuitive,
the experimental evidence is valuable and demonstrates that the CPU4 layer could
form a basis for the \textit{vector memory} discussed by \textit{Webb} in
\cite{Webb2019} (see Section \ref{CXMBBackground}).
\newline
\par

The basic CX model on the AntBot does not model TN neurons, so the input is
computed differently; furthermore, the gain and loss factors are different.
The input can be expressed as:

\begin{align}
  I_{CPU4}^{(t)} =
  I_{CPU4}^{(t - 1)} + s \cdot ((1 - r_{TB1}^{(t)}) \cdot g - l)
\end{align}

where $l = 0.0026$ is the uniform rate of memory loss, $g = 0.005$ is the
uniform rate of memory gain, and $s$ is the current speed. This value is
then clipped to lie in $[0,1]$. As an aside, in the holonomic model
mentioned previously, the input is computed as in Equation \ref{eq:cpu4update}
as the TN neurons are present in the model.
\newline\par

\textbf{Pontine:}
The pontine neurons project contralaterally connecting opposite CBU columns
(shown in Figure \ref{fig:cx} (Right)). The 16 pontine neurons each receive input
from one CPU4 column \cite{Stone2017}; pontine input can be given simply as:
\newline
\par

\begin{equation}
  I_{Pontine}^{(t)} = r_{CPU4}^{(t)}
\end{equation}

The pontine neurons are not implemented on AntBot for simplicity. AntBot is
incapable of holonomic motion, so they would have no functional effect.
\newline
\par

% Function of CPU1
\textbf{CPU1:}
There are 16 CPU1 neurons present in the model. Each of which
receives inhibitory input (weight $= -1$) from a TB1 neuron; where, each
TB1 neuron provides inhibitory input to two CPU1 neurons (in the same pattern
as the TB1-CPU4 connections - see Figure \ref{fig:cx}). Each CPU4 neuron also
provides input to a CPU1 neuron (so we get input from vector memory and current
heading). The CPU1 input can be expressed as:

\begin{equation}
  I_{CPU1}^{(t)} = r_{CPU4}^{(t)} - r_{Pontine}^{(t)} - r_{TB1}^{(t)}
\end{equation}

As can be seen the CPU1 neurons also receive input from the pontine neurons.
The CPU1 neurons form two sets, those connecting to left motor units and those
connecting to the right. On AntBot the pontine term is omitted
(i.e. the input becomes
$I_{CPU1}^{(t)} = r_{CPU4}^{(t)} - r_{TB1}^{(t)}$).
\newline
\par

We choose a direction simply by summing the CPU1 outputs on both sides
and the difference indicates the direction and strength of the
required heading correction:

\begin{equation}
  \theta_h^{(t)} = \theta_h^{(t - 1)} +
  m \cdot (\sum_{i = 1}^{8} r_{CPU1R_{i}} - \sum_{i = 1}^{8} r_{CPU1L_{i}})
\end{equation}

where $m = 0.5$ is a constant \cite{Stone2017}. The angle computation is the
same on the AntBot.
\newline
\par

There are some details which have been omitted for clarity. The stack presented
should effectively describe how the network generates a turning signal from its
current state and inputs to perform effective, accurate path integration. For
further details, please consult \cite{Stone2017} (the STAR Methods section
contains the full technical details of the model).

\subsection{ The Eight MBON Model (CXMB) } \label{CXMBBackground}
In \cite{Zhang2017}, the MB network is modified by adding 7 MBONs. Each MBON has
its own KC-MBON connection array, each with their own unique weights. Each
connection array corresponds to one of the eight cardinal directions represented
by the TB1 layer of the CX model (namely, $0^{\circ}$, $45^{\circ}$,
$90^{\circ}$, $135^{\circ}$, $180^{\circ}$ ,$225^{\circ}$, $270^{\circ}$,
$315^{\circ}$) (see Section \ref{CXBackground}).
\newline
\par

Image memory now has an associated direction, so training is performed
with respect to orientation. For example, if the agent has a heading
of $45^{\circ}$ when a image is stored, then only the corresponding
connection array has its weights updated during learning. Practically,
this is done by querying the TB1 layer of the CX model to find the
current direction according to the model (rather than directly
querying the robot's onboard compass directly). This process is
visualised in Figure \ref{fig:eightmbon}.
\newline
\par

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{EightENModel}
  \caption{\label{fig:eightmbon} Our interpretation of the eight MBON model
    proposed by \textit{Zhang}. Every KC connects to every MBON. All connection
    weights start out at $w=1$. Following the example presented in the text,
    if an image being learned corresponds to facing a direction of $45^{\circ}$,
    then only the conncetions to that MBON (highlighted in red) are eligible to
    have their weights modified. Recall, however, that these weights will only
    be modified if the KC was activated (not shown in the figure).
    }
\end{figure}

While, in theory, this eight MBON model could function independently,
\textit{Zhang} uses it to (rather neatly) augment the CX model
(creating the combined CXMB model); this allows navigation to be
performed using a combination of visual memory and path integration
information. The navigation process can be described by a sequence of
equations. We modify the notation slightly for clarity, but the
equations are the same as presented in \cite{Zhang2017}.
\newline
\par

The MB circuit is shown an image in the usual way; however, we now
get eight responses, giving us a response distribution. This distribution can
be interpreted as giving us the most likely direction of travel, when the image
presented was first observed. This distribution requires some modification to
integrate it into the CX response. Let $M_i$ be the familiarity response of the
$i$th MBON; the responses are normalised as:

\begin{equation}
  \bar{M}_i = \frac{M_i}{\Sigma^{8}_{k = 0} M_k}
\end{equation}

Which gives us the normalised response $\bar{M}_i$ between 0 and 1. $\bar{M}_i$
must be inverted so that the most likely direction has the greatest response
(in the MB model, the most familiar direction would give the lowest response):

\begin{equation}
  \bar{M}^{-1}_i = 1 - \bar{M}_i
\end{equation}

This visual response is then combined with the memory response from the CPU4
layer of the CX model to give output at the CPU1 layer:

\begin{equation}
  CPU1_{output} = k \cdot W_{CPU4} \cdot CPU4 + (1 - k) \cdot W_{MBON} \cdot
  \bar{M}^{-1}
\end{equation}

where $k$ is a weighting factor that determines the relative stregths of the CX
response and the MB response in the output ($k = 0.8$ in \cite{Zhang2017}),
$\bar{M}^{-1}$ is the collection of all inverse normalised MBON responses,
$W_{CPU4}$ is a custom matrix\footnote{This is the term used by \textit{Zhang} to
  describe   the $W_{CPU4}$ matrix. More specifically, this matrix describes the
  connections between the CPU4 and CPU1 layers of the CX model.}, $W_{MBON}$ is
an identity matrix that will expand the MBON response array from 8x1 to 16x1
\cite{Zhang2017}.
\newline
\par

In order to test the CXMB model, \textit{Zhang} first demonstrated the
functionality of a \textit{copied memory model}. The test of the copied memory
model aimed to prove that the CPU4 state of the agent could be copied and stored,
the CPU4 state modified, and then restored from the copy to allow the agent to
navigate home. The copied memory model was tested by sending the agent on a
pre-determined oubound route to a feeder (chosen at random, but consistent
between trials), copying the CPU4 state, and allowing the robot to navigate home
using the CX model; the CPU4 state will be modified by this homeward navigation.
The agent is then replaced at the feeder, its CPU4 state restored from the copy,
and tasked with navigating home a second time. The copied memory model was tested
as a pre-requisite for testing the CXMB model, however, it demonstrates an
important capability of the CX model; namely, the capability to directly load a
state into its memory in order to navigate; a concept referred to as
\textit{vector memory} in \cite{Webb2019}. Indeed, this concept of vector memory
is shown to be quite a useful tool in insect navigation \cite{Webb2019}. It is
worth noting that, the precise biological mechanism employed by insects to
perform this task, is unknown at time of writing. It would be neat to reuse
the CX circuitry, but this is an open question.
\newline
\par

The CXMB model is then tested in the same way. The agent follows its oubound
route to the feeder, navigates back once using the CX model (the MB model is
trained on this first inbound trip), is replaced at the
feeder, and finally, tasked with navigating back again, this time using the CXMB
model. To be clear, the CPU4 state of the CX model is stored after the outbound
route, and loaded back into the network before the second inbound trip.
\textit{Zhang} reports that the second inbound trip (using both CXMB) showed
more heading adjustments during its traversal, and, on average, performed
slightly better than a pure CX implementation\cite{Zhang2017}. It should also
be noted that the average performance of both models in \textit{Zhang's} work
was good \cite{Zhang2017}.
\newline
\par

We note two methodological problems with \textit{Zhang's} work: The outbound
route, while randomly chosen, was the same accross all trials, and, for the
copied memory and CXMB experiments, the agent was placed facing the nest for
the test run (second inbound). To add to the first problem, the outbound route
ended such that the robot was facing back towards the nest. Both of these flaws
could lead to apparent path integration behaviour where, in fact, the agent
could have made it sufficiently close to the nest by simply following a straight
line. While the agent was certainly employing the CX to perform path integration,
we do not feel that the tests chosen provide strong evidence of functionality.

\subsection{ Review of Part 1 }
The work in \cite{Mitchell2018} specifically covers the Mushroom Body
and Optical Flow Collision Avoidance. The Mushroom Body model used in
\cite{Mitchell2018} is the same as that used by \textit{Ardin et al.} in
\cite{Ardin2016}, except for the small differences noted in Section
\ref{MBBackground}. The model tested in \cite{Mitchell2018} used a single MBON
with a scan-based route following strategy. The scanning differed from previous
works as it was implemented as an image manipulation algorithm (termed
\textit{visual scanning}) instead of a physical turn performed by the AntBot.
\newline
\par

Multiple OFCA systems were tested, however, in final experiments an
optical flow filtering system is used (see Section
\ref{sec:offbg}). This strategy proved simple, but effective. While we
move away from it for the bulk of this project, it was used for
initial tests of the CX model, as it provided a simple out-of-the-box
method to generate a non-deterministic path for the model to
integrate.
\newline
\par

Both systems functioned well and provided a solid baseline from which we will
work in this project. The MB model proved very capable at following routes
learned in a non-deterministic fashion through a cluttered environment.

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PLATFORM                                                                    %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% High level description of the platform. This section can be short. Refer back
% to Part 1 for detail.
\section{ Platform } \label{sec:platform}
The AntBot is an autonomous robot built to allow easy testing of the
various navigational algorithms in the \textit{Ant Navigational Toolkit}
on a physical agent. AntBot is assembled using off-the-shelf parts with an
Android phone at its heart. We give a high level description here; additional
detail can be found in \cite{Mitchell2018}.

\subsection{ Hardware }
The robot is composed of three main parts: a Dangu Rover 5 chassis, an Arduino,
and a Google Nexus 5 Android smartphone. The smartphone provides a
(theoretically) solid base for an autonomous agent, with sufficient processing
power and memory to run the models, a convenient touchscreen interface to display
information and an adaptable control interface, a built-in camera, and
extensive software libraries. As such, the smartphone works as the computational
centre, performing all higher sensory processing and acting on this analysis.
Once the algorithm running on the phone has decided on a course of action, it
sends a command via a serial connection to the Arduino board; the board, then
parses the command and translates it into a sequence of motor commands which are
sent to the built-in motor board in the chassis.\newline\par
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.7\textwidth]{AntBotCharging}
  \caption{
    \label{fig:antbotcomp} The AntBot, connected to the charging station.
    This figure also shows the position of the mobile phone, camera attachment,
    and retroflective motion capture markers on the top of the robot.
    }
\end{figure}

Additionally, the robot possesses a $360^{\circ}$ camera attachment, giving the
robot a panoramic view to match that of the desert ant\footnote{In reality,
  the desert ant has a visual field of only ~$280^{\circ}$\cite{Ardin2016},
  however an approxmiation of $360^{\circ}$ was considered appropriate when
  the robot was built.}. The robot was also modified as part of
\cite{Mitchell2018} to allow it to be charged with an external charger without
dismantling it.

\begin{figure}[t!]
  \centering
  \begin{minipage}[t!]{0.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{KogetoDot}
    \caption{The Kogeto Dot 360$^\circ$ panoramic lens attachment.}
  \end{minipage}
  \hfill
  \begin{minipage}[t!]{0.45\textwidth}
    \centering
    \includegraphics[width=0.9\textwidth]{PanoramicView}
    \caption{A sample view of the lens from the front facing camera, before any
      processing.}
  \end{minipage}

\end{figure}

\subsection{ Software }
The robot requires two levels of software; higher level Java (Android) code to
perform the high level functions and lower level code written in C/C++
(Arduino dialect). The Arduino code can be thought of as the firmware.

\subsubsection{ Android } \label{subsubsec:droid}
Android can provide a nice basis for working on robotics projects. The
Operating System permits multiple applications to run asynchronously and
pass information between them; for example, the user could develop an application
to perform all visual processing then broadcast a processed image for other
applications which require it. The original AntBot software was split into
separate five applications: Ant Eye, Visual Navigation, Path Integration,
Combiner, and Serial Communications, neatly compartmentalising each task.
Unfortunately, in recent years, this structure was abandoned and almost all code
is contained within the \textit{AntEye} application. Only the Serial
Communications application remains true-to-design.
\newline
\par

Typical flow within AntEye will see an image captured from the front camera,
processed such that we get a downsampled 90x10 image which represents the
$360^{\circ}$ view around the robot. This image is then used for optical
flow analysis. The image and flow analysis are then made available to any thread
which requires it. A thread will then use this data to run an experiment.
\newline
\par

There are a lot of criticisms we can raise about the robot (in both a hardware
and software sense); ultimately the key problem may be lack of developer
familiarity with the Android platform.  While Android seems to work well at
first glance, it can ultimately be quite restrictive; we leave this to the
later discussion.

\subsubsection{ Arduino }
The Arduino code acts as a bridge between the Android platform and the Dangu
motor board, as there are no libraries which will directly connect the two.
The Android code contains a Command class which allows the user to insert robot
commands into the code; these are then transformed into serial messages by a
broadcast library \cite{Eberding2016} and sent to the Arduino. The Arduino code
contains a parser which will decode the serial messages and then call the
correct method to execute the desired motor commands.

\subsection{ Modifications and Development }\label{sec:mod}
The modifications or additions listed below are a small part of the dissertation,
however, for one reason or another (see Discussion) these ended up taking
most of the project time.\newline\par

\textbf{Compass Sensor:}
While the Android phone has a built-in compass (used for other parts of this
project) this is impractical to use as part of a control system. The
communications delay is too great to allow accurate feedback control.
As a step to introducing proportional control to the robot, we undertook an
investigation into available compass sensors. Multiple sensors and libraries
were tested and ultimately we settled on a Grove 6-Axis Compass \& Accelerometer.
This sensor remained reasonably accurate in when tested alongside motors (a
source of magnetic interference) and the library code was straightforward to
include. The sensor was installed by the Informatics Workshop. While we have not
used it for this project, we would encourage a future student to make use of
this.
\newline
\par

\textbf{Ocular Calibration:}
The $360^{\circ}$ camera attachment is held in place by a pressure sensitive
adhesive which allows the position to shift if the attachment is knocked or
removed. The lens was removed for use in another project and on re-attaching, it
was noticed that the image was warped, indicating that the pre-processing was not
working as required. The image pre-processing algorithm used a hard-coded pixel
value for the centre of the camera attachment which was no longer correct
(see Figure \ref{fig:centre}). We implemented a simple detection
algorithm using the Hough transform available in OpenCV to allow the user
to detect the new position of the camera attachment and set the centre
accordingly. The calibration is not performed automatically as the Hough
transform can be slow, and is highly dependent on lighting; instead, it
need only be done if the attachment is known to have moved.
\newline
\par


\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{Centre}
  \caption{\label{fig:centre} The hard-coded centre used for the polar transform
    (red), against the circle detected using the Hough transform (green).
   }
\end{figure}

\textbf{Code Refactor:} As mentioned above, all behavioural code for
the robot has moved into the AntEye application. In fact, most code
had been integrated into a single Android Activity (Java Class); this
file contained multiple long threads dictating behaviour as well as a
number of utility methods and robot commands. There were also a
ludicrous number of global variables. The resulting file was over 4000
lines long and almost unusable. Particuarly aggregious was the fact
that roughly half of the code was irrelevant to this project, but did
need archived for reference purposes. As such, it was decided that it
was worth refactoring the codebase in order to make it easier to
work. All utility functions were split into their own static Util
class, similarly, the robot control commands were moved to the Command
class. Runnable threads were moved into an archive package and split
into classes according to their use: Mushroom Body, Central Complex,
Optical Flow, or Old Navigation; that final group consists of the
oldest code which is not used explicitly but is useful for
reference. A single thread was left in the main Activity file to act
as a test thread. A lot of unused code was removed entirely.
\newline
\par

There are still many problems with the existing codebase. This simple refactor
ended up taking up a lot of time due to complications from working within the
Android framework; furthermore, many concessions had to be made for the same
reason.
\newline
\par

\textbf{Video Recording:} In order to analyse the visual processing in
more detail, a video recording utility was added to AntBot. Again
platform constraints mean that the utility is perhaps not as well-made
as it could be. A straightforward method using FFMPEG had to be
abandoned due to library conflicts; this method would have allowed a
video file to be recorded directly rather than the frame-stitching
solution described below. The real reason this proved difficult was
that the recording had to take place after preprocessing (image
unwrapping and downsampling to the 90x10 $360^{\circ}$ image). In
order to achieve this frames are passed off to a recording buffer
after processing, the buffer is then processed asynchronously by a
separate thread which saves the frames to a set directory. Pleasantly,
this does not impact the frame rate. The video frames then have to be
retrieved from the robot and stitched together using a custom python
utility (a simple wrapper around an FFMPEG command). The recorded
video can then be analysed offline. This utility was created to allow
offline analysis of optical flow, however, it also highlighted a major
problem with the robot which was not previously known.
\newline
\par

\textbf{Video Pre-processing Pipeline:}
When stitching the video frames from the recorder, it was noticed that the
framerate was not quite right. The framerate on the robot was found to be
an unacceptably low 2fps. In an effort to improve the framerate we isolated
different parts of the processing pipeline. We found the best possible
framerate to be approximately 14fps with no processing (this is computed simply
as the number of times the \textit{onCameraFrame()} callback function is called
per-second). Adding processing steps back to the pipeline, two steps were found
which caused the framerate to crash. The first step was an image convolution
algorithm; the unwrapped image does not line up with the direction of travel of
the agent, so the resulting frame must be shifted (azimuth) so that the centre of
the frame is in line with the forward direction. This step was reducing the
framerate by approximately 5fps. Replacing the implemented algorithm with an
OpenCV library implementation reduced this to approximately 1fps cost. Similarly,
an image downsampling algorithm had been manually implemented which cost around
5fps; again, replacing this with an OpenCV library implementation reduced the
cost to approximately 1fps, if that. The full pipeline can now run at 10-12fps
with the recording utility running on top. When simulating a model like the ECX,
the framerate does drop to 8-10fps. We also attempted to move the frame
processing into its own high-priority thread, however, this resulted in numerous
bugs and did not improve the framerate in any noticable way.
\newline
\par

This raises an interesting problem. We do not know when this code was
added to the robot, knowledge of the platform history would suggest it
was added as part of \cite{Scimeca2017} but we cannot be certain.  In
any project following its insertion, the framerate surely affected the
performance of the agent. We can say for sure that the algorithms used
and tested in \cite{Mitchell2018} were affected. Indeed, in
\cite{Mitchell2018} it is noted that few images were stored during
Visual Navigation experiments. The low storage rate was thought to be
a thread synchronisation issue, but it is now clear that the frames
were simply not finished processing in time to be stored.  In testing
the matched-filter collision avoidance system from \cite{Mitchell2018}
we note a significant change in behaviour after the framerate
improvement. It is suspected that this is due to an increase in flow
information, and therefore an increase in noise which can cause
erroneous and erratic behaviour, not witnessed in \cite{Mitchell2018}.
This is one of the main issues that resulted in our lack of results
for the ECX.
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% METHODS                                                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ Methods } \label{sec:methods}
\subsection{ Collision Avoidance }
% Discuss new method (eight directional FOEs)
The AntBot cannot be fitted with sensors which feedback to the higher models
(see Discussion). We therefore continue to look for a method of collision
avoidance which can be integrated into the Central Complex which uses the
available visual information. The matched-filter model from \cite{Mitchell2018}
performed reasonably consistently but was initially considered too coarse-grained
for integration into a modified CX. Our first approach therefore looked for
expansion patterns in certain areas of the flow field.

\subsubsection{ Shifting Expansion Fields }
We noted in research that the Focus of Expansion is used in many visual collision
avoidance systems. Usually it is used as a stepping stone to compute the
time-to-contact with an obstacle. Some papers describe using the FOE to determine
a turn direction in the event a collision is determined \cite{Stewart2010,
  Vanderstap2012}. We also note that the often the FOE seems to shift to the
deepest part of the image \cite{Vanderstap2012, Souhila2007} (though we should
also note that in \cite{Stewart2010} an avoidance saccade is triggered
\textit{away from} the FOE); this is not an empirical observation,
however, it was enough for us to want to test if the position of the
FOE was predictable given an approaching scene.
\newline
\par

If the FOE \textit{is} drawn to the deepest part of the image then its
location could be reliably used to choose a steering direction in a
collision avoidance system. Furthermore, this has an intuitive
integration into the CX. If we split the $360^{\circ}$ image seen by
AntBot into eight equal sections reflecting the eight cardinal
directions of the CX model, we can use the horizontal position of the
FOE as an input signal. The position would mark a ``bump'' of activity
in a set of eight neurons (similar to the output layer of the eight
MBON MB model). Ultimately we wish to see if the FOE will behave
predictably in the presence of a looming obstacle, which relies on
being able to compute the FOE reliably.  When working with the FOE,
\cite{Mitchell2018} notes difficulty in computing it
consistently. Often nonsensical values would be output by the
computation. This was suspected to be due to a technical detail not
known at the time of implementation and thus warranted further
investigation (see Section \ref{sec:mat}). Here, we managed to compute
reasonably sensible values, though they were not consistent or
predictable (meaning that the technical problems from
\cite{Mitchell2018} did indeed affect the results). To compute the FOE
we use the OpenCV \textit{calcOpticalFlowFarneback()} function to
produce a dense optical flow field then use the computation from
\cite{ODonovan2005} (see Section \ref{OFBackground}).
\newline
\par

While fixing the problems raised by \cite{Mitchell2018} made the computed FOE
more sensible in terms of general location, the lack of consistency was cause
for further investigation. For this purpose, the video recording tool described
in Section \ref{sec:mod} was developed with the goal of performing offline
video analysis of the agent's point of view. Thus, we can visualise the
computed optical flow field observed by the robot under controlled circumstances
(see Figure \ref{fig:flowfield}).\newline\par

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{frame_00066}
  \caption{\label{fig:flowfield}
    \textit{[DRAFT NOTE] I may remove this figure as it does not feel
    useful when Figure \ref{fig:foeframes} is shown later on.}
    A subset of points from the dense optical flow field observed by the agent.
    The agent is experiencing forward translational motion. These frames were
    captured after the framerate improvement from Section \ref{sec:mod}. The
    FOE is also shown in blue. There are no obstacles present in the arena.
  }
\end{figure}

This analysis produced the following observations:
\begin{enumerate}
\item{
  The optical flow field is incredibly noisy: The majority of the flow produced
  is erratic and unpredictable.
}

\item{
  The amount of perceived motion is tiny: In simply looking at the video it is
  incredibly difficult to determine how the motion observed relates to the
  movement experienced by the camera. In most frames the perceived
  flow is as shown in Figure \ref{fig:flowfield}, where each vector is
  only about 1 pixel in length.
}

\item{ The FOE jumps randomly between frames: The FOE is not
  consistent between frames though it can appear to stay in the same
  rough region (we do not think this indicates anything
  significant). We cannot rely on its location when presented with a
  looming object in the frame.  }

\item{ Looming objects can be identified from the flow field: This
  observation explains the good performance seen by
  \cite{Mitchell2018} when examining a matched-filter system. Looming
  objects can appear as a large (potentially single frame) disturbance
  of the low-level noise produced by the flow field.  }
\end{enumerate}

We can therefore conclude that optical flow cannot be used to compute
any fine-grain properties. The flow field at large is just noise, and
only drastic image changes result in a large disturbance in a specific
region of the flow field. While this effectively closes the lid on any
further investigation into the FOE/depth methods which utilise optical
flow, it does give us one semi-predictable property of the flow field
which can potentially be used.

\subsubsection{Matched Filters}
The matched-filter model of collision avoidance as seen in
\cite{Mitchell2018} was used without modification for the experimental
results in this paper.  However, in an effort to establish the ECX
model, we present here an adaptation of matched-filter collision
avoidance which can have its outputs integrated with the CX and MB
models. We will use the language of artificial neural networks, but
the principle is essentially the same as \cite{Mitchell2018}.
\newline\par

The neural model for collision avoidance features two types of neuron, ACC
(ACCumulation) and MRSP (Movement ReSPonse). ACC neurons are key in a working
system, so we discuss them first.
\newline\par

\textbf{ACC Neurons:}
ACC neurons take on the role of the leaky accumulators from \cite{Mitchell2018,
  Stewart2010}. The system contains two ACC neurons, ACC$_{left}$ and
ACC$_{right}$ for the left and right sides respectively. We investigate three
different representations of these neurons: Rate, Leaky Integrate \& Fire
(LI\&F), and Reset Integrate \& Fire (RI\&F). The two I\&F variants are only
subtly different and behaved largely the same but we include them for
completeness. We will go through them in chronological order.
\newline\par

The matched-filter system produces a sum for the left and right hand sides of the
robot. This sum denotes the difference between the expected motion and the
observed motion in the image frame. If we take the difference between the two
sums we can see if one side has experienced a significant deviation from the
expected motion (i.e. a looming obstacle on that side). This difference acts
as the input to the ACC neurons.
\newline\par

\textbf{ACC Neurons (LI\&F):} For both I\&F models, we need to set a
lower bound on what we consider an input.  Each input must be greater
than (absolute value) a lower threshold in an attempt to reduce noisy
inputs to the ACC neurons. For more details as to the output from
filter matching, please see \cite{Mitchell2018}. Once the input has
been thresholded, an ACC neuron at time $t$ can be described as:

\begin{equation}
  V(t) = \gamma \cdot V(t-1) + I_{ext}
\end{equation}

where $V(t)$ is the membrane potential, $t$ is the current timestep,
$\gamma$ is the leak current and $I_{ext}$ is the external current
flowing in - the thresholded difference of flow sums.  We use terms
like membrane potential and external current because these best
describe the function in the language of simulated neurons. It should
be noted by the reader that we do not explicitly model voltages and
currents in the model though this is just a case of semantics; these
properties could be modelled more explicitly.
\newline\par

We also define $V_{fire}$, the value at which the ACC neurons fire.
If $V(t) > V_{fire}$ for some ACC neuron, then the neuron that exceeded
$V_{fire}$ produces some output signal and both have their membrane potential
reset to zero (the resting potential). The neuron that fires then governs the
signal produced by the MRSP layer.
\newline\par

\textbf{ACC Neurons (RI\&F):}
The RI\&F representation is almost identical. The difference arises from the leak
model. In RI\&F we reset the neuron periodically (e.g. every third timestep).
While this is functionally an I\&F neuron, it does not function in any
biologically plausible manner. It is essentially the same as the leaky
accumulators from \cite{Mitchell2018}. We can describe the RI\&F neuron at time
$t$ by:

\begin{equation}
  V(t) =
  \begin{cases}
    V(t-1) + I_{ext} & \text{if } t \not\equiv 0 \bmod p \\
    0 & \text{otherwise}
  \end{cases}
\end{equation}

where $V(t)$, $I_{ext}$, and $t$ are as above and $p$ is the reset period (for
example, if $p = 3$ the membrane potential is reset every 3 timesteps). Firing
behavriour is the same as the LI\&F model.
\newline\par

\textbf{ACC Neurons (Rate):}
While I\&F neurons provide an intuitive neural metaphor for the leaky
accumulators, the CX model (into which this CA system is being built) uses
a rate model for all other neurons (including our own MRSP neurons). As such we
felt it appropriate to investigate such a representation for the ACC layer. In
fact, using a rate representation requires only a single ACC neuron and allows
for the generation of a steering response which is proportional to the size of
the optical flow disturbance (in theory, the bigger the obstacle or loom effect,
the bigger the response); neither of which are possible with the I\&F
representation. Recall, a firing rate given some input $I$ is represented as a
sigmoid:

\begin{equation*}
r = \frac{1}{1 + e^{-(aI - b)}}
\end{equation*}

where $a$ and $b$ determine the \textit{slope} and \textit{bias} of the function
respectively. If we set $a = 5$, $b = 0$ we get the rate function shown in Figure
\ref{fig:accrate}. Importantly, we must scale down the input (difference of flow
sums) such that it lies in (-1,1).\newline\par

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{ACCRate}
  \caption{\label{fig:accrate} Function of firing rate response of the single ACC
    neuron against difference input; sigmoid with $a = 5$, $b = 0$. The input to
  this neuron is scaled down to lie between -1 and 1.}
\end{figure}

If the flow disturbance observed on both sides is equal, then the rate
response should lie around 0.5. A looming object detected on the
right-hand side results in a negative input drawing the output closer
to 0. Conversly, a looming object detected on the left-hand side wil
result in a positive input, driving the output closer to 1. We have,
in a roundabout way, ended up with an optical-flow balance strategy -
a flawed, simplistic, yet not entirely implausible hypothesis for
collision avoidance in insects \cite{Julien2017}. If our output
remains close to 0.5 we need not react, close to 1 we must turn right,
and close to 0 we must turn left. Thus, with a single neuron we can
react to both scenarios; furthermore, the signal is proportional to
the disturbance observed and we can therefore scale our steering
command as necessary.
\newline\par

\textbf{MRSP Neurons}:
There are 16 MRSP neurons laid out in the same fashion as the CPU4 layer. The
MRSP neurons also utilise the same connection pattern as the CPU4-CPU1
connections. The MRSP neurons always use a rate representation. In all
cases the MRSP layer functions by generating a sine wave with the peak in the
desired direction of travel; the method for constructing and positioning this
sine wave is purely algorithmic, we did not investigate a way to model this
behaviour using neural connections.
\newline\par

We first create an array of eight evenly distributed points on a
shifted sine wave; the array is expanded to 16 elements once the
correct curve has been generated, similar to the way the 8 MBON signal
is expanded to 16 neurons for integration.  The curve is shifted by
$-\frac{\pi}{2}$ in the $x$-axis such that $\sin(\frac{\pi}{2})$ is in
the first element (this is simply housekeeping, it makes index
arithmetic easier when creating the output). We then compute an offset
based on the response from the ACC neurons. This offset will always be
an integer between -2 and 2 inclusive; it represents how far we wish
to shift the sine wave from the current direction read from the TB1
layer (how far to the left/right of our current direction do we wish
to turn). How this offset is generated differs between the I\&F and
rate models of the ACC layer.
\newline\par

In the I\&F case, the offset will be -2, 0, or 2 depending on which
ACC neuron fires (ACC$_{right}$, both/neither, or ACC$_{left}$
respectively). This is a coarse but functional reaction which
generates a saccade of approximately $30^{\circ}$ in either
direction. We included the case where both fire to act as a balance
case, however, in practice this never happens. This is functionally
identical to the behaviour displayed by the CA system in
\cite{Mitchell2018}, piped through the CX structure.
\newline\par

In the rate case, we instead receive a single input from the ACC layer.
This input will lie in (0,1) and we need only a simple linear transform to
choose an offset from this value. The transform has the conditions that
$f(0.5) = 0$, $f(1) = 2$, and $f(0) = -2$.

\begin{equation}
  \label{eq:transform}
 f(x) = 4x - 2
\end{equation}

satisfies these conditions. The output of Eq. \ref{eq:transform} is then rounded
to the nearest integer to determine the offset. This method is nice and simple,
however it is highly susceptible to noise, as we will see this is a critical
flaw.

%% STOP POINT
%% Describe the results of the offline analysis
%% Provide a figure showing the flow field.

\subsection{ Path Integration }
The path integration circuit in use is a slightly simplified version of that
presented in Section \ref{CXBackground}. This version does not model the
TN neurons or the Pontine neurons. The model is the same as that used in
\cite{Scimeca2017, Zhang2017}, and the robot-based tests from \cite{Stone2017}.
\newline\par

We use the CX firstly for a round of experiments in which we wish to address a
key methodological flaw observed in the experiments of \textit{Zhang}
\cite{Zhang2017}. We then use the CXMB variant (the 8-MBON model)
as the basis for the ECX model. \textit{Zhang} does not make it entirely clear
(though it is stated) that the 8-MBON model is actually an adapted CX model.
Distance measurements are arbitrary, there is no access to wheel encoder
measurements. Unlike previous works, the agent moves only in the forward
direction so we can reasonably assume that the robot will travel roughly the same
distance in a set time interval.

\subsection{ The Complete System}
Unfortunately, the complete ECX system envisioned was not fully realised. A
complete model has been constructed. Present on the robot is a singular model
which, in theory, will perform collision avoidance, path integration, and
direction-associated visual memory (simultaneously); however, we were
unsuccessful in performing any meaningful testing due to technical and time
constraints. We present the model here and present the issues experienced and
reasoning in Section \ref{sec:results}.
\newline\par

In an effort to keep the integration simple at a late stage in the project,
the first version of this model allowed the collision avoidance system to fully
overwrite any steering instruction in the event of a detected obstacle. While
straightforward and intuitive, this method proved far too coarse. If a full
model was to be constructed, it was clear that a full neural representation
for the collision avoidance system would need to be constructed; this and
tested variants have been described above.
\newline\par

The final model takes the 8-MBON CXMB model and inserts the additional ACC and
MRSP neurons required for the integrated collision avoidance system. The
connections between TB1 and MRSP are the same as the connections between TB1
and CPU4, likewise the connections between MRSP and CPU1 are the same as those
between CPU4 and CPU1. In fact, as the eight MBONs are expanded out to a
sixteen neuron representation (see Section \ref{CXMBBackground}) the
model can be visualised as the CX with two additional layers of sixteen neurons
between CPU4 and CPU1 plus one or two ACC neurons depending on the neural
representation used. The final version implemented on the robot used the rate
representation for a single ACC neuron.
\newline\par

The output of the network can be selected by simply changing the mode of the
CPU1 output stage. In this fashion, one can isolate each system and produce
the steering output using specific systems. The key modes currently present on
the agent are CA-only, and CXCA which are the isolated collision avoidance system
and the combined output of the path integrator and the collision avoidance
system respectively; adding further combinations should be trivial. The limited
selection currently available is due to the component level testing that
was in progress towards the end of the project. The weighting applied to each
layer can be specified and ideally it should be dynamic; we had intented to
approach this during the project but it was not possible. While the visual
memory infrastructure is entirely present, it was not tested; however, as stated
it is identical to that presented by \textit{Zhang} so we have no reason to
think that this subsystem should not work to the degree presented in
\cite{Zhang2017}.

\section{Experimentation and Testing}\label{sec:test}
We re-use the experimental environment from \cite{Mitchell2018}. A
collapsible boundary wall is constructed on the robot-football pitch
in G.17 in the Informatics Forum. Small ``tussocks'' are used as
objects in the environment (e.g. to avoid or to provide visual
information). A sequence of VICON motion-capture cameras are arranged
in a ring above the pitch and allow accurate tracking of the agent's
movement and heading.

\subsection{Path Integration}
The first round of tests aims to establish firmly that the CX model is capable of
robust path integration in the real world. The CX has undergone reasonably
extensive testing on the AntBot, however, two key issues were noted across all
tests.

\begin{enumerate}
\item{
  The outbound route of the agent is always the same. While this route
  was randomly chosen, it is entirely possible that any parameter tuning was
  performed with respect to this route. Good performance on a single route does
  not indicate good performance as a general path integrator.
}

\item{
  The outbound route always finishes with the agent directed at the nest. This
  means that the robot could potentially navigate home by making little to no
  change in its trajectory.
}
\end{enumerate}

The second problem is more of a symptom of the first; to remedy both problems we
need only fix the first. This can be done in a few of ways. Firstly, we could
simply generate a randomised turning command at set (or even random) intervals.
There are many variations on this theme, however, the original goal of this paper
was to provide a complete navigational system. It makes more sense for us to use
a collision avoidance system to generate the outbound routes.
\newline\par

We take the collision avoidance system from \cite{Mitchell2018} to fill this
role. For the standard CX experiments, this collision avoidance system was
entirely unmodified and separate from the path integration circuit. The outbound
route was governed by the CA system (with the path integrator updated on each
step) and the inbound route was governed entirely by the CX. These experiments
pre-date the neural collision avoidance system so there is no way to provide
CA and PI behaviour simulaneously. Thus, the experimental arena was carefully
chosen based on known properties of the CA system.
\newline
\par

For half of the runs the agent was started from the tape markings in
the South West corner of the arena and for the other half the agent
was started from the marking at the South end.  We place only two
tussocks in the South East corner of the arena to ensure the agent is
always directed into the arena. The full reasoning behind this
obstacle placement is given in \cite{Mitchell2018}; while navigation
towards the arena wall is still successful CA behaviour, it is not
useful for testing higher navigational systems. The rest of the arena
is left clear. The CA system from \cite{Mitchell2018} is sensitive
enough to provide non-deterministic reactions where no direct
obstacles are present whilst successfully avoiding the arena walls
(see CA experimental results from \cite{Mitchell2018}). We therefore
manage a non-deterministic outbound route based on CA while keeping
the arena clear for the PI stage of the test. These experiments were
conducted before the framerate improvement so CA behaviour can be
considered comparable to \cite{Mitchell2018}. Any test in which the CA
system failed catastrophically (direct collision) was to be ignored as
only the CX was to be tested, though there was only one such case.
\newline\par

The agent was permitted to make a $180^{\circ}$ (approx.) turn at the
end of the outbound route. In most cases, this does not make the robot
point directly towards the nest. The justification for this procedure
can be seen in Figure \ref{fig:cxnt}. The output from the CX model is
a left/right turning instruction with some strength; the greater then
angle between the current and home vector, the greater the output
signal in the required direction, however, the output is not the
precise angular correction required. As such, the agent required a large
turning arc (and a long time) to reorient itself with the home vector
before it could even begin the journey. Encouraging as this behaviour
is for the performance of the CX circuit, we felt it more practical to
avoid it for formal experiments.
\newline\par

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{CXNT}
  \caption{\label{fig:cxnt} An initial recording from the Central Complex
    PI experiments. The agent successfully navigates home but requires a large
    turning arc to point back towards the nest. This is because the output of the
    CX is not a precise angle but instead a turn direction with some strength.
    This behaviour was our prompt and justification to include an about turn on
    completion of the outbound route; the space present in the arena simply did
    not permit such routes for formal experiments.
  }
\end{figure}

Success was measured by the ability of the agent to touch the tape
square from which it started. This could be any part of the robot
(e.g. a tread running over the corner of the square). The agent was
stopped manually once either a success state was reached or a success
state could not be reached; for example, if the robot collided with
the arena wall during PI, or passed the start point without touching it.
\newline\par

Finally,  we should note that the locomotion of the robot is different to that of
previous tests. Previous works with the CX model look to keep the motion of the
agent continuous. The impracticality of such motion on the AntBot is noted in
\cite{Mitchell2018} and below in our own discussion. Instead, we keep the motion
in discrete steps; the agent moves forward, stops, turns on the spot, and
continues. In the case of the PI experiments, the oubound route is made up of
steps broken by turns generated from OFCA at irregular intervals, the inbound
route consists of steps broken by regular course correction turns (e.g. every two
seconds, the CX output is checked); this behaviour can be seen most
clearly in Figure \ref{fig:cxnt}. The distance measurement is still updated
continuously; we use time-travelled as the distance metric (assuming we
travel the same distance in the same amount of time). While this is generally
considered a bad way to measure distance, many problems are mitigated by the
fact the agent only moves in the forward direction, and to a degree this model
should be robust to \textit{rough} or \textit{noisy} inputs. We are pleased to
report that the CX performs well, even with a problematic distance metric.

\subsection{Collision Avoidance}
\subsubsection{Shifting Expansion Patterns}
The initial tests for the shifted expansion pattern system involved computing
a FOE and checking for its horizontal position in the image frame. Our
original hypothesis was that the FOE would be drawn
to the deepest part (frontal) of an image, and therefore its horizontal position
would dictate the desired direction of travel. We would, of course, take any
predictable behaviour which could be used for collision avoidance.
\newline\par

The agent was set on a collision course with a cluster of tussocks and watched
for consistent reaction behaviour. No such consistent behaviour was observed.
To test our hypothess completely, it was clear that further work was needed to
verify that there was no usable behaviour. The processed image frame (the view
of the agent) may be viewed on the smartphone screen; unfortunately, it is too
small to really pick out any details of a flow vector field. As a general point,
testing optical flow behaviour on the robot is generally quite difficult due to
the field of view. Even outputting an FOE location and moving an object in front
of the robot cannot tell us if we are producing the desired behaviour; firstly,
because the object will not move realistically in the frame; and secondly,
because the robot will observe motion from all directions which would not
normally be present. This is generally true with the robot; it is hard to produce
reliable output from algorithms when testing.
\newline\par

Two solutions to this problem have been developed over previous years:
\textit{LogToFileUtils} developed by \textit{Zhang} to provide textual logging
utilities where it was not normally possible to use such logs \cite{Zhang2017}
and \textit{StatFileUtils}, a variant of \textit{Zhang's} logging utility which
produced regular file output which could be easily parsed to produce graphical
plots from recorded data \cite{Mitchell2018}. Unfortunately, neither were really
useful in testing the FOE position; we could log it, but there would be no way
of correlating its behaviour with the agent's view of the environment. Instead,
we chose to implement a video recording tool which would allow us to record
controlled runs of the agent through an environment. Furthermore, this video
could be processed offline using the same systems implemented on the robot, then
scaled up to give a much clearer view of what the robot was actually seeing.
This proved incredibly useful and allowed us to almost immediately dismiss this
method for use on the AntBot.
\newline\par

The recording tool was used to show the infeasibility of this method
informally, however, we present below some frames from formal
recordings, taken to demonstrate our findings for the purpose of this
paper. Two such recordings were taken, both in the experimental
arena. The first was taken with no objects present, in the second, the
arena contained two objects staggered to the right and left of the
robot's path. These objects were not in the robot's immediate path, as
such, in both recordings the direction of motion was entirely
translational in the direction of view; no turns took place. The robot
was permitted to move from one end of the arena to the other without
interruption, aside from a manual stop which was done with minimal
disturbance. The arena can be seen in Figure \ref{fig:foetest}, the
robot's intended path is shown in red.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{FOETest}
  \caption{\label{fig:foetest} The non-empty arena used for testing
    the shifting expansion field system. Tussocks were set to the left
    and right of the robot's trajectory (shown in red) so that they
    were not on a collision course but should still affect the
    expansion field. We had hoped the FOE would either remain central
    or drift horizontally (in a consistent fashion) as each object was
    passed. Note that the agent was not started directly on the tape
    square, it was offset as shown by the path; this was oversight on
    our part.  }
\end{figure}

\subsubsection{Neural Matched Filters}
The testing for the neural matched filter system was far more
simple. In this case, we know that the underlying method (matched
filter collision avoidance) is sound, we just wish to translate it
into a neural representation. Thus, the first thing we need to test is
the ability to generate an arbitrary turn by inducing activity in the
MRSP layer of the model. We take the current direction from the TB1
layer, generate our sin curve and shift it to the left or right of the
current heading. We can then check if we can generate consistent
steering commands.
\newline
\par

We then test the three different implementations of the ACC neurons
separately. The testing for each type did not extend very far. Each
implementation was tested in a mostly clear arena with multiple
obstacles placed in a large, obvious cluster in the centre. The robot
was started from the starting block (marked with masking tape) on the
South side of the enclosure on a direct collision course with the
cluster. The agent would then attempt to traverse the arena without
collisions using only the neural CA system. The I\&F
variant\footnote{We take the LI\&F representation forward, observed
  behaviour between RI\&F and LI\&F was not significantly different.}
of the ACC neurons was the only implementation that functioned
consistently enough to allow limited testing of the ECX, however, we
later learned that the neural CA was not working to the degree
previously thought.
\newline\par

\subsection{ECX (CXCA)}
The testing methodology for the ECX was to test each individual system
and then full combination. Our earlier experiments with the CX
demonstrate the capability of the circuit as a path integrator - and,
of course, it has been demonstrated in simulation
\cite{Stone2017}. This model has already been tested in conjunction
with the MB (\textit{Zhang's} CXMB) \cite{Zhang2017}. Similarly, the MB
model has been shown to provide robust visual navigation on the robot
\cite{Mitchell2018}.\newline\par

Therefore, the only things left to test are the neural CA system, and
then each step of integration, CA $\rightarrow$ CXCA $\rightarrow$
ECX. Note that testing CA alongside the MB model would be redundant as
the routes recalled from visual memory should incorporate any obstacle
avoidance maneuvers \cite{Mitchell2018}. Fortunately, we can simply
use the full ECX model and turn particular outputs on or off depending
on the desired functionality.
\newline\par

Unfortunately, we did not advance past the CXCA testing stage. The
CXCA was tested only with the LI\&F ACC neurons. We used both an empty
environment (to check for basic functionality) and a lightly cluttered
environment; one with a singular central obstacle and a second with a
tussock density of 2 per pitch quadrant.
\newline\par

The informal tests were conducted by allowing the agent to navigate
outward using the neural CA system; the agent then attempted to
navigate home using a weighted combination of the CX and CA outputs
(CXCA). The reader should note that these informal tests were somewhat
intertwined with the neural CA tests. If the neural CA performed
correctly, then we felt it appropriate to allow the agent to attempt a
homeward navigation.
\newpage

\section{ Results } \label{sec:results}
\subsection{ Path Integration }
We are pleased to report good performance from the CX model. We
observed successful homing in 7/10 tests performed, though in all
cases the agent proceeded in the correct general direction. Of the
failures, two were minor and one was more significant. The full
results can be seen in Table \ref{tab:cxres}.
\newline
\par

\begin{table}[h!]
  \centering
\begin{tabular}{|c|l|l|l|}
\hline
\multicolumn{1}{|l|}{\textbf{Test}} & \textbf{Start Point} &
\textbf{Success/Failure} & \textbf{Distance from Start Point (mm)} \\ \hline
\textbf{AB\textunderscore CX\textunderscore 1}                          & SW                   & Success                  &  306                                  \\ \hline
\textbf{AB\textunderscore CX\textunderscore 2}                          & SW                   & Success                  &  348                                  \\ \hline
\textbf{AB\textunderscore CX\textunderscore 3}                          & SW                   & Failure                  &  517                                  \\ \hline
\textbf{AB\textunderscore CX\textunderscore 4}                          & SW                   & Success*                 &  325                                  \\ \hline
\textbf{AB\textunderscore CX\textunderscore 5}                          & SW                   & Failure                  &  581                                  \\ \hline
\textbf{AB\textunderscore CX\textunderscore 6}                          & S                    & Success                  &  356                                  \\ \hline
\textbf{AB\textunderscore CX\textunderscore 7}                          & S                    & Success                  &  304                                  \\ \hline
\textbf{AB\textunderscore CX\textunderscore 8}                          & S                    & Success                  &  296                                  \\ \hline
\textbf{AB\textunderscore CX\textunderscore 9}                          & S                    & Success                  &  311                                  \\ \hline
\textbf{AB\textunderscore CX\textunderscore 10}                         & S                    & Failure**                & 458                                   \\ \hline
\end{tabular}

\caption{\label{tab:cxres} The compiled results from the CX path
  integration experiments. (*) While this test succeeded, it was
  close.  The CA system failed at the last section of the outbound run
  which could be the cause of the near-failure. (**) We consider this
  the worst failure; while this run is not the furthest from its start
  point, it can be seen from Figure \ref{fig:abcx10i} that the agent
  actively deviates from its route.  }

\end{table}

VICON recordings from all tests are available in Appendix
\ref{ap:cxfigs}. While the results appeared good under observation and
in the figures, we must note the distance measurements in Table
\ref{tab:cxres}. These measurements are taken from the central axis of
the VICON skeleton applied to the robot, essentially, the central axis
of the robot. Due to the size of the robot, we must note that our
success criterion may be deceptive. The closest arrival at the nest is
29.6cm; while the robot did indeed make it back to its starting box,
this is still a reasonably large deviation. Given that the agent's
general direction is almost always correct (except Figure
\ref{fig:abcx10i}), we think it is fair to presume that the deviation
could be due to the inaccurate odometry; counting timesteps where an
agent is in motion is generally considered a crude and inaccurate
method of tracking distance travelled, we use this method as it was
all that was immediately available on the robot (see Discussion).
\newline\par

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_10}
  \caption{\label{fig:abcx10i} PI test AB\textunderscore
    CX\textunderscore 10. This was considered the worst failure of the
  system despite not having the greatest deviation from the start point.}
\end{figure}

An example of favourable behaviour (under experimental protocol) can
be seen in Figure \ref{fig:abcx8i}. In this case, the agent is already
close to its home vector but still corrects to achieve more accurate
homing. This is also the run which achieves the best performance by
the distance-to-start measurement.
\newline\par

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_8}
  \caption{\label{fig:abcx8i} PI test AB\textunderscore
    CX\textunderscore 8. Arguably the most successful recording from
    the formal experiments. The agent still corrects to achieve a
    better homing path, despite the fact it could likely have
    travelled in a straight line. }
\end{figure}

While we still consider these experiments a successful demonstration
of the capability of the CX (especially with the early test run in
Figure \ref{fig:cxnt}), we must acknowledge certain methodological
issues. Firstly, the deviation and secondly the run length. All runs
can be seen to be very short. We think it would be prudent to repeat
these experiments with longer runs and a platform with better
odometric capabilities (see Discussion). Despite our justification, it
may also be prudent to remove the 180$^{\circ}$ turn at the end of the
outbound run. In some recordings, it can be seen that the robot is
still pointing directly home after its outbound run (Figure
\ref{fig:abcx1i}), though this is not always the case.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_1}
  \caption{\label{fig:abcx1i} PI test AB\textunderscore
    CX\textunderscore 1. In this case, we note the exact same
    experimental flaw we sought to avoid in which the robot requires
    no correction to home successfully.}
\end{figure}

\subsection{Collision Avoidance}
\subsubsection{Shifting Expansion Patterns}
The optical flow experiments conducted allow us to state with a high
degree of confidence that the FOE cannot reliably be used to generate
a steering command, in the case of the AntBot. We limit our claims as:

\begin{enumerate}
\item{This is the only platform on which testing has taken place.}
\item{It cannot be denied that the works which inspired this method
    achieved success in using the FOE to direct motion
    \cite{Stewart2010, Vanderstap2012}.}
\end{enumerate}

In light of this, we conclude that it is likely that the AntBot's unique
360$^\circ$ field of view and low resolution vision play a major role
in the problems experienced. For an example, we turn to the five image
frames presented in Figure \ref{fig:foeframes}.
\newline\par

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{frame_00057}
  \includegraphics[width=\textwidth]{frame_00058}
  \includegraphics[width=\textwidth]{frame_00059}
  \includegraphics[width=\textwidth]{frame_00060}
  \includegraphics[width=\textwidth]{frame_00061}
  \caption{\label{fig:foeframes} Five consecutive image frames with
    optical flow information superimposed over the top. It can be seen
    that the FOE position (central pixel of the blue cross) is not
    consistent across multiple frames. This figure also shows the
    disturbance caused by approaching a tussock on the right hand side
    (3rd and 4th frames). Video captured at approximately 10fps
    (i.e. these images were captured accross roughly 0.5 seconds).
  }
\end{figure}

These five frames show how the FOE tracks across the azimuth in the
course of approximately half a second. This certainly allows us to say
that the FOE is not drawn to the deepest part of the image frame,
which should be centre to centre-left across all five frames. These
frames would also indicate that the FOE position is not predictable
given the motion experienced - it is in fact more accurate to say, the
fine motion perceived is not predictable. To show this more concretely
we can look to another example in Figure \ref{fig:foeframesempty}.
\newline\par

\begin{figure}[h!]
  \centering
  \includegraphics[width=\textwidth]{frame_00064}
  \includegraphics[width=\textwidth]{frame_00065}
  \includegraphics[width=\textwidth]{frame_00066}
  \includegraphics[width=\textwidth]{frame_00067}
  \includegraphics[width=\textwidth]{frame_00068}
  \caption{\label{fig:foeframesempty} Another set of consecutive
    frames, this time captured in an environment with no objects
    present. The FOE is still unpredictable, though it does not move
    as drastically in the horizontal axis.
  }
\end{figure}

In this case there are no obstacles present in the arena, and thus
there should be minimal sources flow field disturbance. Nevertheless,
we can still see that the FOE jumps, though not quite as drastically.
The views are perhaps deceptive but for clarity we note that each
pixel represents 4$^\circ$ of horizontal arc. Even small FOE jumps in
the image frame are large with respect to the world.\newline\par

While it would be interesting to test empirically such use of the FOE
in a different setting, we can conclude that the FOE may not be used
to govern any control output on the AntBot; indeed, we would go so far
as to conclude that the FOE is of no use to the AntBot at all.
\newline\par

Going forward, we have no further formal results to present. We will
instead discuss behaviour experienced during developmental
testing. These are not discussions from which concrete conclusions may
be drawn, however, they may provide insight for anyone considering
implementing the neural filter model for collision avoidance, or the
ECX, as a continuation of this work.

\subsubsection{Neural Matched Filters}
The three variants of the neural filter model involve using the
different representations of the ACC neurons. In testing, the I\&F
versions performed identically under observation, so we will discuss
them as one.\newline\par

\textbf{I\&F:} Unfortunately, the performance was generally poor,
though this was the only variant which managed successful collision
free runs (not consistent enough to gather any formal
data). It is no surprise that this variant came closest to replicating
the behaviour (if not the performance) of the original model from
\cite{Mitchell2018} as it is, functionally, a dressed up version of
this model (with the addition of neural steering generation).
\newline\par

With this in mind, one could reasonably ask why this model does not
function consistently where the one from \cite{Mitchell2018}
did. Indeed, we used the model from \cite{Mitchell2018} successfully
in this work for our path integration experiments (and it was tested
successfully alongside the MB model in a cluttered environment during
the code refactor process). The neural OFCA model was implemented
after the improvements to the visual pipeline; as this was the only
change, we suspect this is the problem. In testing the original
model from \cite{Mitchell2018} \textit{after} the improvement to the
framerate, the behaviour was noticably different (and degraded).
\newline\par

As mentioned in Section \ref{sec:mod} we suspect this is due to the
drastic increase in flow information which is now fed into the
model. As can be seen from Figures \ref{fig:foeframes} and
\ref{fig:foeframesempty}, the flow information is relatively noisy
with the features indicitive of an obstacle being large and
obvious. The tuning methods applied in \cite{Mitchell2018} where the
accumulation and reaction thresholds are adjusted are clearly no
longer sufficient; in testing, the model was either hypersensitive or
hyposensitive. The line for a functional middle ground is clearly very
fine. Tuning at the sensory level is required; fortunately, such
tuning could also prove useful for the rate model.
\newline\par

\textbf{Rate:} The rate model provided no predictable reaction,
however, this is very likely due to the fact that the inputs are not
filtered at all. The flow sums for each side are computed, adjusted,
and their full difference is supplied as input to compute the firing
rate. In the I\&F and algorithmic variants, this problem is (or was)
mitigated by the accumulation threshold parameter. It is clear that
such a parameter could be added to the rate model, however, we propose
a solution at a lower level.\newline\par

One should look to clean the optical flow field in some way. This
could be done simply by filtering out all vectors which do not meet
some threshold for length (this is distinctly different from the
thresholds applied in \cite{Mitchell2018}). A crude but potentially
effective method as it can be seen in Figure \ref{fig:foeframes} that
looming obstacles result in a few vectors with large displacements;
all other information could be considered noise. Alternatively,
different filters\footnote{The flow filter in use is still the filter
  implemented by \textit{Scimeca} for speed
  retrieval\cite{Scimeca2017,Mitchell2018}. } or even a dedicated
sensor could be used (see Discussion).\newline\par

Despite the fact that the rate model as presented does not behave
consistently in the presence of obstacles, it would still be our
preferred approach going forward. We feel it better fits the existing
CX architecture and also allows a proportional response to be
generated. It is a more flexible and perhaps, a more biologically
plausible approach to the simulation. Based on our previous experience
with optical flow models for collision avoidance we are confident in
our assertion that the erratic behaviour can be (mostly) explained by
a noisy optical flow field though we should reiterate that this is
conjecture.

\subsection{ECX (CXCA)}
As stated in Section \ref{sec:test}, the ECX model only got as far as
the CXCA (Central Complex with Collision Avoidance) step in testing,
using the I\&F CA model. This parameterisation of the CA model was
consistent enough to allow specific tests in tailored environments
such that we could at least check that steering signals were being
generated as a combination of the path integrator and the collision
avoidance system. As such we could observe (mostly) successful
collision avoidance behaviour on the outbound run. The inbound runs
proved less successful, however. Due to the hypersensitivity of the
collision avoidance system it proved difficult to find a reasonable
weighting to balance the two systems. It was when attempting runs in a
moderately cluttered environment that we discovered the CA was not
working to the limited degree previously thought.
\newline\par

Intuition would suggest that we generate the vast majority of the
steering signal from collision avoidance; any steering signal
generated by this system is clearly an immediate danger to the agent,
the path integration circuit can compensate later. However, under
observation the path integrator never gets the chance to
compensate. An early prototype of the CACX integration allowed the CA
subsystem to completely overwrite the steering command in the event of
a collision detection; with a hypersensitive CA system this presents
the obvious problem that CA saccades become the only steering output.
Later versions which allowed integration of the outputs experienced
the same problem; lowering the precedence of CA either did not
mitigate the problem or resulted in an unacceptable degradation of an
already poor CA system. We can at least conclude that the CXCA and
therefore the complete ECX are plausible once the problems are ironed
out of the CA system. We regret not making more progress towards
making this a usable model, though a complete implementation is now
available on the AntBot.
\newpage


\section{ Discussion and Future Work}
\subsection{Conclusion}
We have presented here an investigation into a unified navigational
base model, inspired by the insect brain. We show experimental results
demonstrating the capability of the Central Complex model as a path
integration circuit over random outbound routes; we also note that
this good performance was achieved with what is generally considered
to be a poor odometric measurement, further showing the strength of
the model. To complete the navigational system, we investigate and
give evidence against a collision avoidance model based on the focus
of expansion of an optical flow field; furthermore, we present a
neural adaptation of the matched filter collision avoidance system
from \cite{Mitchell2018}. This neural adaptation investigates three
different neuronal representations resulting in two different
architectures; this neural matched filter model has been integrated
into \textit{Zhang's} CXMB model to provide a conceptually complete
base model for insect navigation. The code for both implementations
has been added to the AntBot, but currently in use is the rate-based
version.\newline\par

We also provide several technical advancements to the robot which
address many of the problems we experienced through
\cite{Mitchell2018} (MInf Part 1) and this work. This includes a code refactor to
clean up the existing codebase, the addition of a compass sensor at
the Arduino level, a calibration system to auto-detect the position of
the 360$^\circ$ camera attachment, construction of a video recording
tool to record the agent's point of view during tests, and finally,
the identification and correction of a fairly major bug in the visual
pre-processing system which is now known to have affected past works.

\subsection{Future work}
\subsubsection{Building upon the work presented}
While we are ultimately pleased with the progress made in the
development of AntBot, there are undoubtedly things which could be
improved.\newline\par

\textbf{CX Experiments:} The experiments presented undoubtedly show
that the CX model functions well as a path integrator, however, there
are some issues with our methodology. Firstly, the outward routes are
quite short and not as varied as we had hoped. Experiments
demonstrating the network's capabilities over a longer, more varied
(but still non-deterministic) route would be of value. Our routes were
restricted by the experimental enclosure, which in-turn, is restricted
by the size of the pitch in IF G.17. There is no reason a larger
enclosure (which is still viewable by the VICON system) could not be
built, we simply used materials which were immediately available
during \cite{Mitchell2018}. Alternatively a smaller robot may be used.
\newline\par

While the enclosure itself may seem like the limitation here, we think
it is valuable in terms of providing some degree of control to the
visual scene experienced during experiments. As such, we would
recommend the continued use of some enclosing wall. We would also
recommend continued use of the robocup pitch as it is lit
consistently, is equipped with VICON, and is self-contained within a
secure lab (meaning that it will not become unexpectedly unavailable).
\newline\par

\textbf{Neural Matched Filters:} We do not present any formal results
for the neural matched filter system as we encountered unforseen
difficulties after fixing the framerate bug. However, we are convinced
that this sytem is conceptually sound. We would encourage others to
work with the rate-based system, as, after development, this seems the
most intuitive, efficient, and useful representation. As a start, we
would suggest looking at the flow vectors produced during the filter
matching stage (\textit{filterCollisionAvoidance()} in
MainActivity.java) and looking for a way to filter out low level
noise in the flow sum calculation. Furthermore, alternate flow filters
could be investigated.
\newline\par

\textbf{Extended Central Complex:} Functioning neural matched
filtering (or an algorithmic alternative) is a pre-requisite, however,
we would strongly encourage future work on this model. If some
reliable collision detection input can be provided (note this need not
be based on optical flow), then the ECX could be comprehensively
tested. Our original vision for this was a multi-stage navigational
exercise; the robot performs an outward trip using CA alone, homeward
using CACX (during which a visual route is learned), then outward once
more using either visual information or an inverted home vector in
CACX to learn a visual outbound route. On completion of the second
outbound trip, the robot should have all of the information it
requires to make the trip reliably, from visual memory. \newline\par

To apply the different systems dynamically, they must be weighted at
each stage with the initial goal of gradually reducing the reliance on
collision avoidance and path integration, using vision almost
exclusively. A further advancement could see a system which can place
some \textit{confidence} in the output from each system, thereby
choosing the one which gave the best chance of successful navigation.
\newline\par

\subsubsection{Platform}\label{sec:disc:platform}
\textbf{Software:} While our code refactor was extensive, there are
still aspects of the software which could be fixed. There are still
many varibles which are global to the application (referenced via the
MainActivity class). This is not only bad practice in a software
engineering sense, but it makes tracing variable updates needlessly
difficult. Some such variables have been moved where possible but
there is still a lot which could be fixed.\newline\par

Most of the unused code which is still present is commented and/or
specifically marked for archival purposes. We would recommend
removal of any and all remaining dead code which has been
missed. Likewise, many unused variables were removed but some remain;
use great caution when removing these! Code which seems unused may be
used indirectly (even if you search for references and find none), use
all tools afforded by Android Studio to ensure nothing
breaks and update version control excessively.
\newline\par

Library upticks may be useful, but we are not sure of the extent to
which this may be done. We experienced difficulties in using some
libraries during the development of the recording tool due to a
version conflict between JavaCV (which contained required FFMPEG
bindings) and the version of OpenCV present on the robot (3.0.0,
compiled from source). We also experienced problems when investigating
the few possible alternatives as the minimum Android SDK version for
the libraries was greater than the target for the project; to
complicate matters further, a later version of the Android SDK was
incompatible with the Google Nexus 5 platform. The project and
dependency versioning and package management is a bit of a mess and
could do with being cleaned up where possible. A good start point
would be to move the project away from the source version of OpenCV
and onto an appropriate Maven\footnote{Maven - The Android Studio
  package manager.} package. This would, however, require updating
many legacy imports and changing (or removing\footnote{We think this
  is inadvisable.}) a lot of legacy code.\newline\par

\textbf{Hardware/Firmware:}
Potential upgrades to hardware are extremely limited. During this
project, we note the difficulty in adding a single compass sensor to
the robot, with the goal of later adding proportional control
code. The Arduino has a shield board connected which governs all of
the connections to the motor board. This board occupies the connectors
usually used for the I$^2$C bus, however, it is possible (to a limited
extent) to piggy-back on the shield connections. A better solution
which we are advised could be implemented by the Informatics workshop,
would be to have a different shield made up with provisions for a
break-out board which would permit easier addition and testing of
different sensory systems.\newline\par

At one point in the project, we considered adding an ultrasonic sensor
array to the front of the robot (three sensors, mounted in an arc on a
custom 3D printed frame). The technical implications were too great
for the late stage at which this was considered. However, if a future
student could delve into the serial broadcast library and learn how to
send serial messages from the Arduino to the smartphone this could be
of great benefit. Firstly, the chassis has built-in encoders which
could be queried to provide more accurate odometric measurements to
higher models; furthermore, this could perhaps be used to introduce an
algorithmic alternative to the matched filter model for collision
avoidance, which could be useful in testing higher models (e.g. the
ECX) where visual collision avoidance may be unreliable.


%% This section will largely delve personal opinion and feelings towards
%% the work completed. Ironically, the word that comes to mind when
%% thinking on this year's work is: incomplete. While we have presented
%% and implemented the ECX, a complete theoretical base model for insect
%% navigation, we did not manage to gather and present any formal
%% experimental data. The only real barrier to testing towards the end of
%% the project was the tuning of the collision avoidance system,
%% something not predicted to be problematic as we already had a working
%% system from \cite{Mitchell2018}. This became a problem as the work
%% completed in \cite{Mitchell2018} was completed under the assumption
%% that the robot's basic systems functioned sufficiently well that they
%% need not be changed. This assumption was seriously flawed (as noted
%% towards the end of \cite{Mitchell2018}), but such an assumption is
%% inherent in a generational project or platform such as the development
%% of AntBot.
%% \newline\par

%% It is our view that the AntBot should be retired. We believe that the
%% platform has fulfilled its usefulness and it would be worth having a
%% new platform constructed, perhaps as part of a new project. There are
%% certainly still tests which could be conducted on the AntBot; a
%% Mushroom Body circuit with real-valued weightings could be tested, the
%% ECX could be tested if the CA system could be tuned, in fact, likely
%% all of the work from \cite{Mitchell2018} could now be meaningfully
%% repeated. Something as simple as a framerate change could be enough to
%% effect significant change in the MB circuit performance; we have
%% already reported the changes seen in the OFCA sytem. However, the
%% benefit of having the AntBot available to test such system is vastly
%% outweighed by the potential technial difficulties the researcher could
%% experience; it is not an exaggeration to say that we lost weeks worth
%% of work due to ultimately simple problems. We justify our
%% recommendation by reporting our own experiences from
%% \cite{Mitchell2018} and this work.
%% \newline\par

%% %% \textbf{Battery life:} A simple yet perpetually irritating issue. As
%% %% the smartphone screen is constantly on during testing and
%% %% experimentation, the battery life is understandably poor. This is not
%% %% helped by the fact that this battery has been continuously
%% %% charged/discharged for short periods of time as part of the
%% %% development cycle over multiple years; this charging behaviour is
%% %% known to cause battery life to degrade.

%% \subsection{Matrix behaviour}\label{sec:mat}
%% During \cite{Mitchell2018}, we noted that
%% one of the main issues we experienced while implementing visual
%% scanning was matrix manipulation (shifting rows and columns). When
%% checking the manipulation alone, it functioned perfectly fine,
%% however, when moving into its own function problems arose. As it
%% happens, OpenCV matrices are represented by a wrapper object in the
%% Java bindings which essentially acts as a pointer. When treating
%% parameter passing as a reference, suddenly functions involving
%% matrices began to work as expected. This is a technical detail, it is
%% available in the documentation (albeit hard to find), why is it worth
%% mentioning?\newline\par

%% It is worth mentioning as neither of our direct
%% predecessors noticed this (as we understand it, the code was changed
%% significantly after \textit{Eberding's} project so we cannot include
%% him). As a result, some truly ridiculous code has wormed its way into
%% the robot over the years. During our code refactor as part of this
%% project, we found numerous unused functions which treated matrices
%% by-value, where identical code suspiciously appeared in the visual
%% pre-processing pipeline. It was clear that previous students had
%% attempted to give each stage its own function, but this did not work
%% and all code was moved directly into, frankly, a monster of a callback
%% function - called for every camera frame.
%% \newline\par

%% This is understandable. During a masters or honours project, technical
%% hiccups like this are easier to bypass than to solve. Time is tight
%% and there are ``more important'' things to worry about. Each student
%% need only worry about the codebase for one academic year; each student
%% going out does not worry about the code they leave behind and each
%% student going in is told they have a working platform and codebase on
%% which to test their models. Clearly there is some disconnect in these
%% attitudes. Please do not think that we stand upon the mountain top
%% bemoaning the the sins of our predecessors; we are by no means
%% innocent in this department. The code refactor undertaken is an
%% excellent example; it made development far easier for the project, but
%% there were design decisions taken that make no sense in a general
%% software engineering context, but perfect sense if you are experienced
%% with the AntBot. It is inevitable that every student will have to deal
%% with the mess of the previous and ultimately contribute their own. We
%% use the example of matrix manipulation to make this more general
%% point.
%% \newline\par

%% This is perhaps not specific to the AntBot, any generational project
%% would suffer from the same issue. Methods of mitigating this effect
%% simply add more work to the student who must document and justify all
%% coding decisions made, limiting the already limited time they have to
%% work on the biorobotic component. We believe that the AntBot is on the
%% verge of becoming unusable.

%% \subsection{Lack of documentation} The only documentation on the platform
%% is available in the form of previous dissertations. These do not go
%% into sufficient detail to explain the function and usage of each
%% section of the code (as technical documentation should). This means
%% that some potential functionality is near impossible to understand.
%% Again, a specific example; we noted earlier that encoders were not
%% available to provide feedback for odometry. Actually, it could very
%% well be possible to get collect encoder information and send it from
%% the Arduino layer to the smartphone. We do not know, because the
%% serial communications system is not documented, the path taken by
%% serial information is incredibly difficult to follow from the code
%% alone. While there are examples to follow for sending data \textit{to}
%% the Arduino and decoding it, there are no examples for encoding data
%% and sending it \textit{from} the Arduino. It was decided that it was
%% not worth the time required to understand and we would default to
%% odometry based on time (as in previous iterations of the project) so
%% as to collect some data on the CX functionality. In fact, the encoders
%% have not been used since \textit{Eberding's} original work, where they
%% were used only for a specific purpose. This is unfortunate considering
%% the encoder capability of the Rover 5 chassis is given as one of the
%% main reasons for moving away from Roboant \cite{Eberding2016} (perhaps
%% made worse by the fact that optical encoders \textit{are} available
%% for the Polulu Zumo motor board used by Roboant).While we have
%% successful results to present, it is undeniable that the decision not
%% to use encoders likely affected the performance of the model. Encoders
%% are not the most accurate odometry technology, but they would provide
%% a more accurate measure than time - especially on AntBot, where
%% threads are constantly put to sleep to permit time for serial
%% communications.
%% \newline\par

%% \subsection{Flimsy construction} When we inherited the project in late 2017, the
%% robot chassis was not in one piece; the top plates were not attached
%% as students required access to the battery connection to turn the
%% robot on or off and remove the batteries for charging. This was solved
%% as part of \cite{Mitchell2018} but it should never have been an issue
%% with a pre-existing platform.
%% \newline\par

%% When returning this year for this project, the 360$^\circ$ camera
%% attachment had been removed and used for another project. It was
%% reattached and work commenced. We noticed that the CA system was not
%% functioning as we expected during early tests of component systems for
%% the ECX. The system constantly reacted in the same direction,
%% regardless of the stimulus present. Initially we thought this was due
%% to the code refactor (the code, but not the logic, was changed), then
%% due to parameter tuning, and finally we noticed that the processed
%% image was warped. This warp left a large blind spot on one side of the
%% image which explained the behaviour. Previously this warp had only
%% been experienced in cases where part of the lens attachment was
%% obstructed, however, this time the full view was available. This
%% resulted in at least two weeks of breaking down the image processing
%% and unwrap process before finally tracing the issue. Namely, that the
%% centre of the circle used for the polar transform which unwraps the
%% image was hard coded to a specific pixel value. Despite the fact that
%% we were specifically informed, and legacy applications demonstrate,
%% that there was an automatic detection method in place. At least two
%% weeks of available time lost (after finding the image warp) to
%% something that should have been a non-issue.
%% \newline\par

%% \subsection{Limitations imposed by software (Android)}
%% \subsubsection{Library conflicts and limitations}
%% The existing hardware is reaching the end of its life. Specifically,
%% the smartphone is out of date; while mid-tier modern phones could
%% provide better sensors (e.g. compass), faster communication protocols,
%% or faster processing speeds, arguably the biggest limitation of the
%% smartphone is now becoming the software as a result of outdated (and
%% unsupported) hardware. \newline\par

%% This was most prominent when developing the video recording tool. To
%% record a video directly required use of an FFMPEG wrapper, available
%% via a specific library (JavaCV). This library also included OpenCV
%% bindings which led to conflicts with a system which already included
%% an OpenCV library (in a non-standard way, see Assumed
%% Functionality). Furthermore, the OpenCV portion could not be
%% excluded. Switching to the JavaCV library would have required
%% rewriting much of the existing OpenCV imports and code.
%% \newline\par

%% One other potential avenue existed in importing the required libraries
%% independently of the JavaCV library, however, the minimum SDK version
%% required for at least one of the components was higher than the
%% project target which could not be increased because it is not
%% supported on the version of Android installed on the
%% smartphone. Therefore, we ended up with the implemented hack solution
%% of buffering video frames to be stored as individual images which are
%% copied from the robot and stitched together by a python script (using
%% FFMPEG). What we end up with is a massively roundabout way of
%% accomplishing the same thing by using the same technologies in
%% different ways.
%% \newline\par

%% \subsubsection{File management}
%% Small gripe, but retrieving files and file management in general
%% can be convoluted. As an example, the videoframes are stored to a
%% particular directory, this directory is deleted and reformed for every
%% recording to ensure that the old frames are always overwritten,
%% however, this does not happen consistently; why, is a mystery. Often
%% files written during the last run cannot be seen when the phone is
%% plugged into a computer, the user must go into the file manager on the
%% phone and copy the files to a higher directory for them to become
%% visible. This was experienced in \cite{Mitchell2018} when using
%% \textit{StatFileUtils}, but only when dealing with the video frames
%% this year did we note just how ridiculous this process is. The only
%% way to guarantee your video data will be recorded and available is to
%% go into the file manager on the phone, delete the destination (and any
%% copies made in an attempt to read it), run the agent, go back into the
%% file manager, copy the destination directory (usually to the top
%% directory on the SD card). In writing this it occurs that a solution
%% could be to install a Secure Shell (SSH) server on the smartphone to
%% make file transfer easier, but this then requires further development
%% and testing for something that should be available by default.

%% \subsubsection{Software structure restrictions}
%% Android applications have a reasonably strict structure. Certain
%% things, for example scoping rules for specific variables can be
%% enforced unexpectedly in ways which only make sense within the Android
%% ecosystem. When undertaking the code refactor we discovered reasonable
%% explanations as to why everything had been kept in the same file. Our
%% experience of this is minimal but it causes issues when trying to move
%% code around in \textit{traditional} ways. As an example, consider
%% global variables; generally considered bad practice unless absolutely
%% necessary, but, if we want to display their value on-screen then they
%% are absolutely necessary. All variables to be displayed must be global
%% for reasons we admit we do not totally understand. In many cases these
%% are variables modified in multiple places, at multiple stages, in
%% multiple threads (though in theory, not simultaneously). This is a
%% small example but application structure cropped up a few times in
%% different ways. In another case, a function required a Context object
%% which was not available until particular point in application
%% initialisation.
%% \newline\par

%% This is, again, a small problem but it can result in roundabout
%% solutions to problems which would be simple under normal
%% circumstances.

%% \subsubsection{Overhead}
%% The overhead required for learning enough about the Android ecosystem
%% to successfully develop within it is massive. It is no surprise that
%% the original application structure from \cite{Eberding2016} was almost
%% immediately dropped, as this would require learning how applications
%% in Android interact to pass data back and forth. There may also have
%% been latency involved in inter-application communications but no
%% previous works have reported this as a reason for moving away from the
%% original application structure.\newline\par

%% While it is not unreasonable to ask prospective students to learn
%% about Android development, asking them to interpret the existing
%% application structure and legacy code without any documentation, is.
%% While the obvious solution is for a student to reach out to the last
%% \textit{caretaker} of the project, it is also reasonable to expect a
%% student to simply look for the place in the code where they can insert
%% their own, and then build everything around that. It is no surprise
%% that this location was found to be the AntEye application as all
%% visual information is immdiately available.
%% \newline\par

%% This raises a problem in that there is still legacy code that must be
%% running alongside AntEye. An Android Toast appears when starting any
%% thread which states ``Visual Navigation service started.'', and
%% another which appears when stopping a thread to inform you the service
%% has stopped. After multiple years of development it can be almost
%% impossible to figure out what is running where and doing what outside
%% of the small bubble provided by AntEye. Thus, not only does a future
%% student have to understand Android, they have to be able to notice and
%% understand the workarounds applied by previous years (we will bring it
%% up again, with no documentation).

%% \subsection{Limitations of hardware}
%% The construction of the robot further limits development. It is
%% impossible to use any useful form of feedback control due to the
%% communication latency between the smartphone and Arduino
%% \cite{Mitchell2018}. Movements are therefore coarse and
%% inconsistent. Feedback control could perhaps be implemented at the
%% firmware level, but attaching sensors to make this possible is
%% difficult (a compass sensor was added as part of this project, but
%% getting it to show up on the I$^2$C bus proved difficult). This would
%% futher reqire re-writing most of the firmware on the robot (again this
%% firmware is uncommented and undocumented) to implement control adding
%% more busy work to a potential project. It was actually planned that
%% the control code would be re-written as part of this project but this
%% gradually slid further down the list of priorities as difficulties
%% were encountered with higher level systems.
%% \newline\par

%% It could also be argued that implementing proportional control at the
%% level of the firmware is not useful enough to warrant doing in the
%% first place. The motion of the agent would still need to be performed
%% in the same discrete steps. The only potential gain would be more
%% accurate turns which, given the visual scanning implemented for the MB
%% circuit and the lack of scanning in the CXMB/ECX variants, would be of
%% minimal importance.
%% \newline\par

%% While it is difficult to add sensors to the Arduino, it is impossible
%% to fit additional sensors to the phone. Dedicated optical flow sensors
%% could provide more useful input than the field computed from the field
%% of view, however, they cannot be added.
%% \newline\par

%% Already mentioned are the numerous delays required to execute
%% commands. Stopping the robot after detecting, for example, an
%% obstacle, adds a second of delay to the thread calling the
%% function. It also seems that instructions can remain in the pipeline
%% even after the application has been stopped, to the point that a full
%% reset is required to guarantee consistent behaviour. This reset
%% procedure is:

%% \begin{enumerate}
%% \item{Switch off the chassis/motorboard.}
%% \item{Unplug the serial cable from the smartphone.}
%% \item{Kill the entire application on the phone (not just the thread
%%     that was running).}
%% \item{Re-insert the serial cable.}
%% \item{Restart the application.}
%% \item{Switch the chassis back on before running any thread.}
%% \end{enumerate}

%% Performing these steps out of order (or skipping any individual step)
%% will result in unpredictable behaviour. The robot behaves as if it
%% were still executing the previous task. Killing the application is
%% made more difficult by the fact that the application can randomly
%% vanish from the list of running applications, despite the fact it is
%% still running. You can only be sure it has stopped if you see the
%% ``Visual Navigation service stopped'' Toast. At times this behaviour
%% can cause uncertainty as to whether a model does not work, or whether
%% the robot is simply acting out.
%% \newline\par

%% The robot is also large, making testing in the available space
%% difficult. Early in the project, we considered reverting to the old
%% Roboant design. The old parts were put together and tested, but again,
%% this would have required re-writing the firmware at the Arduino level
%% as Roboant used not only a different Arduino board, but a different
%% motor board. Given that the angular turning on the Roboant chassis
%% was observed to be less accurate than the AntBot chassis, we decided
%% not to invest the time in re-writing the firmware. Reasonably accurate
%% turning was preferable to a smaller form factor, but with modern
%% systems, a small robot capable of accurate locomotion should be easily
%% constructable.

%% \subsection{Assumed functionality}
%% The final and most serious issue to raise (which most previous points
%% boil down to) is assumed functionality. When taking on the project it
%% is not unreasonable for a student to assume that the robot (having
%% been through many years of development) works sufficiently well for
%% them to test their thesis. In 2017, we started work on the AntBot with
%% exactly this assumption. AntBot is presented as a stable, reliable
%% platform for testing and development. The discussion here presented
%% should show that this is not the case.\newline\par

%% The best example to show this definitively should be the frame rate
%% bug discovered as part of this project. A significantly reduced
%% framerate affected the perfomance of the CA system tested successfully
%% and likely affected the Mushroom Body circuit's performance as well
%% \cite{Mitchell2018}. Experimental results can only be trusted if the
%% platform can be trusted to work correctly to provide models with the
%% correct information and actions. The MB received reduced visual
%% information, the CA system received reduced motion information, and
%% the CX received highly inaccurate time-based displacement
%% information. Not one of the systems under test received reliable
%% input, simply due to the implementation of the robot.
%% \newline\par

%% Many other platforms could be possible, our favoured approach being a
%% small omni-wheel design capable of holonomic motion with algorithmic
%% redundancies for any biological system (e.g. an ultrasonic collision
%% avoidance system to compare to any visual model). Regardless of
%% platform, it cannot be denied that many of the problems experienced
%% are inherent to a generational project, unless development (software
%% and hardware) is managed more closely.

%% \section{Closing}
%% \textit{In progress, intended to be a single paragraph summary of the
%%   work presented.}


\newpage





%%%%%%%%% END %%%%%%%%%




\bibliographystyle{plain}
\bibliography{working}
\appendix
\section{Genetic Algorithms} \label{ap:ga}
Genetic Algorithms (GAs) are metaheuristic
optimisation algorithms which aim to optimise some (arbitrary) objective
function. GAs perform this optimisation by taking inspiration from the
theory of evolution by natural selection wherein species advance by genetic
mixing (breeding) and a small chance of mutation with each offspring.
The theory of evolution by natural selection suggests that mutations
which are useful to the species become more potent over multiple generations
as they allow the individual to live longer and/or reproduce more than those
without the mutation; mutations which are useless die out or remain inert, and
those which are harmful tend not to survive. This is an extremely simplified
view.
\newline
\par

With appropriate representation, this evolutionary concept can be
translated into an optimisation algorithm. Say we choose to represent our data as
4-bit binary strings. To start with, we generate some population of $n$ random
4-bit bitstrings in the set range; two solutions are picked using some selection
process, usually based on \textit{fitness}\footnote{Fitness - some measure of how
  \textit{good} the solution is - e.g., minimisation would mean a lower value is
  fitter.} and ``breed'' with probability $p_c$; the offspring are then mutated
with (very small) probability $p_m$; finally the
offspring/mutant-offspring/original bitstrings \footnote{Note that those chosen
  may not breed with probability $p_c - 1$, and
similarly, they may not be mutated with probability $p_m - 1$} form the next
generation and the process repeats until some termination criterion is satisfied;
this could be time, number of generations, stagnation of the population, etc.
\newline
\par

This short explanation is merely to provide a high level idea of the concepts
employed when working with GAs; there is a great deal of depth and nuance
not captured by (and not necessary for) this paper. A tutorial by
\textit{Whitley} can be found in \textit{Statistics and Computing, Volume 4
  (1994)} \cite{Whitley1994}. 

\section{CX Vicon Recordings}\label{ap:cxfigs}
\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_1}
  \caption{\label{fig:abcx1}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_2}
  \caption{\label{fig:abcx2}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_3}
  \caption{\label{fig:abcx3}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_4}
  \caption{\label{fig:abcx4}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_5}
  \caption{\label{fig:abcx5}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_6}
  \caption{\label{fig:abcx6}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_7}
  \caption{\label{fig:abcx7}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_8}
  \caption{\label{fig:abcx8}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_9}
  \caption{\label{fig:abcx9}}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{AB_CX_10}
  \caption{\label{fig:abcx10}}
\end{figure}
\end{document}


% Dissertation bibliography file (BibTeX).
% References and papers

% Mitchell2018   - Developing AntBot: Visual Navigation based on the insect brain
% Stone2017      - An anatomically constrained model for path integration in the
%                  bee brain
% Webb2018       - Barbara's review paper
% Burger1989     - On Computing a 'fuzzy' focus of expansion.
% Tistarelli1991 - Original source for the common LSE method for computing the
%                  FOE
% Haferlach200   - Evolving a neural model of insect path integration (CX before
%                  it was cool)
% Pfeiffer2014   - CX became cool. Good source for the neuroarchitecture of the
%                  physiological CX
@masterthesis{ Mitchell2018,
author = "Robert Mitchell",
title = "{Developing AntBot: Visual Navigation based on the insect brain}",
year = 2018
}

@article{Grob2017,
  title={The role of celestial compass information in Cataglyphis ants during learning walks and for neuroplasticity in the central complex and mushroom bodies},
	  author={Grob, Robin and Fleischmann, Pauline N and Gr{\"u}bel, Kornelia and Wehner, R{\"u}diger and R{\"o}ssler, Wolfgang},
		  journal={Frontiers in behavioral neuroscience},
			  volume={11},
				  pages={226},
					  year={2017},
						  publisher={Frontiers}
							}

@INPROCEEDING{Vanderstap2012,
author={N. {van der Stap} and R. {Reilink} and S. {Misra} and I. A. M. J. {Broeders} and F. {van der Heijden}},
booktitle={2012 4th IEEE RAS EMBS International Conference on Biomedical Robotics and Biomechatronics (BioRob)},
title={The use of the focus of expansion for automated steering of flexible endoscopes},
year={2012},
volume={},
number={},
pages={13-18},
keywords={cancer;endoscopes;image sequences;medical image processing;colon cancer screening;clinical process;flexible endoscope steering automation;screening efficiency;endoscope heading direction;computer algorithm;heading direction determination;focus of expansion estimation;FOE estimation;optical flow field;human colonoscopy images;motorized flexible endoscope applications;Endoscopes;Humans;Feature extraction;Estimation;Vectors;Colonoscopy;Cameras},
doi={10.1109/BioRob.2012.6290804},
ISSN={2155-1782},
month={June},}

@article{Fleischmann2018,
title = "The Geomagnetic Field Is a Compass Cue in Cataglyphis Ant Navigation",
journal = "Current Biology",
volume = "28",
number = "9",
pages = "1440 - 1444.e2",
year = "2018",
issn = "0960-9822",
doi = "https://doi.org/10.1016/j.cub.2018.03.043",
url = "http://www.sciencedirect.com/science/article/pii/S0960982218303725",
author = "Pauline Nikola Fleischmann and Robin Grob and Valentin Leander Müller and Rüdiger Wehner and Wolfgang Rössler",
keywords = "learning walks, path integration, orientation, earth’s magnetic field, Helmholtz coil, desert ant, initial calibration, celestial compass, snapshot, home vector",
abstract = "Summary
Desert ants (Cataglyphis) are famous insect navigators. During their foraging lives, the ants leave their underground colonies for long distances and return to their starting point with fair accuracy [1, 2]. Their incessantly running path integrator provides them with a continually updated home vector [3, 4, 5]. Directional input to their path integrator is provided by a visual compass based on celestial cues [6, 7]. However, as path integration is prone to cumulative errors, the ants additionally employ landmark guidance routines [8, 9, 10, 11]. At the start of their foraging lives, they acquire the necessary landmark information by performing well-structured learning walks [12, 13], including turns about their vertical body axes [14]. When Cataglyphis noda performs these pirouettes, it always gazes at the nest entrance during the longest of several short stopping phases [14]. As the small nest entrance is not visible, the ants can adjust their gaze direction only by reading out their path integrator. However, recent experiments have shown that, for adjusting the goal-centered gaze directions during learning walks, skylight cues are not required [15]. A most promising remaining compass cue is the geomagnetic field, which is used for orientation in one way or the other by a variety of animal species [16, 17, 18, 19, 20, 21, 22, 23, 24, 25]. Here, we show that the gaze directions during the look-back-to-the-nest behavior change in a predictable way to alterations of the horizontal component of the magnetic field. This is the first demonstration that, in insects, a geomagnetic compass cue is both necessary and sufficient for accomplishing a well-defined navigational task."
}

@article{Stone2017,
  title={An anatomically constrained model for path integration in the bee brain},
  author={Stone, Thomas and Webb, Barbara and Adden, Andrea and Weddig, Nicolai Ben and Honkanen, Anna and Templin, Rachel and Wcislo, William and Scimeca, Luca and Warrant, Eric and Heinze, Stanley},
  journal={Current Biology},
  volume={27},
  number={20},
  pages={3069--3085},
  year={2017},
  publisher={Elsevier}
}

@masterthesis{Webb2018,
author = "Barbara Webb",
title = "{PLACEHOLDER: JEB REVIEW PAPER; GET CORRECT CITATION"}",
year = 2018
}

@INPROCEEDINGS{Burger1989, 
author={W. Burger and B. Bhanu}, 
booktitle={Proceedings CVPR '89: IEEE Computer Society Conference on Computer Vision and Pattern Recognition}, 
title={On computing a 'fuzzy' focus of expansion for autonomous navigation}, 
year={1989}, 
volume={}, 
number={}, 
pages={563-568}, 
keywords={computer vision;computerised navigation;computerised picture processing;fuzzy set theory;mobile robots;computer vision;fuzzy focus of expansion;autonomous navigation;dynamic scene analysis;translational motion;mobile robot;camera rotation;digitization;noise;connected region;camera heading;outdoor images;erroneous point matches;image displacement vector field;Navigation;Cameras;Focusing;Vehicle dynamics;Image analysis;Layout;Fuzzy systems;Motion analysis;Mobile robots;Robot vision systems}, 
doi={10.1109/CVPR.1989.37902}, 
ISSN={1063-6919}, 
month={June},}

@inproceedings{Tistarelli1991,
  title={Dynamic stereo in visual navigation},
  author={Tistarelli, Massimo and Grosso, Enrico and Sandini, Giulio},
  booktitle={Computer Vision and Pattern Recognition, 1991. Proceedings CVPR'91., IEEE Computer Society Conference on},
  pages={186--193},
  year={1991},
  organization={IEEE}
}

@article{Haferlach2007,
  title={Evolving a neural model of insect path integration},
  author={Haferlach, Thomas and Wessnitzer, Jan and Mangan, Michael and Webb, Barbara},
  journal={Adaptive Behavior},
  volume={15},
  number={3},
  pages={273--287},
  year={2007},
  publisher={Sage Publications Sage UK: London, England}
}

@article{Pfeiffer2014,
  title={Organization and functional roles of the central complex in the insect brain},
  author={Pfeiffer, Keram and Homberg, Uwe},
  journal={Annual review of entomology},
  volume={59},
  pages={165--184},
  year={2014},
  publisher={Annual Reviews}
}


@Article{Whitley1994,
author="Whitley, Darrell",
title="A genetic algorithm tutorial",
journal="Statistics and Computing",
year="1994",
month="Jun",
day="01",
volume="4",
number="2",
pages="65--85",
abstract="This tutorial covers the canonical genetic algorithm as well as more experimental forms of genetic algorithms, including parallel island models and parallel cellular genetic algorithms. The tutorial also illustrates genetic search by hyperplane sampling. The theoretical foundations of genetic algorithms are reviewed, include the schema theorem as well as recently developed exact models of the canonical genetic algorithm.",
issn="1573-1375",
doi="10.1007/BF00175354",
url="https://doi.org/10.1007/BF00175354"
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LEGACY FROM 2018                                                              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References and papers

%Eberding2016     - Leo's Dissertation (DONE)
%Wehner2009       - The architecture of the desert ants navigational toolkit
%Ardin2016        - Using the insect mushroom body to encode route memory
%Scimeca2017      - Luca's Dissertation
%Souhila2007      - Optical flow based Robot Obstacle Avoidance
%Low2005          - Obstacle Detection using Optical Flow
%Wehner2006       - Ant Navigation: One-Way Routes Rather Than Maps
%Baddeley2012     - A model of ant route navigation using scene familiarity%
%Kodzhabashev2015 - Ant route following without scanning (Klinokinesis paper)
%Zhang2017        - Developing AntBot (Zhaoyu's Dissertation)
%ODonovan2005     - Optical Flow: Techniques and Applications
%Dittmar2010      - Goal seeking in honeybees: matching of optic flow snapshots?
%Stewart2010      - A model of visual–olfactory integration for odour
%                   localisation in free-flying fruit flies.
%Mobbs1982        - The brain of the honeybee Apis mellifera. I. The connections
%                   and spatial organization of the mushroom bodies (Royal
%                   Society)
%Kodzhabashev2014 - ROBOANT: BUILD YOUR OWN ANDROID ROBOT
%                   (https://blog.inf.ed.ac.uk/insectrobotics/roboant/)
%

@masterthesis{ Eberding2016,
author = "Leonard Eberding",
title = "{Development and Testing of an Android-Application-Network based on
the Navigational Toolkit of Desert Ants to control a Rover using Visual
Navigation and Route Following.}",
year = "2016"
}

@masterthesis{ Scimeca2017,
author = "Luca Scimeca",
title = "{AntBot: A biologically inspired approach to Path Integration}",
year = "2017"
}

@masterthesis{ Zhang2017,
author = "Zhaoyu Zhang",
title = "{Developing AntBot: a mobile-phone powered autonomous robot based on the insect brain}",
year = "2017"
}



@article{Wehner2009,
author = {Wehner, R\"udiger},
year = {2009},
month = {09},
pages = {85-96},
title = {The architecture of the desert ant's navigational toolkit (Hymenoptera: Formicidae)},
volume = {12},
journal = {Myrmecological News}
}

@article{Willshaw1969,
author = {D.J. Willshaw, O.P. Buneman, AND H. C. Longuet-Higgins},
journal = {Nature},
publisher = {Nature Publishing Group},
title = {Non-Holographic Associative Memory},
year = {1969},
url = {http://dx.doi.org/10.1038/222960a0},
doi = {10.1038/222960a0}
}

@article{Ardin2016,
    author = {Ardin, Paul AND Peng, Fei AND Mangan, Michael AND Lagogiannis, Konstantinos AND Webb, Barbara},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Using an Insect Mushroom Body Circuit to Encode Route Memory in Complex Natural Environments},
    year = {2016},
    month = {02},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pcbi.1004683},
    pages = {1-22},
    abstract = {Author Summary We propose a model based directly on insect neuroanatomy that is able to account for the route following capabilities of ants. We show this mushroom body circuit has the potential to store a large number of images, generated in a realistic simulation of an ant traversing a route, and to distinguish previously stored images from highly similar images generated when looking in the wrong direction. It can thus control successful recapitulation of routes under ecologically valid test conditions.},
    number = {2},
    doi = {10.1371/journal.pcbi.1004683}
}

@article{Souhila2007,
author = {Kahlouche Souhila and Achour Karim},
title ={Optical Flow Based Robot Obstacle Avoidance},
journal = {International Journal of Advanced Robotic Systems},
volume = {4},
number = {1},
pages = {2},
year = {2007},
doi = {10.5772/5715},

URL = { 
        https://doi.org/10.5772/5715
    
},
eprint = { 
        https://doi.org/10.5772/5715
    
}
,
    abstract = { In this paper we try to develop an algorithm for visual obstacle avoidance of autonomous mobile robot. The input of the algorithm is an image sequence grabbed by an embedded camera on the B21r robot in motion. Then, the optical flow information is extracted from the image sequence in order to be used in the navigation algorithm. The optical flow provides very important information about the robot environment, like: the obstacles disposition, the robot heading, the time to collision and the depth. The strategy consists in balancing the amount of left and right side flow to avoid obstacles, this technique allows robot navigation without any collision with obstacles. The robustness of the algorithm will be showed by some examples. }
}

@INPROCEEDINGS{Low2005,
    author = {Toby Low and Gordon Wyeth},
    title = {Obstacle detection using optical flow},
    booktitle = {in Proceedings of the 2005 Australasian Conf. on Robotics \& Automation},
    year = {2005}
}

@article{Wehner2006,
title = "Ant Navigation: One-Way Routes Rather Than Maps",
journal = "Current Biology",
volume = "16",
number = "1",
pages = "75 - 79",
year = "2006",
issn = "0960-9822",
doi = "https://doi.org/10.1016/j.cub.2005.11.035",
url = "http://www.sciencedirect.com/science/article/pii/S0960982205014120",
author = "R{\"u}diger Wehner and Martin Boyer and Florian Loertscher and Stefan Sommer and Ursula Menzi"
}

@article{Baddeley2012,
    author = {Baddeley, Bart AND Graham, Paul AND Husbands, Philip AND Philippides, Andrew},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {A Model of Ant Route Navigation Driven by Scene Familiarity},
    year = {2012},
    month = {01},
    volume = {8},
    url = {https://doi.org/10.1371/journal.pcbi.1002336},
    pages = {1-16},
    abstract = {Author Summary The interest in insect navigation from diverse disciplines such as psychology and engineering is to a large extent because performance is achieved with such limited brain power. Desert ants are particularly impressive navigators, able to rapidly learn long, visually guided foraging routes. Their elegant behaviours provide inspiration to biomimetic engineers and for psychologists demonstrate the minimal mechanistic requirements for complex spatial behaviours. In this spirit, we have developed a parsimonious model of route navigation that captures many of the known properties of ants routes. Our model uses a neural network trained with the visual scenes experienced along a route to assess the familiarity of any view. Subsequent route navigation involves a simple behavioural routine, in which the simulated ant scans the world and moves in the most familiar direction, as determined by the network. The algorithm exhibits both place-search and route navigation using the same mechanism. Crucially, in our model it is not necessary to specify when or what to learn, nor separate routes into sequences of waypoints; thereby providing proof of concept that route navigation can be achieved without these elements. As such, we believe it represents the only detailed and complete model of insect route guidance to date.},
    number = {1},
    doi = {10.1371/journal.pcbi.1002336}
}

@InProceedings{Kodzhabashev2015,
author="Kodzhabashev, Aleksandar
and Mangan, Michael",
editor="Wilson, Stuart P.
and Verschure, Paul F.M.J.
and Mura, Anna
and Prescott, Tony J.",
title="Route Following Without Scanning",
booktitle="Biomimetic and Biohybrid Systems",
year="2015",
publisher="Springer International Publishing",
address="Cham",
pages="199--210",
abstract="Desert ants are expert navigators, foraging over large distances using visually guided routes. Recent models of route following can reproduce aspects of route guidance, yet the underlying motor patterns do not reflect those of foraging ants. Specifically, these models select the direction of movement by rotating to find the most familiar view. Yet scanning patterns are only occasionally observed in ants. We propose a novel route following strategy inspired by klinokinesis. By using familiarity of the view to modulate the magnitude of alternating left and right turns, and the size of forward steps, this strategy is able to continually correct the heading of a simulated ant to maintain its course along a route. Route following by klinokinesis and visual compass are evaluated against real ant routes in a simulation study and on a mobile robot in the real ant habitat. We report that in unfamiliar surroundings the proposed method can also generate ant-like scanning behaviours.",
isbn="978-3-319-22979-9"
}

@article{ODonovan2005,
  title={Optical flow: Techniques and applications},
  author={O'Donovan, Peter},
  year={2005}
}

@article {Dittmar2010,
	author = {Dittmar, Laura and St{\"u}rzl, Wolfgang and Baird, Emily and Boeddeker, Norbert and Egelhaaf, Martin},
	title = {Goal seeking in honeybees: matching of optic flow snapshots?},
	volume = {213},
	number = {17},
	pages = {2913--2923},
	year = {2010},
	doi = {10.1242/jeb.043737},
	publisher = {The Company of Biologists Ltd},
	abstract = {Visual landmarks guide humans and animals including insects to a goal location. Insects, with their miniature brains, have evolved a simple strategy to find their nests or profitable food sources; they approach a goal by finding a close match between the current view and a memorised retinotopic representation of the landmark constellation around the goal. Recent implementations of such a matching scheme use raw panoramic images ({\textquoteleft}image matching{\textquoteright}) and show that it is well suited to work on robots and even in natural environments. However, this matching scheme works only if relevant landmarks can be detected by their contrast and texture. Therefore, we tested how honeybees perform in localising a goal if the landmarks can hardly be distinguished from the background by such cues. We recorded the honeybees{\textquoteright} flight behaviour with high-speed cameras and compared the search behaviour with computer simulations. We show that honeybees are able to use landmarks that have the same contrast and texture as the background and suggest that the bees use relative motion cues between the landmark and the background. These cues are generated on the eyes when the bee moves in a characteristic way in the vicinity of the landmarks. This extraordinary navigation performance can be explained by a matching scheme that includes snapshots based on optic flow amplitudes ({\textquoteleft}optic flow matching{\textquoteright}). This new matching scheme provides a robust strategy for navigation, as it depends primarily on the depth structure of the environment.},
	issn = {0022-0949},
	URL = {http://jeb.biologists.org/content/213/17/2913},
	eprint = {http://jeb.biologists.org/content/213/17/2913.full.pdf},
	journal = {Journal of Experimental Biology}
}


@article {Stewart2010,
	author = {Stewart, Finlay J. and Baker, Dean A. and Webb, Barbara},
	title = {A model of visual{\textendash}olfactory integration for odour localisation in free-flying fruit flies},
	volume = {213},
	number = {11},
	pages = {1886--1900},
	year = {2010},
	doi = {10.1242/jeb.026526},
	publisher = {The Company of Biologists Ltd},
	abstract = {Flying fruit flies (Drosophila melanogaster) locate a concealed appetitive odour source most accurately in environments containing vertical visual contrasts. To investigate how visuomotor and olfactory responses may be integrated, we examine the free-flight behaviour of flies in three visual conditions, with and without food odour present. While odour localisation is facilitated by uniformly distributed vertical contrast as compared with purely horizontal contrast, localised vertical contrast also facilitates odour localisation, but only if the odour source is situated close to it. We implement a model of visuomotor control consisting of three parallel subsystems: an optomotor response stabilising the model fly{\textquoteright}s yaw orientation; a collision avoidance system to saccade away from looming obstacles; and a speed regulation system. This model reproduces many of the behaviours we observe in flies, including visually mediated {\textquoteleft}rebound{\textquoteright} turns following saccades. Using recordings of real odour plumes, we simulate the presence of an odorant in the arena, and investigate ways in which the olfactory input could modulate visuomotor control. We reproduce the experimental results by using the change in odour intensity to regulate the sensitivity of collision avoidance, resulting in visually mediated chemokinesis. Additionally, it is necessary to amplify the optomotor response whenever odour is present, increasing the model fly{\textquoteright}s tendency to steer towards features of the visual environment. We conclude that visual and olfactory responses of Drosophila are not independent, but that relatively simple interaction between these modalities can account for the observed visual dependence of odour source localisation. CAcollision avoidanceCBrandom chequerboardEMDelementary motion detectorHShorizontally stripedIRinfraredLPTClobula plate tangential cellLVlocalised vertical contrastLV-farLV arena with odour source offset from vertical contrastLV-nearLV arena with odour source aligned with the vertical contrastODolfactory signalOD'estimated temporal derivative of ODOD*OD' following adaptive gain controlOLodour localisationOLIodour localisation indexOMRoptomotor responseSRspeed regulationVOCvolatile organic compound},
	issn = {0022-0949},
	URL = {http://jeb.biologists.org/content/213/11/1886},
	eprint = {http://jeb.biologists.org/content/213/11/1886.full.pdf},
	journal = {Journal of Experimental Biology}
}


@article {Mobbs1982,
	title = {The brain of the honeybee Apis mellifera. I. The connections and spatial organization of the mushroom bodies},
	volume = {298},
	number = {1091},
	pages = {309--354},
	year = {1982},
	doi = {10.1098/rstb.1982.0086},
	publisher = {The Royal Society},
	abstract = {The mushroom bodies of the bee are paired neuropils in the dorsal part of the brain. Each is composed of the arborizations of over 17 x 104 small interneurons of similar architecture called Kenyon cells. Golgi staining demonstrates that these neurons can be divided into five groups distinguished on the basis of their dendritic specializations and geometry. The mushroom body neuropils each consist of a pair of cup-shaped structures, the calyces, connected by two short fused stalks, the pedunculus, to two lobes, the α- and β-lobes. Each calyx is formed from three concentric neuropil zones, the basal ring, the collar and the lip. The calyces are organized in a polar fashion; within the calyces each of the five categories of Kenyon cell has a distribution limited to particular polar contours. The dendritic volumes of neighbouring Kenyon cells arborizing within each individual contour are greatly overlapped. Fibres from groups of neighbouring cells within a calycal contour are gathered into bundles that project into the pedunculus, each fibre dividing to enter both the the α- and β-lobes. The pedunculus and the lobes are conspicuously layered. Kenyon cells with neighbouring dendritic fields within the same calycal contour occupy a single layer in the pedunculus and lobes. Thus the two- polar organization of the calyces is transformed into a Cartesian map within the pedunculus, which continues into the α- and β-lobes. The calyx receives input fibres from both the antennal lobes and the optic neuropils. The branching patterns of these cells reflect the polar organization of the calyces as their terminals are restricted to one or more of the three gross compartments of the calycal neuropil. The course of these tracts and the morphologies of the fibres that they contain are described. Cells considered to represent outputs from the mushroom bodies arborize in the pedunculus and α- and β-lobes. Generally the arborizations of the output neurons reflect the layered organization of these neuropils. Fibres from the two lobes run to the anterior median and lateral protocerebral neuropil, and the anterior optic tubercle. Additionally there is an extensive network of feedback interneurons that inter- connect the α- and β-lobes with the ipsi- and contralateral calyces. Many individual neurons have branches in both the α- and β-lobes and in the pedunculus. The pathways and geometries of the fibres subserving the two lobes are described. The hypothesis of Vowles (1955) that the individual lobes represent a separation of sensory and motor output areas is shown to be incorrect. The anatomy of the bee{\textquoteright}s mushroom bodies suggests that they process second-order antennal and fourth- and higher-order visual information. The feedback pathways are discussed as possible means of creating long-lasting after-effects which may be important in complex timing processes and possibly the formation of short-term memory.},
	issn = {0080-4622},
	URL = {http://rstb.royalsocietypublishing.org/content/298/1091/309},
	eprint = {http://rstb.royalsocietypublishing.org/content/298/1091/309.full.pdf},
	journal = {Philosophical Transactions of the Royal Society of London B: Biological Sciences}
}


@misc{Kodzhabashev2014,
author="Kodzhabashev, Aleksandar",
 title = "{Roboant: Build your own android robot}",
 
 note = {{https://blog.inf.ed.ac.uk/insectrobotics/roboant/} \textit{(Last visited 22/01/2018)}},
 year = "2014"
}

@article {Wittlinger1965,
	author = {Wittlinger, Matthias and Wehner, R{\"u}diger and Wolf, Harald},
	title = {The Ant Odometer: Stepping on Stilts and Stumps},
	volume = {312},
	number = {5782},
	pages = {1965--1967},
	year = {2006},
	doi = {10.1126/science.1126912},
	publisher = {American Association for the Advancement of Science},
	abstract = {Desert ants, Cataglyphis, navigate in their vast desert habitat by path integration. They continuously integrate directions steered (as determined by their celestial compass) and distances traveled, gauged by as-yet-unknown mechanisms. Here we test the hypothesis that navigating ants measure distances traveled by using some kind of step integrator, or {\textquotedblleft}step counter.{\textquotedblright} We manipulated the lengths of the legs and, hence, the stride lengths, in freely walking ants. Animals with elongated ({\textquotedblleft}stilts{\textquotedblright}) or shortened legs ({\textquotedblleft}stumps{\textquotedblright}) take larger or shorter strides, respectively, and concomitantly misgauge travel distance. Travel distance is overestimated by experimental animals walking on stilts and underestimated by animals walking on stumps.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/312/5782/1965},
	eprint = {http://science.sciencemag.org/content/312/5782/1965.full.pdf},
	journal = {Science}
}

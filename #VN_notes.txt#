Notes for visual navigation experimentation:

Part 1: VICON
- I can use vicon to record runs and the timer on the threads easily gives me enough time to
  run recordings for outward, then outward memory.

- Would like to run a single recording then get the data from it to see how it is formatted
- The data is in a .csv so pandas may be useful; want to see what plots I can produce

notes:
- Vicon files need modified slightly before they can be plotted. However, this is easy to do, and
  once done, they can be parsed and plotted with ease.

how-to:
- Record runs on vicon
- Export runs to csv in the Desktop\antbot_tracks folder on the vicon machine
- Upload the csvs to Drive
- Download on laptop
- Remove lines 1, 2, 3, and 5
- Run vicon.py to get a trajectory plot of the two routes.
- Do with this whatever needs done

Part 2: Experiments; design and implementation
- Want different arenas. Arenas which are easy for the OF to navigate as this is not what
  we're testing.

- Want to make sure the robot has to make some navigational decisions.

- Homeward routes? How hard would this be to implement as a curiosity?

  - During learning learn the outward image, learn the homeward image at the same time by doing
    a rotateMatInAzimuth variant which returns a matrix. If possible, it may be nice to get an image
    from the cameras to show the route taken.

- Optical flow errors result in a reset; they happen but they should not be causing issues for the
  visual navigation experiments. This may be waived if the effects are not too bad.

- Want to track the full trajectory; if the robot goes wrong and goes into a wall or similar,
  switch off the chassis and let the robot sit. Do not replace on route if the robot gets lost.

Part 3: Moar experiments
- Difference in the sensitivity of OF was noticed; this was not in-fact the sensitivity of the OF,
  rather the fact that the robot does not stop before turning as it does in Visual Navigation.

  - This could be the source of a couple of the errors in the flow experiments but it does not account for them
    all.

- Reward signal frequency has been increased in an attempt to learn more images; we are learning more
  but any improvement is not readily apparent.

Part 4:
- Problems:
  1. Tussocks are too short to give any reliable information; likely that visual cues are being taken
     from other parts of the image.
  2. Scanning only on obstacle detection was a bad idea given the inconsistency with the optical flow
  3. Robot behaviour could become inconsistent or strange resulting in a reboot. The cause is unknown
     but likely related to the communications between the arduino and phone being disrupted.

- Solutions:
  1. Try using larger objects in a dense-ish formation.
  2. Scan every 10cm AND on obstacle detection (Turns out there was already a two second limit)
  3. None

  Even with experimental issues the network is showing promise.

- Curio:
  1. Window size - Frame offset does affect window size. Wider -> Side is of higher effect
  2. Homeward-bound - 

Todo and notes file for Project:

02 / 10 / 17:
todo:
 - Optical flow detection implementation
   > onCvCameraFrame is the main callback method used (everything happens
     in response to a new visual stimulus
   > From here the threads are run
   > Looking at repurposing existing method computeSparseOpicFlow()
     originally used for speed calculation. Not used anywhere else, so
     may be modified.
 
notes:
 - Optical flow paper: http://www.araa.asn.au/acra/acra2005/papers/low.pdf ::CITE::
   > Detect corners using sparse flow and goodFeaturesToTrack
   > Compute range over a series of frames
   > Halt if below some threshold
   > Tune manually

 - Cross correlation matching?

 - Turns out the optical flow as described by Luca isn't actually there
   > Actually I was wrong, it is, but it's not in the flow computation method; it's in the getSpeeds method
 - Seems to be a number of implementations of image rotation/reshaping/remapping


 - Line 2117: if ( Maths.abs( mod((int) currentPointsTotrack.get(i,j)[0] + 12, 90) - mod((int) prevPointsToTrack.get(i,j)[0] + 12, 90)) < 70 ) {...}
   Taking a distance between two x coordinates of points + 12, this is the pixel offset for the left hand side.
   Looks like, if the distance difference between points in the image (mod 90 for image overlap), is greater than 70, then we have enough information to compute
   a flow vector (IMPORTANT FOR OBSTACLES)

 - So, where to implement my stuff?..
   >Can add functions in MainActivity.java - Function for detection? Then work the behaviour into the appropriate threads (i.e. if clear move step else halt)

progress:
- Actually know how the existing stuff works. The method described by Luca IS present despite what I initially thought; he computes the flow vectors manually
- Added menu options and appropriate back-end code
- Have an idea as to how to implement behaviour:
  A low level (instinct level) function running on each frame to check distances using the methods described in the paper linked above


06 / 10 / 17:
- Getting this optical flow off the ground
  >Compute the focus of expansion

- May need to restructure existing OF flow code so sparse code may be used without
  messing with dense code
  
notes:
- Papers for OF: http://www.dgp.toronto.edu/~donovan/stabilization/opticalflow.pdf
                 http://journals.sagepub.com/doi/full/10.5772/5715

progress:
- Code for computing FOE is in place
- Need to look at using this info
		 
08 / 10 / 17:
- Compute velocity of the robot.
  > For this need to build new arduino functions.
  > Unless I can use getSpeedsFromSparse/Dense flow
    - Will try this method first to avoid fiddling with Arduino

- Compute TTC
- Maybe compute image depth
- Build a basic thread to get the robot to detect an obstacle and halt

notes:
- No speed retrieval methods on hardware apparently
  > How was Luca using the encoders???

- No idea what metric my TTC is in, assuming millisecs but could be secs. Check to see
  if V was per second, TTC should be the same.

progress:
- Have now got code for computing TTC, and hence image depth should be ok
  (Depth = TTC * V)

- Have a very simple test thread to check if the system actually works. How to debug this
  I am completely lost. Could try drawing flow arrows and printing debug info to
  the runOnUiThread.

- Debugging on the robot time!

09 / 10 / 17:
- Get code working

notes:
- Running seems inconsistent (Being stupid, make sure arduino is on
  before trying to run)

- Need to use prevPoints, and currentPoints BEFORE they're used for left/right flow

- IMPORTANT: MATRIX MULTIPLICATION IN ANDROID uses Core.gemm() function to do proper matrix dot product


15 / 10 / 17:
notes:
- Not the most productive week, sidetracked by CT.
- Need to fill out this file for every work session; need to know what was happening when I stopped!

progess:
- Currently debugging AntEye, calculation of the FOE is producing strange results
- Computing u and v is fine (results look weird but worry about that later), then when
  added to A, they show strange, erratic values. May be worth a Stack Overflow question God forbid.
  - Would be worth checking b too to make sure the method does what it's suppoesed to.
  - Otherwise, find a solution quickly, too small a problem to hang about over.

16 / 10 / 17:
- Figure out Mat.put() problem (DONE)
  > Matrix indexing is still from 0...
  > Still getting some strange outputs
  > Fixed, due to matrix initialisation + use of pushback
  > Check b, seems fine

- Refactor code for A and b calculations (DONE)
  > double should be float[] (for u and v)
  > remove debug outputs

- Get true FOE (wrap values around axis to get a value) (DONE)

- Work on TTC calculation, look for bugs.

notes:
- Need to be careful with matrix ops, can be finicky
- Should FOE be absolute? Negative FOE x or y does't make sense
  > Not absolute, wrapped, e.g. -2.5 should be 87.5 (subtract from 90 if neg)
    fx = -95
    fx = Math.abs(fx) = 95	
    fx = fx % 90 = 5
    fx = 89 - 5 = 84

- Printing the correct value tends to help when debugging...
- FOE is returning a 2x1 despite output saying it's 1x2 which is strange
  and annoying.

progress:
- Seem to have a working FOE calculation and this is being sucessfully passed
  up the chain to the obstacle detection function

- Aside: Might be a good idea to draw the FOE on screen, look into this

27 / 10 / 17:
- Draw FOE on debug image
- if wrong, then fix
- if looks correct, split image

notes:
- Speed is changing erratically
- OF information for speed is the same as that for FOE, could be causing problems


30 / 10 / 17:

notes:
- Need a way of saccading; idea, compute left and right flow fields
  as Luca did. Then look for FOE in each of these

  [     | .   ]  // Say image is centered at -45 (left flow) FOE should be thereish

  If it's anywhere to the left, then we need to..

  Ok, don't split the frame, just look for FOE, then turn away from it

- Need to figure out how foe would change based on flow field...


06 / 11 / 17:
today:
- Fix movement and stopping commands (DONE)
- TTC gone wild??? WHY?!
  - Soln, take average ttc values (possibly average out the FOE as well.
  - This is getting stupid.
  - Taking average FOE over 5 frames, looks more consistent than before.

notes:
- Lost time due to ADB and weird TTC issues.
- FOE averageing:
  - Init global_foe to (45, 5)
  - FOE is a 2x1, ensure all matrices reflect this, stupid amount of null
    pointer exceptions
  - global_ttc should be just above whatever threshold you are setting
  

10 / 11 / 17:
today:
- Normailize input image

notes:
- RGBA normalisation is making the app noticably slower, this may present an issue.
- Now, only normalize the blue channel for effiency but still slow.

progress:
- Little; normalisation had no noticeable affect on the calculations + behaviour

17 / 11 / 17:
today:
- Draw points on the debug frame, hope to confirm suspicions about
  cvGoodFeaturesToTrack();

- Switch to using dense flow for all optic flow (DONE)
- Move to speed from encoders, either how Luca did it, or from the arduino direct
  
- Test
- Look into creating an optical flow filter as in:
  http://jeb.biologists.org/content/jexbio/213/11/1886.full.pdf

  Looks like a uniform flow filter shifted left and right then overlaid.

notes:
- cvGoodFeaturesToTrack():
  - Movement is being detected where there is none, small but varied
  - Somewhat inconsistant, new features are being picked up and vanishing while
    the robot is sat still. Take images to document.
    - Images taken, better examples could be collected.

- Wheel encoders:
  - It looks like the arduino keeps track of these, but:
    1. Not for the go command. No encoder information is collected or sent
    2. In it's other uses, it is used internally, not sent back, where is this
       message?
    3. Need to find this, otherwise I'm going to have to edit Arduino code which is
       a scary prospect.

- Have not tested

progress:
- Determined that cvGoodFeatures was giving bad/inconsistent information
- Switched flow methods
- Closer to true wheel encoder usage, lord knows what was going on before
- Have an idea of what to do flow filter wise, but need to look into practical
  implementations of such a filter.

20 / 11 / 17:
todo:
- quick tests of new OF
- check Luca's filters for OF against his dissertation to see what he was doing
- figure out how broadcasting and recieving works here, need those encoder values
  and they're not being sent
- Get Leon's email.

notes:
- prevPointsToTrack for some reason not initialised in computeDenseOpticFlow
- prevPointsToTrack not used at all for dense flow
- Testing did not happen due to this

21 / 11 / 17:
todo:
- fix FOE calculation for dense flow (DONE)
- more work on flow filtering

notes:
- old code for computing foe was downright strange, be careful with new stuff
- luca's flow filtering was done using CX_Holonomic.get_prefered_flow() which
  uses his method of projecting the vectors onto a unit circle.
- the left and right preferred flow will be identical as the directional flag is
  never used

  How can I use this?

23 / 11 / 17:
notes:
- Luca's filtering done using the dot product i.e. the projection of the actual
  viewed flow onto the expected.

- TTC now infinite, distances seem more realistic and FOE is consistant
- Need to figure out image depth to use TTC, otherwise also, FLOW FILTERING!!!

24 / 11 / 17:
- Solve Infinite TTC (DONE)
- Filtering: figure it out. (DONE, I think...)
  - How to gain a filter from Luca's code (known, figured out a couple of days ago)
  - How to use this to detect obstacles???
    - For each flow vector detected:
      	  project it onto the expected (dot product)
	  add the result to an accumulator
	  if we exceed some threshold
	  reset and saccade away from the FOE.

- Implement filtering for the purpose of collision avoidance (DONE)

- Look more into the orginal method using the OF paper, convinced I'm missing
  something

notes:
- Infinite TTC caused by fact that speed is less than one when the robot is
  stationary, we divide by speed^distances which comes out as ~0.02^900 -> 0
  Dividing by 0 gives infinity in Java

- Solution?: Could just set speeds to 1 if sufficiently small.
- Could test to see if it drops when actually running the bot.
  - Tests show it does, though not sure how it behaves
  - Answer: strangely. TTC goes from infinite to slightly greater than 0
    in one jump.
  - Problem: As soon as the power was actually computable (big enough to where,
    it wouldn't -> 0), it becomse infinite as you're still going to the power of
    900. Attempted solution was to scale back the power by the same degree that we
    increased the speed, however no joy. It only takes one sufficiently large
    speed reading to throw the whole thing off.
  - May finally be time to accept that this approach will not work.
    
- Maybe remove normalisation for performance
  - Was already off for some reason?
- Moment of realisation when all of my flow filtering code essenially does
  what Luca's already did. So, I feel like I may be computing speed information?

progress:
- Figured out some problems with the old CA approach
- Implemented an OF filter for Collision Avoidance.
- Now need to test with this and tune it to see what thresholds would be
  appropriate.

- Construct a testing environment; observe flow values as obstacles approach on
  either side.

- Still need to send Leo an email to sort out the encoders, but the dense flow
  speed actually looks a lot more consistent.

28 / 11 / 17:
todo:
- Send Leo an email RE encoders. (DONE)
- Experiment with collision avoidance filtering
  (Something that would be good, more to write about)
  - Modify obstacle detection thread (DONE) / Create new thread + menu option using
    the filter instead of ttc.
  - Look at how the L/R flow's behave with obstacles approaching, slightly
    angled, and directly ahead. (IP, see notes);

- Dense flow speed looks good, having an encoder option would be good though

notes:
- making test thread:
  - For now, just want to set the robot to go and watch the values change.

- testing:
  Initial runs:
  - right flow always seems to be larger than left. why?
  - may try slowing the bot down a little

  Object pass, object on the left:
  - Left value, dropped sometimes, didn't change others. On passing the value consistantly increased
  - Right value, unaffected (good)

  Object pass, object on the right:
  - Left value, no variation
  - Right value, inconsistant variation, seemed to drop sometimes, no change others.
  - Right value still greater by about 10, (so by about 1000 in reality), what is the reason for this discrepancy???

  General:
  - Value on the opposite side to the object will not change which was expected.
  - Value on the object side seems to drop, but it's not consistant. 

Barbara's model used an accumulator, I'm not accumulating flow information, just
the immediate result from the filter.

I was implementing flow filters in the way Luca did in an effort to understand
how they worked. Now I feel like I do. The current flow sums are coming out as
negative. So, a majority of them are negative (from the looks of the values,
all of them). Therefore, the angle between the filter and the actual vectors is
greater than 90 degrees. Hmm....

- Sorted the negative values


progress:
- Sent Leo that email, if no response within a reasonable time, I will triple
  check then implement my own code in the go command on the arduino

- Debugged my filtering code. Taking negatives was the wrong approach; when
  breaking it down it became apparent that the filter was being applied to
  the wrong side of the input image (the +/- 12 terms were reversed).

- Now my filter values look better; note, something approaching from behind will
  cause the value to drop and a stop to be triggered, so this is good.

- I can now use the flow to manipulate the behaviour of the robot in a more
  consistant way than I could previous. Now I just need to figure out the
  output for an obstacle approaching input. Needs more experimentation but no
  time left today. Nice to actually be running experiments again...
  
29 / 11 / 17:
today:
- Experiment with flow filtering / some weighting
  - Left and right now very consistent
  - Need some way of differentiating/thresholding
    - Side that is approaching faster is greater
    - Aside: Know what Luca was doing with image shift now. (left is right vv)
    - Take side which is approaching faster, trigger turn in opposite direction
    - Do not observe flow info while turning

  - Idea: Use difference threshold instead of straight threshold?
  
- If getting more pertinant info, start adding behaviour

notes:
- Need to build some test boxes, reasonable size, painted black
- My code is noticeably slower than the old CX code...
  - Fixed, sleep(600) needed in thread loops to give the image processing
    time to execute.

progress:
- Fixed efficiency issues, not my image processing code, but related to
  synchronisation issues.
- Flow information now quite consistant
- Fixed image shift (left flow was being displayed and stored as right and vice
  versa)
- Now have some idea how to use this information!
  - Information is good, but ranges are a tad wild - Some tuning required 
  - Taking the flow difference then setting positive and negative threshold
  - Initial threshold value is +/-5000 for the difference
    - Stop, rotate +/- 45, move
    - Problem, at different points in motion, certain flow signals mean different
      things: e.g. when avoiding a negative signal shouldn't necessarily trigger
      a turn.

01 / 12 / 17:
today:
- Figure out how to threshold:
  - Sufficiently Big/Small/In some window?
  - Accumulation? Swing cancels any previous? (May be a good idea)
- Implement reactive behaviour.

notes:
Basic thresholding: Value > T => Trigger turn
- Working in avoidance thread now, trying to simply trigger a left turn
- An initial value for T = -5000 actually worked alright, but it seems
  sensitive to noise (e.g. simply passing things on the right)

- Utilising the "leaky accumulator" idea;
  accumulate readings x where x > T, this helps deal with some noise
   - still possible to trigger false saccades, in some cases
   
  could accumulate within a certain time period, or, take a series of readings
  with the same sign (e.g. if we see n contiguous readings of x, where x > |T|,
  then react).

Within Time Period:
- If you just add the ca_difference to the accumulators, if one breaches the
  threshold then there's a significant bias.

- Initial tests with dual threshold promising
- More motor command issues, stop not being registered.
  Initial value for AT chosen based on observations as robot approaches obstacles
  Initial value for RT chosen based on initial value for AT.

  1. AT = 5000, RT = 20000:
     - Functioned extremely well in some tests, poorly in others
     - Seemed to prefer turning right

  2. AT = 5000, RT = 30000:
     - Showed responses, though too late
       turn was being triggered, after a collision had occurred.

  3. AT = 5000, RT = 25000:
     - Again, response is triggered, though later than would be desired
     - Near collision or "bounce" off the surface
     - Can see promise with a different style of turn (-ve, +ve instead of arc)

     Repeat with alternate turn style:
     - Just bad
  
  - Did not take into account time allowed for samples:
    - Increasing time => Increased sensitivity
    
- Ongoing issue; need to fix speeds so AntBot actually goes straight

progress:
- Have something that is working in the way I need it to, however it needs tests
  and tuning (major)

- Need to play with thresholding some more

- Robot seems to prefer turning left for some reason, convinced there's some
  command buffer delay (i.e. issueing 3 or 4 go commands to avoid instead of one)

  - Maybe stop, then continue with new speeds, communication latency may be too
    great for continuous motion

  - It is reacting, but too late


04 / 12 / 17:
today:
- work with tuning the filter behaviour

notes:
1. AT = 5000, RT = 20000
   - Reacted well, but too slowly
   - Speeds are good check batteries.
   - Batteries fine.

   - Noticing inconsistency and repeated motion
     - Right turn triggered for seemingly no reason

2. AT = 5000, RT = 20000, No obstacle
   - 1st run:
     - Repeated slight left turns, last was large
     - No right motion despite that being the previously perceived bias.
   - 2nd run:
     - Started turning immediately
     - Followed a random looking pattern with some repeated movements
       (e.g. there was a distinct left/right swerve motion)

- Maybe a command queue issue?
  - Not this issue, but may be an underlying problem
  - The fact that the robot started moving before the activity was loaded is very odd
  - Could be thread resolution? Thread is never properly stopped

Added in thread timer: 15 seconds
Runs with no obstacle present
1. Initiated a left turn by the centre of the pitch and moved towards the opening
   turned right before reaching it

2. Initiated the same left turn near the centre of the pitch, another slight left turn where
   (1) turned right and headed towards the opening

3. Same as (2) with a strange short stop and final motor jump?

4. Different number of turns but still gravitates towards the opening in the mesh

From other end of the pitch:

5. Same right turns as previous tests

6. Again, not as severe as (5)

Ok, so the robot always seems to want to go right in an empty space. Always makes roughly three turns
of different sizes at roughly the same places. Always finishes in roughly the same place

Introduce obstacle on the right to see if a left turn is triggered instead of the standard right

7. Weaker right turn triggered but still right, not left. This is odd as nothing major has been changed

8. Same run as (7), Left turn triggered, far too early

- Changed time count method (counting loop iterations instead of raw time. Same behaviour still
- Change accumulators

- Changed reaction threshold to 10000,  works better. Accumulation threshold still 5000
  reasoning: very first test with no accumulation worked very well, thinking is that a lower
  reaction threshold allows multiple smaller values to make a difference rather than focusing on a
  few larger ones.

08 / 12 / 17:
today:
- work more with tuning
- emergency stop? how?

notes:
- Still issues with tuning
  - Seems to be that in general, lower reaction thresholds are better (too sensitive > not sensitive
    enough)
  - Experiment with dual vs individual accumulation
  - Still preferring a right turn WHY?!
  - Try running inverse scenario
    - May be background interference; nope...

  - Right turn preference may be resolved: Potential issue; speed problem mentioned previously
    the robot was always drifting to the left which may have been triggering that right turn.
    Solution: set the right speed to ~90% of the left and the robot will move in an almost straight
    line (slight right drift, but seems negligable). Robot now seems to react nicely to the obstacles.
    Should be noted that behaviour becomes unpredictable in an open space (background elements
    could be playing with the flow).

    Best results so far: AT = 5000, RT = 10000, Speed: 25 (22). Still don't have an emergency stop.
    May be worth trying a halt and turn

18 / 12 / 17:
- Optic Flow:
  - Try halt and turn (works waaaay better)
  - Find out what more wants done with this
  - Reaction time still too slow in some instances (How could this be fixed?)
    - Sensitivity is still there so can't be too over-zealous with the sensitivity...
    

- Mushroom Body / Willshaw:
  - Read...


notes:
- Don't worry about integrating OF into CX
- No downward image for speed computation
  

20 / 12 / 17:
today:
- MB / Willshaw, code walkthrough, commenting


notes:
- Willshaw, seems that first image is used to set thresholding for all subsequent images

19 / 01 / 17: Happy New Year!
today:
- MB/Willshaw familiarisation
  - Thank goodness I commented this the last time around
  - Seem to want to use between 200 and 400 KCs to learn the first image
  - This is used to ensure the threshold is set so that an appropriate amount
    of KCs are used.
  - Brightness is taken as a cumulative sum over all connected vPNs, not
    thresholding them individually.
    - These implementation details should be included in the background section
      of the dissertation.   
- Implement new thread for the following process:
  - Navigate for time T through obstacle course
  - Replace at start
  - Navigate the same route using vision
  - We will create copies of the Willshaw module for each new implementation
    for the sake of clarity; also I feel that the OF one will take a bit more
    modification.
- Plan modifications for MB model.
  - Real valued weightings
  - Optical flow snapshots: Honeybee paper
    - Saving the dense flow images which store the pixel and the pixels new
      location.
    - How to engineer this into the MB network?..
  - Need the paper for vPN weighting strategy

notes:

26 / 01 / 18:
- Add new menu item to run the route nav pattern (DONE)
- Write the CA with memory thread (DONE)
- Write the VN thread from MB (Planned)

notes:
- So far, the plan is to continue using scanning behaviour.
  Klinokinesis is more realistic, but the hardware is limiting
  in this case. We can't turn fast enough using klinokinesis in
  a densly populated environment.

08 / 02 / 18:
- Finish basic VN thread (Done-ish but could maybe use refinement)
  
- Make modified Willshaw network

notes:
- Images are stored every second. This should hopefully be enough to allow the
  robot to recapitulate the route with a fine level of detail.

- Thought: Image of robot looking at a box before a turn and image of next
  section in the route may have similar levels of familiarity. Learning rate
  could play a difference hear

- Do I want OF and vn going at the same time? I.e. an OF CA trigger will
  trigger a scan instead of an avoidance saccade? Or could I move for 2 seconds
  triggering saccades as necessary?

- First option preferable. CA warning will trigger a scan.
  - Obstacle detected: directional decision was made here
  - Also avoids issues with running into the box then scanning, or scanning
    while right next to something

13 / 02 / 18:
today:
- Write RealWillshaw code : Done, I think
- Come up with experiment structure for Real : Done
  - One learning CA run
  - Then learning memory runs (i.e. run the robot from memory, save
    the most familiar image).
- Write experiment structure for real : In progress...

- In lab:
  - Finish hardware stuff (exception of battery)
  - Measure available space; buy battery ASAP

notes:
- RealWillshaw
  - Every KC starts with a weighting of 1
  - Upon activation KC weigting is: w = w * r; where r is learning rate
    starting with r = 0.9;


14 / 02 / 18:
- Still have no defined end point for the nav thread.

notes:
- Nav threads appear to be done? Now need to go into labs and test :(
- Still need a working power source. Battery ordered, awaiting delivery
  but need contingency plans should this fail too. Can maybe construct a
  series battery pack that will fit along with the extra components.

19 / 02 / 18:
- Experiment with new speeds and reaction thresholds for OF
- Try running the new nav threads. See if shit goes south

notes:
- New power source installed; much more powerful than the old one, needs
  recalibration of OF sections


20 / 02 / 18:
- Willshaw code needs tested, need metrics to see how it is behaving/performing
- Create log file which is parseable for easy graph production.

21 / 02 / 18:
- Create modified log file utility for statistical data (done)
- Switch to image convolution instead of physical scanning (techincal
  limitations)
- Write python script for interpreting stats and producing graphs

notes:
- image of size 90x10 360 / 90 = 4; So 4 degrees azimuth per pixel
  Current algorithm rotates through 60 degrees +/-30 either side of the
  forward vector. So, could rotate -15 pixels, then rotate one-by-one
  for 30 iterations, checking to see how familiar that image is to the
  network. Find minmum as before; then compute angle and rotate to roughly
  that angle; the robot seemingly cannot handle small angles well.

  Note: current algorithm is meant to behave as described above; however
  due to imprecise control, the robot does not act as it is meant to.
  I do not think that having a gyroscope or compass for control purposes
  is unreasonable. May suggest this for future iterations.





  
  


Todo and notes file for Project:

02 / 10 / 17:
todo:
 - Optical flow detection implementation
   > onCvCameraFrame is the main callback method used (everything happens
     in response to a new visual stimulus
   > From here the threads are run
   > Looking at repurposing existing method computeSparseOpicFlow()
     originally used for speed calculation. Not used anywhere else, so
     may be modified.
 
notes:
 - Optical flow paper: http://www.araa.asn.au/acra/acra2005/papers/low.pdf ::CITE::
   > Detect corners using sparse flow and goodFeaturesToTrack
   > Compute range over a series of frames
   > Halt if below some threshold
   > Tune manually

 - Cross correlation matching?

 - Turns out the optical flow as described by Luca isn't actually there
   > Actually I was wrong, it is, but it's not in the flow computation method; it's in the getSpeeds method
 - Seems to be a number of implementations of image rotation/reshaping/remapping


 - Line 2117: if ( Maths.abs( mod((int) currentPointsTotrack.get(i,j)[0] + 12, 90) - mod((int) prevPointsToTrack.get(i,j)[0] + 12, 90)) < 70 ) {...}
   Taking a distance between two x coordinates of points + 12, this is the pixel offset for the left hand side.
   Looks like, if the distance difference between points in the image (mod 90 for image overlap), is greater than 70, then we have enough information to compute
   a flow vector (IMPORTANT FOR OBSTACLES)

 - So, where to implement my stuff?..
   >Can add functions in MainActivity.java - Function for detection? Then work the behaviour into the appropriate threads (i.e. if clear move step else halt)

progress:
- Actually know how the existing stuff works. The method described by Luca IS present despite what I initially thought; he computes the flow vectors manually
- Added menu options and appropriate back-end code
- Have an idea as to how to implement behaviour:
  A low level (instinct level) function running on each frame to check distances using the methods described in the paper linked above


06 / 10 / 17:
- Getting this optical flow off the ground
  >Compute the focus of expansion

- May need to restructure existing OF flow code so sparse code may be used without
  messing with dense code
  
notes:
- Papers for OF: http://www.dgp.toronto.edu/~donovan/stabilization/opticalflow.pdf
                 http://journals.sagepub.com/doi/full/10.5772/5715

progress:
- Code for computing FOE is in place
- Need to look at using this info
		 
08 / 10 / 17:
- Compute velocity of the robot.
  > For this need to build new arduino functions.
  > Unless I can use getSpeedsFromSparse/Dense flow
    - Will try this method first to avoid fiddling with Arduino

- Compute TTC
- Maybe compute image depth
- Build a basic thread to get the robot to detect an obstacle and halt

notes:
- No speed retrieval methods on hardware apparently
  > How was Luca using the encoders???

- No idea what metric my TTC is in, assuming millisecs but could be secs. Check to see
  if V was per second, TTC should be the same.

progress:
- Have now got code for computing TTC, and hence image depth should be ok
  (Depth = TTC * V)

- Have a very simple test thread to check if the system actually works. How to debug this
  I am completely lost. Could try drawing flow arrows and printing debug info to
  the runOnUiThread.

- Debugging on the robot time!

09 / 10 / 17:
- Get code working

notes:
- Running seems inconsistent (Being stupid, make sure arduino is on
  before trying to run)

- Need to use prevPoints, and currentPoints BEFORE they're used for left/right flow

- IMPORTANT: MATRIX MULTIPLICATION IN ANDROID uses Core.gemm() function to do proper matrix dot product


15 / 10 / 17:
notes:
- Not the most productive week, sidetracked by CT.
- Need to fill out this file for every work session; need to know what was happening when I stopped!

progess:
- Currently debugging AntEye, calculation of the FOE is producing strange results
- Computing u and v is fine (results look weird but worry about that later), then when
  added to A, they show strange, erratic values. May be worth a Stack Overflow question God forbid.
  - Would be worth checking b too to make sure the method does what it's suppoesed to.
  - Otherwise, find a solution quickly, too small a problem to hang about over.

16 / 10 / 17:
- Figure out Mat.put() problem (DONE)
  > Matrix indexing is still from 0...
  > Still getting some strange outputs
  > Fixed, due to matrix initialisation + use of pushback
  > Check b, seems fine

- Refactor code for A and b calculations (DONE)
  > double should be float[] (for u and v)
  > remove debug outputs

- Get true FOE (wrap values around axis to get a value) (DONE)

- Work on TTC calculation, look for bugs.

notes:
- Need to be careful with matrix ops, can be finicky
- Should FOE be absolute? Negative FOE x or y does't make sense
  > Not absolute, wrapped, e.g. -2.5 should be 87.5 (subtract from 90 if neg)
    fx = -95
    fx = Math.abs(fx) = 95	
    fx = fx % 90 = 5
    fx = 89 - 5 = 84

- Printing the correct value tends to help when debugging...
- FOE is returning a 2x1 despite output saying it's 1x2 which is strange
  and annoying.

progress:
- Seem to have a working FOE calculation and this is being sucessfully passed
  up the chain to the obstacle detection function

- Aside: Might be a good idea to draw the FOE on screen, look into this

27 / 10 / 17:
- Draw FOE on debug image
- if wrong, then fix
- if looks correct, split image

notes:
- Speed is changing erratically
- OF information for speed is the same as that for FOE, could be causing problems


30 / 10 / 17:

notes:
- Need a way of saccading; idea, compute left and right flow fields
  as Luca did. Then look for FOE in each of these

  [     | .   ]  // Say image is centered at -45 (left flow) FOE should be thereish

  If it's anywhere to the left, then we need to..

  Ok, don't split the frame, just look for FOE, then turn away from it

- Need to figure out how foe would change based on flow field...


06 / 11 / 17:
today:
- Fix movement and stopping commands (DONE)
- TTC gone wild??? WHY?!
  - Soln, take average ttc values (possibly average out the FOE as well.
  - This is getting stupid.
  - Taking average FOE over 5 frames, looks more consistent than before.

notes:
- Lost time due to ADB and weird TTC issues.
- FOE averageing:
  - Init global_foe to (45, 5)
  - FOE is a 2x1, ensure all matrices reflect this, stupid amount of null
    pointer exceptions
  - global_ttc should be just above whatever threshold you are setting
  

10 / 11 / 17:
today:
- Normailize input image

notes:
- RGBA normalisation is making the app noticably slower, this may present an issue.
- Now, only normalize the blue channel for effiency but still slow.

progress:
- Little; normalisation had no noticeable affect on the calculations + behaviour

17 / 11 / 17:
today:
- Draw points on the debug frame, hope to confirm suspicions about
  cvGoodFeaturesToTrack();

- Switch to using dense flow for all optic flow (DONE)
- Move to speed from encoders, either how Luca did it, or from the arduino direct
  
- Test
- Look into creating an optical flow filter as in:
  http://jeb.biologists.org/content/jexbio/213/11/1886.full.pdf

  Looks like a uniform flow filter shifted left and right then overlaid.

notes:
- cvGoodFeaturesToTrack():
  - Movement is being detected where there is none, small but varied
  - Somewhat inconsistant, new features are being picked up and vanishing while
    the robot is sat still. Take images to document.
    - Images taken, better examples could be collected.

- Wheel encoders:
  - It looks like the arduino keeps track of these, but:
    1. Not for the go command. No encoder information is collected or sent
    2. In it's other uses, it is used internally, not sent back, where is this
       message?
    3. Need to find this, otherwise I'm going to have to edit Arduino code which is
       a scary prospect.

- Have not tested

progress:
- Determined that cvGoodFeatures was giving bad/inconsistent information
- Switched flow methods
- Closer to true wheel encoder usage, lord knows what was going on before
- Have an idea of what to do flow filter wise, but need to look into practical
  implementations of such a filter.

20 / 11 / 17:
todo:
- quick tests of new OF
- check Luca's filters for OF against his dissertation to see what he was doing
- figure out how broadcasting and recieving works here, need those encoder values
  and they're not being sent
- Get Leon's email.

notes:
- prevPointsToTrack for some reason not initialised in computeDenseOpticFlow
- prevPointsToTrack not used at all for dense flow
- Testing did not happen due to this

21 / 11 / 17:
todo:
- fix FOE calculation for dense flow (DONE)
- more work on flow filtering

notes:
- old code for computing foe was downright strange, be careful with new stuff
- luca's flow filtering was done using CX_Holonomic.get_prefered_flow() which
  uses his method of projecting the vectors onto a unit circle.
- the left and right preferred flow will be identical as the directional flag is
  never used

  How can I use this?

23 / 11 / 17:
notes:
- Luca's filtering done using the dot product i.e. the projection of the actual
  viewed flow onto the expected.

- TTC now infinite, distances seem more realistic and FOE is consistant
- Need to figure out image depth to use TTC, otherwise also, FLOW FILTERING!!!

24 / 11 / 17:
- Solve Infinite TTC (DONE)
- Filtering: figure it out. (DONE, I think...)
  - How to gain a filter from Luca's code (known, figured out a couple of days ago)
  - How to use this to detect obstacles???
    - For each flow vector detected:
      	  project it onto the expected (dot product)
	  add the result to an accumulator
	  if we exceed some threshold
	  reset and saccade away from the FOE.

- Implement filtering for the purpose of collision avoidance (DONE)

- Look more into the orginal method using the OF paper, convinced I'm missing
  something

notes:
- Infinite TTC caused by fact that speed is less than one when the robot is
  stationary, we divide by speed^distances which comes out as ~0.02^900 -> 0
  Dividing by 0 gives infinity in Java

- Solution?: Could just set speeds to 1 if sufficiently small.
- Could test to see if it drops when actually running the bot.
  - Tests show it does, though not sure how it behaves
  - Answer: strangely. TTC goes from infinite to slightly greater than 0
    in one jump.
  - Problem: As soon as the power was actually computable (big enough to where,
    it wouldn't -> 0), it becomse infinite as you're still going to the power of
    900. Attempted solution was to scale back the power by the same degree that we
    increased the speed, however no joy. It only takes one sufficiently large
    speed reading to throw the whole thing off.
  - May finally be time to accept that this approach will not work.
    
- Maybe remove normalisation for performance
  - Was already off for some reason?
- Moment of realisation when all of my flow filtering code essenially does
  what Luca's already did. So, I feel like I may be computing speed information?

progress:
- Figured out some problems with the old CA approach
- Implemented an OF filter for Collision Avoidance.
- Now need to test with this and tune it to see what thresholds would be
  appropriate.

- Construct a testing environment; observe flow values as obstacles approach on
  either side.

- Still need to send Leo an email to sort out the encoders, but the dense flow
  speed actually looks a lot more consistent.

28 / 11 / 17:
todo:
- Send Leo an email RE encoders. (DONE)
- Experiment with collision avoidance filtering
  (Something that would be good, more to write about)
  - Modify obstacle detection thread (DONE) / Create new thread + menu option using
    the filter instead of ttc.
  - Look at how the L/R flow's behave with obstacles approaching, slightly
    angled, and directly ahead. (IP, see notes);

- Dense flow speed looks good, having an encoder option would be good though

notes:
- making test thread:
  - For now, just want to set the robot to go and watch the values change.

- testing:
  Initial runs:
  - right flow always seems to be larger than left. why?
  - may try slowing the bot down a little

  Object pass, object on the left:
  - Left value, dropped sometimes, didn't change others. On passing the value consistantly increased
  - Right value, unaffected (good)

  Object pass, object on the right:
  - Left value, no variation
  - Right value, inconsistant variation, seemed to drop sometimes, no change others.
  - Right value still greater by about 10, (so by about 1000 in reality), what is the reason for this discrepancy???

  General:
  - Value on the opposite side to the object will not change which was expected.
  - Value on the object side seems to drop, but it's not consistant. 

Barbara's model used an accumulator, I'm not accumulating flow information, just
the immediate result from the filter.

I was implementing flow filters in the way Luca did in an effort to understand
how they worked. Now I feel like I do. The current flow sums are coming out as
negative. So, a majority of them are negative (from the looks of the values,
all of them). Therefore, the angle between the filter and the actual vectors is
greater than 90 degrees. Hmm....

- Sorted the negative values


progress:
- Sent Leo that email, if no response within a reasonable time, I will triple
  check then implement my own code in the go command on the arduino

- Debugged my filtering code. Taking negatives was the wrong approach; when
  breaking it down it became apparent that the filter was being applied to
  the wrong side of the input image (the +/- 12 terms were reversed).

- Now my filter values look better; note, something approaching from behind will
  cause the value to drop and a stop to be triggered, so this is good.

- I can now use the flow to manipulate the behaviour of the robot in a more
  consistant way than I could previous. Now I just need to figure out the
  output for an obstacle approaching input. Needs more experimentation but no
  time left today. Nice to actually be running experiments again...
  
29 / 11 / 17:
today:
- Experiment with flow filtering / some weighting
  - Left and right now very consistent
  - Need some way of differentiating/thresholding
    - Side that is approaching faster is greater
    - Aside: Know what Luca was doing with image shift now. (left is right vv)
    - Take side which is approaching faster, trigger turn in opposite direction
    - Do not observe flow info while turning

  - Idea: Use difference threshold instead of straight threshold?
  
- If getting more pertinant info, start adding behaviour

notes:
- Need to build some test boxes, reasonable size, painted black
- My code is noticeably slower than the old CX code...
  - Fixed, sleep(600) needed in thread loops to give the image processing
    time to execute.

progress:
- Fixed efficiency issues, not my image processing code, but related to
  synchronisation issues.
- Flow information now quite consistant
- Fixed image shift (left flow was being displayed and stored as right and vice
  versa)
- Now have some idea how to use this information!
  - Information is good, but ranges are a tad wild - Some tuning required 
  - Taking the flow difference then setting positive and negative threshold
  - Initial threshold value is +/-5000 for the difference
    - Stop, rotate +/- 45, move
    - Problem, at different points in motion, certain flow signals mean different
      things: e.g. when avoiding a negative signal shouldn't necessarily trigger
      a turn.

01 / 12 / 17:
today:
- Figure out how to threshold:
  - Sufficiently Big/Small/In some window?
  - Accumulation? Swing cancels any previous? (May be a good idea)
- Implement reactive behaviour.

notes:
Basic thresholding: Value > T => Trigger turn
- Working in avoidance thread now, trying to simply trigger a left turn
- An initial value for T = -5000 actually worked alright, but it seems
  sensitive to noise (e.g. simply passing things on the right)

- Utilising the "leaky accumulator" idea;
  accumulate readings x where x > T, this helps deal with some noise
   - still possible to trigger false saccades, in some cases
   
  could accumulate within a certain time period, or, take a series of readings
  with the same sign (e.g. if we see n contiguous readings of x, where x > |T|,
  then react).

Within Time Period:
- If you just add the ca_difference to the accumulators, if one breaches the
  threshold then there's a significant bias.

- Initial tests with dual threshold promising
- More motor command issues, stop not being registered.
  Initial value for AT chosen based on observations as robot approaches obstacles
  Initial value for RT chosen based on initial value for AT.

  1. AT = 5000, RT = 20000:
     - Functioned extremely well in some tests, poorly in others
     - Seemed to prefer turning right

  2. AT = 5000, RT = 30000:
     - Showed responses, though too late
       turn was being triggered, after a collision had occurred.

  3. AT = 5000, RT = 25000:
     - Again, response is triggered, though later than would be desired
     - Near collision or "bounce" off the surface
     - Can see promise with a different style of turn (-ve, +ve instead of arc)

     Repeat with alternate turn style:
     - Just bad
  
  - Did not take into account time allowed for samples:
    - Increasing time => Increased sensitivity
    
- Ongoing issue; need to fix speeds so AntBot actually goes straight

progress:
- Have something that is working in the way I need it to, however it needs tests
  and tuning (major)

- Need to play with thresholding some more

- Robot seems to prefer turning left for some reason, convinced there's some
  command buffer delay (i.e. issueing 3 or 4 go commands to avoid instead of one)

  - Maybe stop, then continue with new speeds, communication latency may be too
    great for continuous motion

  - It is reacting, but too late


04 / 12 / 17:
today:
- work with tuning the filter behaviour

notes:
1. AT = 5000, RT = 20000
   - Reacted well, but too slowly
   - Speeds are good check batteries.
   - Batteries fine.

   - Noticing inconsistency and repeated motion
     - Right turn triggered for seemingly no reason

2. AT = 5000, RT = 20000, No obstacle
   - 1st run:
     - Repeated slight left turns, last was large
     - No right motion despite that being the previously perceived bias.
   - 2nd run:
     - Started turning immediately
     - Followed a random looking pattern with some repeated movements
       (e.g. there was a distinct left/right swerve motion)

- Maybe a command queue issue?
  - Not this issue, but may be an underlying problem
  - The fact that the robot started moving before the activity was loaded is very odd
  - Could be thread resolution? Thread is never properly stopped

Added in thread timer: 15 seconds
Runs with no obstacle present
1. Initiated a left turn by the centre of the pitch and moved towards the opening
   turned right before reaching it

2. Initiated the same left turn near the centre of the pitch, another slight left turn where
   (1) turned right and headed towards the opening

3. Same as (2) with a strange short stop and final motor jump?

4. Different number of turns but still gravitates towards the opening in the mesh

From other end of the pitch:

5. Same right turns as previous tests

6. Again, not as severe as (5)

Ok, so the robot always seems to want to go right in an empty space. Always makes roughly three turns
of different sizes at roughly the same places. Always finishes in roughly the same place

Introduce obstacle on the right to see if a left turn is triggered instead of the standard right

7. Weaker right turn triggered but still right, not left. This is odd as nothing major has been changed

8. Same run as (7), Left turn triggered, far too early

- Changed time count method (counting loop iterations instead of raw time. Same behaviour still
- Change accumulators

- Changed reaction threshold to 10000,  works better. Accumulation threshold still 5000
  reasoning: very first test with no accumulation worked very well, thinking is that a lower
  reaction threshold allows multiple smaller values to make a difference rather than focusing on a
  few larger ones.

08 / 12 / 17:
today:
- work more with tuning
- emergency stop? how?

notes:
- Still issues with tuning
  - Seems to be that in general, lower reaction thresholds are better (too sensitive > not sensitive
    enough)
  - Experiment with dual vs individual accumulation
  - Still preferring a right turn WHY?!
  - Try running inverse scenario
    - May be background interference; nope...

  - Right turn preference may be resolved: Potential issue; speed problem mentioned previously
    the robot was always drifting to the left which may have been triggering that right turn.
    Solution: set the right speed to ~90% of the left and the robot will move in an almost straight
    line (slight right drift, but seems negligable). Robot now seems to react nicely to the obstacles.
    Should be noted that behaviour becomes unpredictable in an open space (background elements
    could be playing with the flow).

    Best results so far: AT = 5000, RT = 10000, Speed: 25 (22). Still don't have an emergency stop.
    May be worth trying a halt and turn

18 / 12 / 17:
- Optic Flow:
  - Try halt and turn (works waaaay better)
  - Find out what more wants done with this
  - Reaction time still too slow in some instances (How could this be fixed?)
    - Sensitivity is still there so can't be too over-zealous with the sensitivity...
    

- Mushroom Body / Willshaw:
  - Read...


notes:
- Don't worry about integrating OF into CX
- No downward image for speed computation
  

20 / 12 / 17:
today:
- MB / Willshaw, code walkthrough, commenting


notes:
- Willshaw, seems that first image is used to set thresholding for all subsequent images

19 / 01 / 17: Happy New Year!
today:
- MB/Willshaw familiarisation
  - Thank goodness I commented this the last time around
  - Seem to want to use between 200 and 400 KCs to learn the first image
  - This is used to ensure the threshold is set so that an appropriate amount
    of KCs are used.
  - Brightness is taken as a cumulative sum over all connected vPNs, not
    thresholding them individually.
    - These implementation details should be included in the background section
      of the dissertation.   
- Implement new thread for the following process:
  - Navigate for time T through obstacle course
  - Replace at start
  - Navigate the same route using vision
  - We will create copies of the Willshaw module for each new implementation
    for the sake of clarity; also I feel that the OF one will take a bit more
    modification.
- Plan modifications for MB model.
  - Real valued weightings
  - Optical flow snapshots: Honeybee paper
    - Saving the dense flow images which store the pixel and the pixels new
      location.
    - How to engineer this into the MB network?..
  - Need the paper for vPN weighting strategy

notes:

26 / 01 / 18:
- Add new menu item to run the route nav pattern (DONE)
- Write the CA with memory thread (DONE)
- Write the VN thread from MB (Planned)

notes:
- So far, the plan is to continue using scanning behaviour.
  Klinokinesis is more realistic, but the hardware is limiting
  in this case. We can't turn fast enough using klinokinesis in
  a densly populated environment.

08 / 02 / 18:
- Finish basic VN thread (Done-ish but could maybe use refinement)
  
- Make modified Willshaw network

notes:
- Images are stored every second. This should hopefully be enough to allow the
  robot to recapitulate the route with a fine level of detail.

- Thought: Image of robot looking at a box before a turn and image of next
  section in the route may have similar levels of familiarity. Learning rate
  could play a difference hear

- Do I want OF and vn going at the same time? I.e. an OF CA trigger will
  trigger a scan instead of an avoidance saccade? Or could I move for 2 seconds
  triggering saccades as necessary?

- First option preferable. CA warning will trigger a scan.
  - Obstacle detected: directional decision was made here
  - Also avoids issues with running into the box then scanning, or scanning
    while right next to something

13 / 02 / 18:
today:
- Write RealWillshaw code : Done, I think
- Come up with experiment structure for Real : Done
  - One learning CA run
  - Then learning memory runs (i.e. run the robot from memory, save
    the most familiar image).
- Write experiment structure for real : In progress...

- In lab:
  - Finish hardware stuff (exception of battery)
  - Measure available space; buy battery ASAP

notes:
- RealWillshaw
  - Every KC starts with a weighting of 1
  - Upon activation KC weigting is: w = w * r; where r is learning rate
    starting with r = 0.9;


14 / 02 / 18:
- Still have no defined end point for the nav thread.

notes:
- Nav threads appear to be done? Now need to go into labs and test :(
- Still need a working power source. Battery ordered, awaiting delivery
  but need contingency plans should this fail too. Can maybe construct a
  series battery pack that will fit along with the extra components.

19 / 02 / 18:
- Experiment with new speeds and reaction thresholds for OF
- Try running the new nav threads. See if shit goes south

notes:
- New power source installed; much more powerful than the old one, needs
  recalibration of OF sections


20 / 02 / 18:
- Willshaw code needs tested, need metrics to see how it is behaving/performing
- Create log file which is parseable for easy graph production.

21 / 02 / 18:
- Create modified log file utility for statistical data (done)
- Switch to image convolution instead of physical scanning (techincal
  limitations)
- Write python script for interpreting stats and producing graphs

notes:
- image of size 90x10 360 / 90 = 4; So 4 degrees azimuth per pixel
  Current algorithm rotates through 60 degrees +/-30 either side of the
  forward vector. So, could rotate -15 pixels, then rotate one-by-one
  for 30 iterations, checking to see how familiar that image is to the
  network. Find minmum as before; then compute angle and rotate to roughly
  that angle; the robot seemingly cannot handle small angles well.

  Note: current algorithm is meant to behave as described above; however
  due to imprecise control, the robot does not act as it is meant to.
  I do not think that having a gyroscope or compass for control purposes
  is unreasonable. May suggest this for future iterations.

22 / 02 / 18:
- Write a rough python script for producing graph data from stats file
- If time go to B&Q and get stuff for obstacles

notes:
- Implemented a rough scanning algorithm which uses image rotation
  This needs a test to see that it doesn't crash the phone, but it should work.
  The algorithm is simple but sound, it's the technicalities of Java that may
  cause issues.
  
- And, because its not yet noted; the file writing is working, but for some
  reason /storage/emulated/0/ doesn't show up in the file browser on my linux.
  If you go on the phone's file explorer, the files are visible; may be some
  cache refreshing trick. This needs researched but is low priority.

26 / 02 / 18:
in the break:
- bought supplies for obstacles from b&q
  - measured antbot, blocks should be on their edge with plants glued on top
todo:
- debug scanning
  - -ve rotations were accounted for but not in loop arithmatic resulting
    in a lot of indexing problems
  - have an index of 7515? pixels
  - after fixing loop references scanning now works without crashing, some
    wild indices are still present; why?
  - I am literally retarded; thought I was adding when I was concatenating.

- test scanning to see if it works
  - it does, the network doesn't... see below

notes:
- Turning is done using set wheel speeds and encoder values; so power source
  may be causing over turning but I think this is unlikely

- Small angles seem imprecise but large ones are roughly correct
  - Reccommendation for future iterations is to introduce more complex robot
    control systems

- some ideas came up today which are worth exploring
  - Untrained network image recognition:
    How does an untrained network recognise images?
    Easy enough to check, comment out the code from the learning run
    Expect: All angles should return high values, none should be
    especially low.
    What if?: If they are low then there is something fundamentally wrong
    with this implementation of the network which needs addressed before
    any experiments can take place. It is likely that this will kill
    experiments
    
  - Save images from scans:
    Firstly to make sure that the scanning is functioning as expected
    Secondly to look at how similar the images are, is the scan to
    fine?
    Rough method; is there a way of saving images from openCV? If
    not then add heatmapping functionality to AntBotStats and output it
    Expect: Scanning to work as it should, images should all be off by one
    pixel from the last in the desired direction
    What if?: If this doesn't work its likely to do with the way the
    whole image copying process works, however, the fact that there
    are different familiarity results being returned; this should not be
    the case if the image copying doesn't work.

  - Find a way of representing the network:
    I want to be able to see a visual representation of the network
    However, I have no idea how I can do this. Need a compact, yet
    descriptive representation of 900 inputs connected randomly to
    20,000 neurons, all running to a single neuron

    Idea that could mostly be implemented in AntBotStats
    Show image, heatmap of KC activation, output value of EN

    Image can be given as a heatmap or potentially a jpg. KCs can
    be formatted into a 100x200 matrix ant printed as a heatmap.

27 / 02 / 18:
todo:
- check untrained model (done)
- troubleshoot...

notes:
- untrained model returning zero on images (partially explainable from
  the fact that the EN sums KC activation but this does not account for
  as much + range)

- lighting in the room likely part of the problem (turn on image normalisation again to
  see if there's a difference).
  - Number of activated KCs does seem to be affected by lighting; there is still
    the bias on one side, but the range is much less. Still high but less.


06 / 02 / 18:
interim:
- graphing tool partially extended for showing the network
- obstacles created and tests run; antbot can see them

todo:
- willshaw fiddling:
  - increasing threshold -> learnedKCs is lower which is good
  - scan may be too fine-grained lots of zero entries in a particular
    part of the array

  - half the number of scans (double the pixel distance)
  - This was a good shout, the 31 scan routine had too much detail
    which was causing confusion.

  - Still getting many zero entries:
    What could cause a zero?
    All KCs activated for this image were used for learning

    No KCs activated for this image. error is initialised to 0
    - ^this. producing output to stat files has showed that the number
       of KCs being activated for a given image is either very low
       or 0; thus the probability of those KCs having been used for learning
       at some point is quite high.

  - Need to wrap up now, brightness threshold is currently 2000. this could go up a
    smidge.

  - Error needs initialised to something non-zero; the way of computing is nice and
    simple but it also means that less KCs being activated automatically makes for
    a more familiar image as no KCs being activated will result in a familiarity of 0.
    - Perhaps maximum KCs; then error = error - willshaw...
    - Error would likely never be zero, but it should have a more definite minimum.

  - good day.
    


07 / 03 / 18:
todo:
- Alter familiarity calculation
- Experiment with normalised PN input
  - PN value = val / sum(all_vals)

notes:
- pls work
  - Initial improvement may have been a fluke

- Still have the same problem only now it's inverted
  Problem:
	error init to zero
  	each time we activate a KC add its weight
	minimum error value denotes most familiar image
	threshold -> we need to activate a sufficient number of KCs each time
	~200
	For those which activate no KCs, we return the max learned KCs; maybe
	not guaranteed to be a maximum, but should always be greater than the minimum

	New problem
	If we do not activate enough KCs then the sum will be low
	Lower the threshold to ensure enough are active.

- PN normalisation causing weird hang in learn image for first image. This could be
  due to threshold adjustment

10 / 03 / 18:
todo:
- Attempt to fix Willshaw net
  - Check PN normalisation values, make sure that loop terminates
    - Was being silly, as per, doing integer division at that scale results in 0
    - Now have a normalised image to work with (fast too).
- Run some experiments on OF systems
  - This didn't happen but for good reason; implemented and roughly debugged
    a system whereby we only pay attention to a frontal arc for obstacle avoidance
    - This wasn't done previously because I had spent so much time on OF, though
      it turned out to be straightforward-ish
    - This is a better system, it just needs tuned; also more biologically plausable
      as insects have certain ommatadia which are higher definition (need a reference
      for this).

notes:



11 / 03 / 18:
todo:
- get functionality back in the OF system; new system is better but needs fixed

12 / 03 / 18:
todo:
- fix the OF; start with the old, then move to the new

notes:
- OF tests:
  - Double check against git code; then check with box
    - Took old code direct from github
     - Test with the box was as successful as previous tests and fairly consistent
       - Behaviour is fairly inconsistent now? - This worked well before?
       - Increased speeds, box tests seem to work but affected by side information
       

     - Test with tussocks:
       - Problem: the way the camera is set up means that the image is warped. We don't
       	 get a band, we get a ring. In essence, far more is visable than should be when using
	 this robot

       - Looked at the camera frame; two things, one, the attachment isn't centered
       	 (I don't think this is an issue); two, part of the image was obscured which
	 explains why there was a large obstruction on one side of the image which
	 I couldn't explain before. This may explain a lot of problems with the optic flow.
	 Need to "calibrate" before all tests from now on; this was stupid.

       - OF Acutally seems to work fairly well now in general, however, the robot
         fails to turn left with tussocks, only right; however, it will turn left
	 away from the netting which is odd. (Behavioural note)

       - Setting up the arena to test with that.
       - Basic OF works now to the same degree as original functionality; camera obstruction
         was likely playing a huge part in the problems observed.

       - Quick test of focused flow 30 degree window, then leave if it doesn't work
         within two modifications (good food for thought if there are problems though)

	 Seems to work ( may require some tuning as it seems to be unpredictable
	 in dense environments. )

- Willshaw Network:
  - Back to the matter in hand. KC activation is still seemingly far too low.
    A hacky fix is to threshold the output of the network. If too few KCs are
    activated we return the maximum unfamiliarity; in the same fashion if the
    unfamiliarity is greater than the maximum unfamiliarity; the scan is ignored.
    Using this fix has improved performance as; in the cases the image wasn't
    recognised, often the best course of action was to keep going straight but this
    is a poor fix, there is something wrong with the network. Now I need to figure out
    what it is...

    - Thankfully the OF is back to its original standard (generally good with some mistakes).

  - Modified the arena so that OF played a minimal role, clearly there is still some tuning
    issues (May be the spinlock but I think the frontal arc is still too wide)

    - With this modified arena the willshaw network (sort of) worked. The robot did not
      get the first part of the route but seemingly recovered (how much sense does this make?)
      The robot ended roughly where it ended on the initial run. More tests of this nature
      required. This shows that the hacky modifications are cleaning up the data but
      there has to be a better way of doing this...

	 
14 / 03 / 18:
yesterday:
- Noticed problems with the image rotation; if anybody ever tells you references don't
  exist in Java, slap them.
- This explained the large problem of KC activation inconsistency.
- Test method for discussion: Leave the robot sitting still, allow it to go through the learning
  routine and learn some images, these should largely activate the same KCs

  Then allow the robot to try to recall the images. The same KCs should be activated; they weren't
  Removing the image rotation demonstrated the issue.

todo:
- Fix image rotation; algorithm needs improved for -ve rotation.
- Fix image rotation; make copies of the image and rotate them, rather than rotating the image
  passed in. It's passed by reference which is screwing everything up.
- Test willshaw with new image rotation

notes:
- Image rotation fixed, now working well
- When sat still we expect the following:
  - Learned KCs and total KCs high for first image
  - LearnedKCs low and total KCs high for the remaining images
  - On recall the scan should consistently choose index 8 (forward as most familiar)
  - This worked as expected
  - Image rotation likely worked before but algorithm is now improved and reference accounted for

- VICON:
  - ViconTracker software.
  - Should be a case of load, select object, then record. See about outputting data to a CSV
    then parsing and plotting using ABS

- Willshaw:
  - Now functions a lot better, KC activation is about perfect I think
  - The lower threshold is still good I think, but the upper one is no longer
    required. I am not seeing any KC activations in the range that they were
    previously
    
  - Unsure as to why it functions so much better now than it did when using scanning originally
  - The upper threshold is set to an experimental value of 600, though it may now be removable.

OPTICAL FLOW EXPERIMENTS:
- Now that everything seems to work, I will move on to experiments.

Section 1: correctness testing
ARENA 1: Basic functionality - clear pitch
- Clear with no tussocks present in the arena
- Robot started centrally in the South end of the pitch
- The robot should navigate to the box at the North end of the pitch
- Some spurious flow reactions will be triggered despite the high walled arena.
- This is due to the distorted FOV AntBot has. This MUST be mentioned in the report.

A1E:
- We expect zero collisions of any sort on this run.

ARENA 2: Basic functionality

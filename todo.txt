Todo and notes file for Project:

02 / 10 / 17:
todo:
 - Optical flow detection implementation
   > onCvCameraFrame is the main callback method used (everything happens
     in response to a new visual stimulus
   > From here the threads are run
   > Looking at repurposing existing method computeSparseOpicFlow()
     originally used for speed calculation. Not used anywhere else, so
     may be modified.
 
notes:
 - Optical flow paper: http://www.araa.asn.au/acra/acra2005/papers/low.pdf ::CITE::
   > Detect corners using sparse flow and goodFeaturesToTrack
   > Compute range over a series of frames
   > Halt if below some threshold
   > Tune manually

 - Cross correlation matching?

 - Turns out the optical flow as described by Luca isn't actually there
   > Actually I was wrong, it is, but it's not in the flow computation method; it's in the getSpeeds method
 - Seems to be a number of implementations of image rotation/reshaping/remapping


 - Line 2117: if ( Maths.abs( mod((int) currentPointsTotrack.get(i,j)[0] + 12, 90) - mod((int) prevPointsToTrack.get(i,j)[0] + 12, 90)) < 70 ) {...}
   Taking a distance between two x coordinates of points + 12, this is the pixel offset for the left hand side.
   Looks like, if the distance difference between points in the image (mod 90 for image overlap), is greater than 70, then we have enough information to compute
   a flow vector (IMPORTANT FOR OBSTACLES)

 - So, where to implement my stuff?..
   >Can add functions in MainActivity.java - Function for detection? Then work the behaviour into the appropriate threads (i.e. if clear move step else halt)

progress:
- Actually know how the existing stuff works. The method described by Luca IS present despite what I initially thought; he computes the flow vectors manually
- Added menu options and appropriate back-end code
- Have an idea as to how to implement behaviour:
  A low level (instinct level) function running on each frame to check distances using the methods described in the paper linked above


06 / 10 / 17:
- Getting this optical flow off the ground
  >Compute the focus of expansion

- May need to restructure existing OF flow code so sparse code may be used without
  messing with dense code
  
notes:
- Papers for OF: http://www.dgp.toronto.edu/~donovan/stabilization/opticalflow.pdf
                 http://journals.sagepub.com/doi/full/10.5772/5715

progress:
- Code for computing FOE is in place
- Need to look at using this info
		 
08 / 10 / 17:
- Compute velocity of the robot.
  > For this need to build new arduino functions.
  > Unless I can use getSpeedsFromSparse/Dense flow
    - Will try this method first to avoid fiddling with Arduino

- Compute TTC
- Maybe compute image depth
- Build a basic thread to get the robot to detect an obstacle and halt

notes:
- No speed retrieval methods on hardware apparently
  > How was Luca using the encoders???

- No idea what metric my TTC is in, assuming millisecs but could be secs. Check to see
  if V was per second, TTC should be the same.

progress:
- Have now got code for computing TTC, and hence image depth should be ok
  (Depth = TTC * V)

- Have a very simple test thread to check if the system actually works. How to debug this
  I am completely lost. Could try drawing flow arrows and printing debug info to
  the runOnUiThread.

- Debugging on the robot time!

09 / 10 / 17:
- Get code working

notes:
- Running seems inconsistent (Being stupid, make sure arduino is on
  before trying to run)

- Need to use prevPoints, and currentPoints BEFORE they're used for left/right flow

- IMPORTANT: MATRIX MULTIPLICATION IN ANDROID uses Core.gemm() function to do proper matrix dot product


15 / 10 / 17:
notes:
- Not the most productive week, sidetracked by CT.
- Need to fill out this file for every work session; need to know what was happening when I stopped!

progess:
- Currently debugging AntEye, calculation of the FOE is producing strange results
- Computing u and v is fine (results look weird but worry about that later), then when
  added to A, they show strange, erratic values. May be worth a Stack Overflow question God forbid.
  - Would be worth checking b too to make sure the method does what it's suppoesed to.
  - Otherwise, find a solution quickly, too small a problem to hang about over.

16 / 10 / 17:
- Figure out Mat.put() problem (DONE)
  > Matrix indexing is still from 0...
  > Still getting some strange outputs
  > Fixed, due to matrix initialisation + use of pushback
  > Check b, seems fine

- Refactor code for A and b calculations (DONE)
  > double should be float[] (for u and v)
  > remove debug outputs

- Get true FOE (wrap values around axis to get a value) (DONE)

- Work on TTC calculation, look for bugs.

notes:
- Need to be careful with matrix ops, can be finicky
- Should FOE be absolute? Negative FOE x or y does't make sense
  > Not absolute, wrapped, e.g. -2.5 should be 87.5 (subtract from 90 if neg)
    fx = -95
    fx = Math.abs(fx) = 95	
    fx = fx % 90 = 5
    fx = 89 - 5 = 84

- Printing the correct value tends to help when debugging...
- FOE is returning a 2x1 despite output saying it's 1x2 which is strange
  and annoying.

progress:
- Seem to have a working FOE calculation and this is being sucessfully passed
  up the chain to the obstacle detection function

- Aside: Might be a good idea to draw the FOE on screen, look into this

27 / 10 / 17:
- Draw FOE on debug image
- if wrong, then fix
- if looks correct, split image

notes:
- Speed is changing erratically
- OF information for speed is the same as that for FOE, could be causing problems


30 / 10 / 17:

notes:
- Need a way of saccading; idea, compute left and right flow fields
  as Luca did. Then look for FOE in each of these

  [     | .   ]  // Say image is centered at -45 (left flow) FOE should be thereish

  If it's anywhere to the left, then we need to..

  Ok, don't split the frame, just look for FOE, then turn away from it

- Need to figure out how foe would change based on flow field...


06 / 11 / 17:
today:
- Fix movement and stopping commands (DONE)
- TTC gone wild??? WHY?!
  - Soln, take average ttc values (possibly average out the FOE as well.
  - This is getting stupid.
  - Taking average FOE over 5 frames, looks more consistent than before.

notes:
- Lost time due to ADB and weird TTC issues.
- FOE averageing:
  - Init global_foe to (45, 5)
  - FOE is a 2x1, ensure all matrices reflect this, stupid amount of null
    pointer exceptions
  - global_ttc should be just above whatever threshold you are setting
  

10 / 11 / 17:
today:
- Normailize input image

notes:
- RGBA normalisation is making the app noticably slower, this may present an issue.
- Now, only normalize the blue channel for effiency but still slow.

progress:
- Little; normalisation had no noticeable affect on the calculations + behaviour

17 / 11 / 17:
today:
- Draw points on the debug frame, hope to confirm suspicions about
  cvGoodFeaturesToTrack();

- Switch to using dense flow for all optic flow (DONE)
- Move to speed from encoders, either how Luca did it, or from the arduino direct
  
- Test
- Look into creating an optical flow filter as in:
  http://jeb.biologists.org/content/jexbio/213/11/1886.full.pdf

  Looks like a uniform flow filter shifted left and right then overlaid.

notes:
- cvGoodFeaturesToTrack():
  - Movement is being detected where there is none, small but varied
  - Somewhat inconsistant, new features are being picked up and vanishing while
    the robot is sat still. Take images to document.
    - Images taken, better examples could be collected.

- Wheel encoders:
  - It looks like the arduino keeps track of these, but:
    1. Not for the go command. No encoder information is collected or sent
    2. In it's other uses, it is used internally, not sent back, where is this
       message?
    3. Need to find this, otherwise I'm going to have to edit Arduino code which is
       a scary prospect.

- Have not tested

progress:
- Determined that cvGoodFeatures was giving bad/inconsistent information
- Switched flow methods
- Closer to true wheel encoder usage, lord knows what was going on before
- Have an idea of what to do flow filter wise, but need to look into practical
  implementations of such a filter.

20 / 11 / 17:
todo:
- quick tests of new OF
- check Luca's filters for OF against his dissertation to see what he was doing
- figure out how broadcasting and recieving works here, need those encoder values
  and they're not being sent
- Get Leon's email.

notes:
- prevPointsToTrack for some reason not initialised in computeDenseOpticFlow
- prevPointsToTrack not used at all for dense flow
- Testing did not happen due to this

21 / 11 / 17:
todo:
- fix FOE calculation for dense flow (DONE)
- more work on flow filtering

notes:
- old code for computing foe was downright strange, be careful with new stuff
- luca's flow filtering was done using CX_Holonomic.get_prefered_flow() which
  uses his method of projecting the vectors onto a unit circle.
- the left and right preferred flow will be identical as the directional flag is
  never used

  How can I use this?

23 / 11 / 17:
notes:
- Luca's filtering done using the dot product i.e. the projection of the actual
  viewed flow onto the expected.

- TTC now infinite, distances seem more realistic and FOE is consistant
- Need to figure out image depth to use TTC, otherwise also, FLOW FILTERING!!!

24 / 11 / 17:
- Solve Infinite TTC (DONE)
- Filtering: figure it out. (DONE, I think...)
  - How to gain a filter from Luca's code (known, figured out a couple of days ago)
  - How to use this to detect obstacles???
    - For each flow vector detected:
      	  project it onto the expected (dot product)
	  add the result to an accumulator
	  if we exceed some threshold
	  reset and saccade away from the FOE.

- Implement filtering for the purpose of collision avoidance (DONE)

- Look more into the orginal method using the OF paper, convinced I'm missing
  something

notes:
- Infinite TTC caused by fact that speed is less than one when the robot is
  stationary, we divide by speed^distances which comes out as ~0.02^900 -> 0
  Dividing by 0 gives infinity in Java

- Solution?: Could just set speeds to 1 if sufficiently small.
- Could test to see if it drops when actually running the bot.
  - Tests show it does, though not sure how it behaves
  - Answer: strangely. TTC goes from infinite to slightly greater than 0
    in one jump.
  - Problem: As soon as the power was actually computable (big enough to where,
    it wouldn't -> 0), it becomse infinite as you're still going to the power of
    900. Attempted solution was to scale back the power by the same degree that we
    increased the speed, however no joy. It only takes one sufficiently large
    speed reading to throw the whole thing off.
  - May finally be time to accept that this approach will not work.
    
- Maybe remove normalisation for performance
  - Was already off for some reason?
- Moment of realisation when all of my flow filtering code essenially does
  what Luca's already did. So, I feel like I may be computing speed information?

progress:
- Figured out some problems with the old CA approach
- Implemented an OF filter for Collision Avoidance.
- Now need to test with this and tune it to see what thresholds would be
  appropriate.

- Construct a testing environment; observe flow values as obstacles approach on
  either side.

- Still need to send Leo an email to sort out the encoders, but the dense flow
  speed actually looks a lot more consistent.

28 / 11 / 17:
todo:
- Send Leo an email RE encoders. (DONE)
- Experiment with collision avoidance filtering
  (Something that would be good, more to write about)
  - Modify obstacle detection thread (DONE) / Create new thread + menu option using
    the filter instead of ttc.
  - Look at how the L/R flow's behave with obstacles approaching, slightly
    angled, and directly ahead. (IP, see notes);

- Dense flow speed looks good, having an encoder option would be good though

notes:
- making test thread:
  - For now, just want to set the robot to go and watch the values change.

- testing:
  Initial runs:
  - right flow always seems to be larger than left. why?
  - may try slowing the bot down a little

  Object pass, object on the left:
  - Left value, dropped sometimes, didn't change others. On passing the value consistantly increased
  - Right value, unaffected (good)

  Object pass, object on the right:
  - Left value, no variation
  - Right value, inconsistant variation, seemed to drop sometimes, no change others.
  - Right value still greater by about 10, (so by about 1000 in reality), what is the reason for this discrepancy???

  General:
  - Value on the opposite side to the object will not change which was expected.
  - Value on the object side seems to drop, but it's not consistant. 

Barbara's model used an accumulator, I'm not accumulating flow information, just
the immediate result from the filter.

I was implementing flow filters in the way Luca did in an effort to understand
how they worked. Now I feel like I do. The current flow sums are coming out as
negative. So, a majority of them are negative (from the looks of the values,
all of them). Therefore, the angle between the filter and the actual vectors is
greater than 90 degrees. Hmm....

- Sorted the negative values


progress:
- Sent Leo that email, if no response within a reasonable time, I will triple
  check then implement my own code in the go command on the arduino

- Debugged my filtering code. Taking negatives was the wrong approach; when
  breaking it down it became apparent that the filter was being applied to
  the wrong side of the input image (the +/- 12 terms were reversed).

- Now my filter values look better; note, something approaching from behind will
  cause the value to drop and a stop to be triggered, so this is good.

- I can now use the flow to manipulate the behaviour of the robot in a more
  consistant way than I could previous. Now I just need to figure out the
  output for an obstacle approaching input. Needs more experimentation but no
  time left today. Nice to actually be running experiments again...
  
29 / 11 / 17:
today:
- Experiment with flow filtering / some weighting
  - Left and right now very consistent
  - Need some way of differentiating/thresholding
    - Side that is approaching faster is greater
    - Aside: Know what Luca was doing with image shift now. (left is right vv)
    - Take side which is approaching faster, trigger turn in opposite direction
    - Do not observe flow info while turning

  - Idea: Use difference threshold instead of straight threshold?
  
- If getting more pertinant info, start adding behaviour

notes:
- Need to build some test boxes, reasonable size, painted black
- My code is noticeably slower than the old CX code...
  - Fixed, sleep(600) needed in thread loops to give the image processing
    time to execute.

progress:
- Fixed efficiency issues, not my image processing code, but related to
  synchronisation issues.
- Flow information now quite consistant
- Fixed image shift (left flow was being displayed and stored as right and vice
  versa)
- Now have some idea how to use this information!
  - Information is good, but ranges are a tad wild - Some tuning required 
  - Taking the flow difference then setting positive and negative threshold
  - Initial threshold value is +/-5000 for the difference
    - Stop, rotate +/- 45, move
    - Problem, at different points in motion, certain flow signals mean different
      things: e.g. when avoiding a negative signal shouldn't necessarily trigger
      a turn.

01 / 12 / 17:
today:
- Figure out how to threshold:
  - Sufficiently Big/Small/In some window?
  - Accumulation? Swing cancels any previous? (May be a good idea)
- Implement reactive behaviour.

notes:
Basic thresholding: Value > T => Trigger turn
- Working in avoidance thread now, trying to simply trigger a left turn
- An initial value for T = -5000 actually worked alright, but it seems
  sensitive to noise (e.g. simply passing things on the right)

- Utilising the "leaky accumulator" idea;
  accumulate readings x where x > T, this helps deal with some noise
   - still possible to trigger false saccades, in some cases
   
  could accumulate within a certain time period, or, take a series of readings
  with the same sign (e.g. if we see n contiguous readings of x, where x > |T|,
  then react).

Within Time Period:
- If you just add the ca_difference to the accumulators, if one breaches the
  threshold then there's a significant bias.

- Initial tests with dual threshold promising
- More motor command issues, stop not being registered.

  1. AT = 5000, RT = 20000:
     - Functioned extremely well in some tests, poorly in others
     - Seemed to prefer turning right

  2. AT = 5000, RT = 30000:
  

- Ongoing issue; need to fix speeds so AntBot actually goes straight





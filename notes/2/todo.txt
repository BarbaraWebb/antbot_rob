The List: Stuff I have to do but am putting off for time management's sake
- Re-write ABS (Planned to slot into ILW).
- Build obstacles (Also planned for ILW but could do with being before)
- Check the Equations in the dissertation background. Noticed a few wrong in
  the CX section.

Milestones:
- (Done!) Get old CX working (and recorded)
- (Done!) Finish interim report
- (Done!) Finish first three chapters
- (IP) Complete FOE OFCA model
- Integrate into CXMB model (I think this is my best bet although there may
  be scope for modifying the CXMB model).
- (OPTIONAL; if you have time!) create an alternate 8MBON model (I think Zhaoyu's
	came from Barbara	and is likely best to stick with); could maybe try real
	weightings in the KC-MBON synapses?

	Barbara has mentioned that there were issues with Zhang, however, I think these
	were the methodological issues I have discussed already. I already have CX
	tests to cover this, however, CXMB + OFCA would be awesome.

18 / 02 / 2019:
Todo:
- Check framerate when recording isn't running.
	- Roughly the same, maybe negligably higher.
	- On average, getting 2/3 frames per second with occasional spikes and dips
	- Speaking to Jan, he suggested looking at the framerate without the farneback
		computation; also, just checking out the framerate through the stages of
		image processing.
	- Raising the framerate may help with the optical flow.
	- All of the delays sprinkled through the code are also abnormal. Why do we
	  have to put sleep statements in each while loop? Unless this is a really
		terrible way of providing synchronisation.
	- When dealing with all of this, we have to acknowledge that matched filters
	  worked last year.
	- So, an avenue for exploration; relegate the image preprocessing to its own
		thread, then set this to be a max priority thread.
		- Same frame buffering method as VideoRecorder???
		- Probably not, we always need to know what the current frame is, but
			processing likely wont be complete by the time the next frame is passed
			to the application.
		- Could just treat the newest frame off the pile as the current one?
		- The same frames still need to be made global so they are visible to
			the rest of the system. Safety push, then get started...

	  - I'm using the same frame buffer system after all; I'd like to be able to
		  put the thread to sleep, but I don't think it'll be an issue.

		- Really want images to be available as much as possible. So, do we only
		  update the current image after all processing is complete? Supposedly
			slow, but I'm not so sure.

    - Basically; start moving code across and see what happens; don't get too
		  bogged down with details, but keep notes.

		- I'm genuinely furious. The amount of unused, stupid code here is ridiculous.
		- Aaaand, now I'm lost. I have no idea where I am in the process...

		- I may have stumbled on a much easier way to extract the blue channel but I
		  am loath to mess with it. 

- Play about with new sensors to see if you can get anything to work
- Look at TinkerCAD to model a piece for the front of the robot which
	can be used to mount the sensors.
- Seriously; have a think as to whether this is the best course of action.
	This is pretty extreme and it could easily have its own problems.

13 / 02 / 2019:
Notes:
- Plotting the flow field seems to show that the flow detected is miniscule and
	erratic.
- I've emailed Garry to look for alternatives as I don't think anything flow
	based	will do particularly well
- That said, I'm not sure the recordings are the best. Even as a human its hard
	to tell whats going on until the last minute.
- Comfortingly, Luca's code seems to give a similar reading for the flow
	magnitude.
- The AntBot gets about 2 frames per second. Seriously?


12 / 02 / 2019:
Wrap-up:
- Video recording utilities work; can plot the FOE on the video frame
- As expected there isn't really a reliable relationship between a looming
 	obstacle and the FOE position, although in the recording it does jump
	right
- I need to redo the recordings anyway, so I'll check again, but as a preliminary
  this is good, and I can start to look at alternatives.
07 / 02 / 2019:
Todo:
- Compile recordings into video
	(Should be doable with an ffmpeg command)
	(actually worked it into a python script so its reusable)
	- Works nicely, but highlighted a problem with the recorder, i.e. if a
		recording is shorter than a previous one, frames are left over.
		Plan is to fix this, write the analysis, then re-record.

	- Fixed overwrite issue, on recording start, delete the directory if it exists
	  and recreate.
	- Would be nice to automatically pull video off of the phone, but this seems
		more hassle than its worth

- Write offline video processing (tried Python, didn't work for god knows what
	reason)
	- Compute optical flow field
	- Compute FOE
	- Draw FOE and flow field
	- Upres to 900x100
	- Store resulting video

06 / 02 / 2019
Todo:
- Last-ditch effort to get libraries working
- Write buffered version instead


04 / 02 / 2019:
Wrap-up:
- First three chapters of dissertation complete! As always, could use a proof,
	but the legwork is done.
Todo:


Done:
- (Done) Finish dissertation redraft
	- Cell descriptions for the AntBot if required.
	- TN:
		- TN Input as described in Stone2017 does exist on the robot, however
		  it is never used (according to Android Studio). Instead we take the
			raw speeds computed from Optical Flow (See Scimeca for details)
			and pass them as input.

- (Done) Finish platform summary
	- Added a potential section on Video Recording, but this isn't pressing as it
		currently doesn't work.
	- Just about done; just need to add figures.
		- All figures from last year can likely be used. Double check that they're
			relevant.
		- Grab the images of the centre detection.
		- Maybe a picture of the directory structure? I don't think this is
			necessary.

03 / 01 / 2019:
Wrap up:
- Fixed most of the points raised by Barbara, only haven't mentioned the AntBot
	implementations of the AntBot CX.

- Haven't done anything further on the platform summary.
Todo:
- Fix points raised by Barbara in feedback
- Finish the platform summary; this should complete milestone 3

Notes:
- In 1.1 the phrase "synaptic plasticity" is used, which is underlined in red,
	and I'm not sure why.

- How FOE will work needs some more work from me.
	- Have a look for some papers that have done this. It seemed to be a fairly
		large body of work when researched.

- (Fixed) GA stuff moved to Appendix (I'm keeping it, because I've written it and
	an appendix is out of the way).

- (non-issue) 2.3.1 paragraph break highlighted on \textit{Haferlach et al.}
	present the model shown in figure... I don't know why this is highlighted.

- (non-issue Comment about CPU4L/R being identical seemed to cast doubt when
	Barbara mentioned it; I've left it unchanged but it would make sense for these
	to be the same in the absence of Holonomic motion.

- (Fixed) Pontine rate in Equation 14 is highlighted, but it was there in Stone
	et al. Need to ask about this.

	Changed a plus to a minus.

- Need to mention AntBot implementations for each neuronal type. I don't think
	this will be too bad. AAAAAAAAAAAAAAAAAAGGGGGGGHHHHHHHHHH! So, in looking into
	how the TN neurons work on AntBot, its clear that this will need a lot more
	time. May not be worth going into too much detail, but where to stop???
	In most cases it just looks like a function to get the output from the
	neuron using the rate model from Stone et al. which means they're mostly the
	same


Blocked:
- FOE Testing
Done:

01 / 01 / 2019:
Todo:
- Try to get the video recording software working, or at least figure out the
	issue
- Check out "The Senses: A comprehensive reference" in the library on optical
  flow sensitive cells (Horizontal and vertical); Jan suggested chapter 6 or 7,
	paper by Krapp. Ideally this will be relevant to the ant.



Notes:
- bytedeco/javacv1.4.4
- Can now confirm, reoving the bytedeco library has removed the duplicate entry.
	Fuck.
- Okay, there are no resources on how to use multiple versions of opencv in the
	same project.
- Going to see if I can rebuild the recorder using the utilities available in
	opencv 3.0.0
	- I tried to exclude the javacpp subpackage but no joy
	- javacv seems to wrap OpenCV, FFMPEG and more. So, there should be
	  equivalent opencv 3.0.0 functions; turning out now that javacv 1.4.4
		has recently been updated to use OpenCV 4.0.0
	- What do I actually need, and how can I get them?
		- org.bytedeco.javacpp.opencv_core : org.opencv.core.Core (I think?)
		- org.bytedeco.javacv.AndroidFrameConverter :
		- org.bytedeco.javacv.FFmpegFrameRecorder :
		- org.bytedeco.javacv.Frame :
		- org.bytedeco.javacpp.opencv_core.IplImage : (Supposedly IplImage is
																									 deprecated but the library
																									 version in javacv isn't
																									 clear; advice is to use
																									 Mat)
		- Another potential solution is to extract exactly the code for the
		  ffmpeg stuff from the library

- Attempted using the bytedeco javacpp library (does not include OpenCV)
  then tearing out the parts of the javaCV library that I needed for recordings.
	Compilation is successful, however there is a runtime error,
	ClassNotFoundException. THis is a massive issue as I have no idea how to fix it.

- Another potential is to include the javacv library and exclude the opencv parts.
  This seems to be doable via the compile command in the gradle file.

Blocked:
- FOE testing
Done:

31 / 01 / 2019:
Todo:
- Continue writing video recording utility for the AntBot

Notes:
- Video recording utility mostly finished (just need to add stop calls), however
	a major issue has arisen in that the project won't compile. Gradle sync and
	build seems to complete successfully, however, compiling for a device does not
	work.

- The problem appears to be duplicate code (i.e. two things with the same name
	that the compiler cannot resolve); why this is ocurring I have no idea.

- It appears to be a problem with conflicting android SDK versions being
	referenced from the Imgproc class, however, I think it is likely related to the
	dual OpenCV imports. OpenCV is included from the OpenCV300 library, but I
	needed functionality from the bytedeco/javacv version and I think there may be
	some overlap. Android Studio is none too helpful in fixing this.

Blocked:
- Test video recording

Done:
  ;_;

30 / 01 / 2019:
Todo:
- Further debugging on FOE computation

Notes:
- Problems with FOE:
	- FOE x and y are almost always small; even after multiplication by 10 and
	  modulo 90. What could cause this? Working theory is that the motion detected
		is actually very little; i.e. the sum computed will work out to a very
		small value. There is also the fact that we consider all motion vectors,
		so negative flow vectors will draw the FOE to the back, while positive will
		draw the FOE to the front at (hopefully) an equal rate. In theory, we should
		also see FOE x-values around 90 if this is the case, but we do not.

	- FOE is negative. As commented in last entry, this is likely to do with
		negative flow being received from the rear of the robot. Backward motion
		seems to give similar FOE values so I am hopeful.

	- Even when the computation is limited to the central 30 pixels, the FOE is
		always computed on the left-hand side. At this point, the idiot notices that,
		if the origin were centred in the image, then the FOE readings might make
		more sense

  - It's looking like I'm going to have to record some video from the robot, then
	  then use opencv on it to see if I can find a solution.


Blocked:
Done:
28 / 01 / 2019:
-
Time since last entry spent writing the interim report and catching up on
course material.
-

Todo:
- Test and debug the FOE computation; be mindful, this will likely not be finished
  today.

Blocked:
Notes:
- FOE calculation will now compute something, let's see what...
	- AntEye Stopped: CvException; Hooray, first invocation of Core.gemm
		- Error: (-215) a_size.width == len
		- Not a solution, but worth noting; Mat.size() gives a string in the form
		  COLSxROWS rather than the expected ROWSxCOLS
		- Okay, so gemm docs claim that the first matrix is automatically transposed,
		  which does not seem to be the case.
		- Don't reinitialise Mats inside a function; the Mat Header will only last
		  for the scope of the function.

- I'm now computing, well, something. Reminding me a tad of last year. The FOE
  is coming out as 0.0...; 0.0.. in most cases and at some points it is negative;
	I have no idea how the negative values work...
- Try using a subset of pixels to clear out some noise.
- Taking every 10th column provides a much more consistent measurement.
- Still behaviour isn't great.
- I'm dumb, you can account for negative flow readings because of the 360 camera;
	going to try reading only from the front facing pixels.

- On closing; FOE still very small (less than 1 without amplification factor).
  I really could do with figuring out what causes this. Given that the FOE is
	arrived at by summing flow vectors, it could be to do with the 360 camera.
	Token attempt made to work with only the frontal pixels but this isn't working
	any better so far as I can tell. Re-visit this idea and perhaps break down the
	calculation. Yes this is horrible but it could provide your solution.

Done:
- (Done) FOE is being computed, but I'm not convinced as to its accuracy. See
				 notes

22 / 01 / 2019:
Todo:
- Write the background for Stone et al's model
- Add a section header for Haferlach et al's model
	- This and a little physiological background should be given as part of the
	  background though I fear they may not make it in for the interim report.
	- I am significantly behind here (due to more of a break over christmas).
	  Ideally, by the end of this week I should have a complete background and
		platform summary but this is likely to extend to next week (this should
		really be a hard deadline). If I can have the first three chapters
		completed by the end of Week 3 I'll feel a bit better about shifting
		focus.
Notes:
- It would be excellent to have an example of the tb1 layer output
- What do the Pontine neurons actually do?
	- Identical activity to CPU4 neurons, but they provide a normalisation
	  effect by subtracive inhibition of neurons with opposite directional
		sensitivities. (Isn't this also a ring attractor then?)
- What is the difference between CPU1a and CPU1b?
	- Distinct connection patterns between the two types from the PB to
		the CBU. This is preserved across insect species.

- For TL/CL1 neurons have a look at Pfeiffer and Homberg; I'd assume its
	some kind of polarized light sensitivity cell followed by some kind of
	processing step.
21 / 01 / 2019:
Todo:
- Make a start on the CX background
Notes:
Blocked:
Done:
-(Done) Re-read CX paper


20 / 01 / 2019:
Todo:
- Restructure MB section in Dissertation

Notes:
- Light work day. The MB background section needs restructured
  as currently its pretty bad. Also need to look at the review
	from last year; I don't want to repeat too much between the sections

- Moved review to the end of the background as it worked better, allowing
  me to actually explain the concepts in the relevant sections.

Blocked:
Done:
- (Done) MB section restructured and background re-ordered. Happier now but
  its not great. I think Webb will be able to give more succinct feedback and
	I can move onto the CX>

18 / 01 / 2019:
Todo:
- Write an implementation for estimating the focus of expansion.
- Devise a method to move towards the focus of expansion
- Start trying to split the image into the eight regions.
	- This literally just needs to be an X value check on the FOE

Notes:
- The FOE estimation will go in the Util class; pass in the MainActivity object
  which contains the currentFrame and previousFrame (whatever they're actually
	called).
	- Method: Compute matrix A = [ u v ] where V = (u,v) is the flow vector
		associated with pixel P = (x,y). I'm going to need to figure this out again...
		Hooray!
	- Idea; instead of returning the FOE explicitly, you could simply return the
	  zone in which it resides. (image split into eight even zones -
		90/8 = 11.25). How do we split into 11.25px windows??? Likely approximate to
		11px intervals.

- The FOE implementation is written but no testing has been done. The testing
  will likely take a while. Do the old hope test to see if it magically works
	first time (it won't), but the matrix ops are going to be a pain. Really
	triple, quadruple check them at each stage. Key to testing is the matrix size.
	OpenCV types should be correct.

Blocked:
- Test FOE computation
- Devise a method to move towards the focus of expansion
- Start trying to split the image into the eight regions.

Done:
- (Done) FOE implementation written, UNTESTED

17 / 01 / 2019:
Todo:
- Run CX baseline tests (see CXE.txt in this directory)

Notes:
Blocked:
Done:
- (Done) Recordings for CX completed
- (Done) Recordings exported to csv
- (Done) Recordings added to the Repo

#
# Notes from 20/11/18 to 17/01/2019 were kept in a physical journal which is in
# IF G.17; on 17/01/2019 it was decided to revert to the text file format.
#

20 / 11 / 2018:
Notes:
- Code refactor finished
- Auto-calibration finished
- New sensor researched and added.
- Sequential thread... In progress.

The robot functions as it did last year when running the VN thread.
Optic flow mostly works and mushroom body works well, replicating the
learned route. OF implemented on the sequential thread doesn't work,
why? I think I'm dumb, I hope I'm dumb because it'll save a lot of work.
Okay, not dumb in the way I thought but in a different way.

I had originally left out the accumulator for the other side as I was
having issues with the turning (caused by the image problems). I also
had no accumulation resets. The loop counter wasn't ever incremented
and the accumulators weren't reset on each turn. Now seems to be working
but inconsistency still observed. Watch for this. It should be
remembered that the optical flow would occassionally bug out.

We'll immediately switch to using time + speed to measure distance.
Less time faffing. The robot moves one unit each second that it is mobile.
Getting this precise will be tricky, pay attention to where the timers
are started/stopped

On finishing:
Distance computation has been fixed. One unit per second, though for some
very short intervals no time can be recorded (this could be fixed by
allocating 10 units per second perhaps).

Optical flow playing up on final test and generally being a pain.
This may have been to do with the large objects I was using to try and induce
CA responses; I think it more likely that it just isn't working.

Todo:
- Test the older PI threads; make sure the existing CX actually works
  as described in Zhaoyu and Luca's papers (maybe alter the speeds).
- Figure out how to make the CXtheta value useful. Currently it is
  4 or 5 or zero. This is useless considering the robot doesn't turn to
	reset its orientation to be facing homeward. Make sure you're using
	this in the same way that Scimeca and Zhang did. CXtheta is meant
	to be in degrees so it should be directly passable to the turnAround
	function.
- Robot needs charged. 

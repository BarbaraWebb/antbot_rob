The List: Stuff I have to do but am putting off for time management's sake
- (IP) Platform summary
- Re-write ABS (Planned to slot into ILW).
- Build obstacles (Also planned for ILW but could do with being before)

Milestones:
- (Done!) Get old CX working (and recorded)
- (Done!) Finish interim report
- (IP) Finish first three chapters
- (IP) Complete FOE OFCA model
- Integrate into CXMB model (I think this is my best bet although there may
  be scope for modifying the CXMB model).
- (OPTIONAL; if you have time!) create an alternate 8MBON model (I think Zhaoyu's
	came from Barbara	and is likely best to stick with); could maybe try real
	weightings in the KC-MBON synapses?

	Barbara has mentioned that there were issues with Zhang, however, I think these
	were the methodological issues I have discussed already. I already have CX
	tests to cover this, however, CXMB + OFCA would be awesome.

01 / 01 / 2019:
Todo:
- Try to get the video recording software working, or at least figure out the
	issue
- Check out "The Senses: A comprehensive reference" in the library on optical
  flow sensitive cells (Horizontal and vertical); Jan suggested chapter 6 or 7,
	paper by Krapp. Ideally this will be relevant to the ant.



Notes:
- bytedeco/javacv1.4.4
- Can now confirm, reoving the bytedeco library has removed the duplicate entry.
	Fuck.
- Okay, there are no resources on how to use multiple versions of opencv in the
	same project.
- Going to see if I can rebuild the recorder using the utilities available in
	opencv 3.0.0
	- I tried to exclude the javacpp subpackage but no joy
	- javacv seems to wrap OpenCV, FFMPEG and more. So, there should be
	  equivalent opencv 3.0.0 functions; turning out now that javacv 1.4.4
		has recently been updated to use OpenCV 4.0.0
	- What do I actually need, and how can I get them?
		- org.bytedeco.javacpp.opencv_core : org.opencv.core.Core (I think?)
		- org.bytedeco.javacv.AndroidFrameConverter :
		- org.bytedeco.javacv.FFmpegFrameRecorder :
		- org.bytedeco.javacv.Frame :
		- org.bytedeco.javacpp.opencv_core.IplImage : (Supposedly IplImage is
																									 deprecated but the library
																									 version in javacv isn't
																									 clear; advice is to use
																									 Mat)
		- Another potential solution is to extract exactly the code for the
		  ffmpeg stuff from the library

- Attempted using the bytedeco javacpp library (does not include OpenCV)
  then tearing out the parts of the javaCV library that I needed for recordings.
	Compilation is successful, however there is a runtime error,
	ClassNotFoundException. THis is a massive issue as I have no idea how to fix it.

- Another potential is to include the javacv library and exclude the opencv parts.
  This seems to be doable via the compile command in the gradle file.

Blocked:
- FOE testing
Done:

31 / 01 / 2019:
Todo:
- Continue writing video recording utility for the AntBot

Notes:
- Video recording utility mostly finished (just need to add stop calls), however
	a major issue has arisen in that the project won't compile. Gradle sync and
	build seems to complete successfully, however, compiling for a device does not
	work.

- The problem appears to be duplicate code (i.e. two things with the same name
	that the compiler cannot resolve); why this is ocurring I have no idea.

- It appears to be a problem with conflicting android SDK versions being
	referenced from the Imgproc class, however, I think it is likely related to the
	dual OpenCV imports. OpenCV is included from the OpenCV300 library, but I
	needed functionality from the bytedeco/javacv version and I think there may be
	some overlap. Android Studio is none too helpful in fixing this.

Blocked:
- Test video recording

Done:
  ;_;

30 / 01 / 2019:
Todo:
- Further debugging on FOE computation

Notes:
- Problems with FOE:
	- FOE x and y are almost always small; even after multiplication by 10 and
	  modulo 90. What could cause this? Working theory is that the motion detected
		is actually very little; i.e. the sum computed will work out to a very
		small value. There is also the fact that we consider all motion vectors,
		so negative flow vectors will draw the FOE to the back, while positive will
		draw the FOE to the front at (hopefully) an equal rate. In theory, we should
		also see FOE x-values around 90 if this is the case, but we do not.

	- FOE is negative. As commented in last entry, this is likely to do with
		negative flow being received from the rear of the robot. Backward motion
		seems to give similar FOE values so I am hopeful.

	- Even when the computation is limited to the central 30 pixels, the FOE is
		always computed on the left-hand side. At this point, the idiot notices that,
		if the origin were centred in the image, then the FOE readings might make
		more sense

  - It's looking like I'm going to have to record some video from the robot, then
	  then use opencv on it to see if I can find a solution.


Blocked:
Done:
28 / 01 / 2019:
-
Time since last entry spent writing the interim report and catching up on
course material.
-

Todo:
- Test and debug the FOE computation; be mindful, this will likely not be finished
  today.

Blocked:
Notes:
- FOE calculation will now compute something, let's see what...
	- AntEye Stopped: CvException; Hooray, first invocation of Core.gemm
		- Error: (-215) a_size.width == len
		- Not a solution, but worth noting; Mat.size() gives a string in the form
		  COLSxROWS rather than the expected ROWSxCOLS
		- Okay, so gemm docs claim that the first matrix is automatically transposed,
		  which does not seem to be the case.
		- Don't reinitialise Mats inside a function; the Mat Header will only last
		  for the scope of the function.

- I'm now computing, well, something. Reminding me a tad of last year. The FOE
  is coming out as 0.0...; 0.0.. in most cases and at some points it is negative;
	I have no idea how the negative values work...
- Try using a subset of pixels to clear out some noise.
- Taking every 10th column provides a much more consistent measurement.
- Still behaviour isn't great.
- I'm dumb, you can account for negative flow readings because of the 360 camera;
	going to try reading only from the front facing pixels.

- On closing; FOE still very small (less than 1 without amplification factor).
  I really could do with figuring out what causes this. Given that the FOE is
	arrived at by summing flow vectors, it could be to do with the 360 camera.
	Token attempt made to work with only the frontal pixels but this isn't working
	any better so far as I can tell. Re-visit this idea and perhaps break down the
	calculation. Yes this is horrible but it could provide your solution.

Done:
- (Done) FOE is being computed, but I'm not convinced as to its accuracy. See
				 notes

22 / 01 / 2019:
Todo:
- Write the background for Stone et al's model
- Add a section header for Haferlach et al's model
	- This and a little physiological background should be given as part of the
	  background though I fear they may not make it in for the interim report.
	- I am significantly behind here (due to more of a break over christmas).
	  Ideally, by the end of this week I should have a complete background and
		platform summary but this is likely to extend to next week (this should
		really be a hard deadline). If I can have the first three chapters
		completed by the end of Week 3 I'll feel a bit better about shifting
		focus.
Notes:
- It would be excellent to have an example of the tb1 layer output
- What do the Pontine neurons actually do?
	- Identical activity to CPU4 neurons, but they provide a normalisation
	  effect by subtracive inhibition of neurons with opposite directional
		sensitivities. (Isn't this also a ring attractor then?)
- What is the difference between CPU1a and CPU1b?
	- Distinct connection patterns between the two types from the PB to
		the CBU. This is preserved across insect species.

- For TL/CL1 neurons have a look at Pfeiffer and Homberg; I'd assume its
	some kind of polarized light sensitivity cell followed by some kind of
	processing step.
21 / 01 / 2019:
Todo:
- Make a start on the CX background
Notes:
Blocked:
Done:
-(Done) Re-read CX paper


20 / 01 / 2019:
Todo:
- Restructure MB section in Dissertation

Notes:
- Light work day. The MB background section needs restructured
  as currently its pretty bad. Also need to look at the review
	from last year; I don't want to repeat too much between the sections

- Moved review to the end of the background as it worked better, allowing
  me to actually explain the concepts in the relevant sections.

Blocked:
Done:
- (Done) MB section restructured and background re-ordered. Happier now but
  its not great. I think Webb will be able to give more succinct feedback and
	I can move onto the CX>

18 / 01 / 2019:
Todo:
- Write an implementation for estimating the focus of expansion.
- Devise a method to move towards the focus of expansion
- Start trying to split the image into the eight regions.
	- This literally just needs to be an X value check on the FOE

Notes:
- The FOE estimation will go in the Util class; pass in the MainActivity object
  which contains the currentFrame and previousFrame (whatever they're actually
	called).
	- Method: Compute matrix A = [ u v ] where V = (u,v) is the flow vector
		associated with pixel P = (x,y). I'm going to need to figure this out again...
		Hooray!
	- Idea; instead of returning the FOE explicitly, you could simply return the
	  zone in which it resides. (image split into eight even zones -
		90/8 = 11.25). How do we split into 11.25px windows??? Likely approximate to
		11px intervals.

- The FOE implementation is written but no testing has been done. The testing
  will likely take a while. Do the old hope test to see if it magically works
	first time (it won't), but the matrix ops are going to be a pain. Really
	triple, quadruple check them at each stage. Key to testing is the matrix size.
	OpenCV types should be correct.

Blocked:
- Test FOE computation
- Devise a method to move towards the focus of expansion
- Start trying to split the image into the eight regions.

Done:
- (Done) FOE implementation written, UNTESTED

17 / 01 / 2019:
Todo:
- Run CX baseline tests (see CXE.txt in this directory)

Notes:
Blocked:
Done:
- (Done) Recordings for CX completed
- (Done) Recordings exported to csv
- (Done) Recordings added to the Repo

#
# Notes from 20/11/18 to 17/01/2019 were kept in a physical journal which is in
# IF G.17; on 17/01/2019 it was decided to revert to the text file format.
#

20 / 11 / 2018:
Notes:
- Code refactor finished
- Auto-calibration finished
- New sensor researched and added.
- Sequential thread... In progress.

The robot functions as it did last year when running the VN thread.
Optic flow mostly works and mushroom body works well, replicating the
learned route. OF implemented on the sequential thread doesn't work,
why? I think I'm dumb, I hope I'm dumb because it'll save a lot of work.
Okay, not dumb in the way I thought but in a different way.

I had originally left out the accumulator for the other side as I was
having issues with the turning (caused by the image problems). I also
had no accumulation resets. The loop counter wasn't ever incremented
and the accumulators weren't reset on each turn. Now seems to be working
but inconsistency still observed. Watch for this. It should be
remembered that the optical flow would occassionally bug out.

We'll immediately switch to using time + speed to measure distance.
Less time faffing. The robot moves one unit each second that it is mobile.
Getting this precise will be tricky, pay attention to where the timers
are started/stopped

On finishing:
Distance computation has been fixed. One unit per second, though for some
very short intervals no time can be recorded (this could be fixed by
allocating 10 units per second perhaps).

Optical flow playing up on final test and generally being a pain.
This may have been to do with the large objects I was using to try and induce
CA responses; I think it more likely that it just isn't working.

Todo:
- Test the older PI threads; make sure the existing CX actually works
  as described in Zhaoyu and Luca's papers (maybe alter the speeds).
- Figure out how to make the CXtheta value useful. Currently it is
  4 or 5 or zero. This is useless considering the robot doesn't turn to
	reset its orientation to be facing homeward. Make sure you're using
	this in the same way that Scimeca and Zhang did. CXtheta is meant
	to be in degrees so it should be directly passable to the turnAround
	function.
- Robot needs charged. 

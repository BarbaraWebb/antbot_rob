The List: Stuff I have to do but am putting off for time management's sake
- Re-write ABS (Planned to slot into ILW).
- Build obstacles (Also planned for ILW but could do with being before)
- Check the Equations in the dissertation background. Noticed a few wrong in
  the CX section.

Milestones:
- (Done!) Get old CX working (and recorded)
- (Done!) Finish interim report
- (Done!) Finish first three chapters
- (IP) Complete FOE OFCA model
- Integrate into CXMB model (I think this is my best bet although there may
  be scope for modifying the CXMB model).
- (OPTIONAL; if you have time!) create an alternate 8MBON model (I think Zhaoyu's
	came from Barbara	and is likely best to stick with); could maybe try real
	weightings in the KC-MBON synapses?

	Barbara has mentioned that there were issues with Zhang, however, I think these
	were the methodological issues I have discussed already. I already have CX
	tests to cover this, however, CXMB + OFCA would be awesome.

23 / 03 / 19:
Todo:
- Dissertation

Notes:
- Need a bit of background to describe the neural representations I am using
	Short and sweet, introduce only the basic concepts.

22 / 03 / 2019:
Wrap-up:
- Rate model did not work as well as the I&F model (which also didn't work...)
  I suspect the problem is now down to the optical flow computation, noise needs
	filtered out, but I no longer have time to spend on it. Write methods, and then
	experiment with parameters offline. Noise may be reducable by considering a
	subset dense field, but I would rather use a different filter.
Todo:
- Try a rate model for the accumulators.

Notes:
- How do I write this as a rate model??? Firing rate is somehow proportional to
  input. So, the output would just be a sigmoid of the input?

	The differences between the accumulation are tiny. If we put them through a
	sigmoid the differences are still tiny, but if you only take the one with the
	maximum rate then you're generating a turn regardless. What happens if you just
	take the rates and feed them into the neurons to the left and right of the
	current direction? The tiny differences may be enough?

	Each accumulator has to inhibit the other. The larger the rate, the larger the
	inhibition
		left = sigmoid(leftsum)
		right = sigmoid(rightsum)
		nleft = left - right;
		nright = right - left;
	
	
	

Done:
- (Done) Get better recordings of the AntBot's forward motion, look at flow pattern
	- Recordings good, flow adequately shows what I'm trying to show.
	- Videos moved to drive for safekeeping
- (Done) See if periodically clearing the CA layer does anything
	This wouldn't have an effect as the CA layer is completely redone each
	iteration.
17 / 03 / 2019:
Todo:
- Quick check, dot ca by weightings from CPU4 to CPU1, see behaviour (if any);
- Assuming this doesn't work immediately, begin investigating why the raw values
	don't work to generate an angle. How could I generate an arbitrary turning
	angle as output (this is probably your key question for the day).

Notes:
- CACX didn't work as I thought it was in last entry. I was producing an
	incorrect mapping.
- Dotting by CPU4-CPU1 weightings gives output. The strength of this output
	seems variable. Worth a run to check.
	xs
- Response in a clear environment looked promising. Putting in a full arena shows
  that it doesn't work cleanly but symptoms indicate a hypersensitive CA component.
	- Desensitized the CA component and the robot behaves as if there is no CA. This
		is a good sign.
	- Offsets are definitely showing the correct direction. I still end up with
		weird turns. I'd expect one direction, but it goes another. However,
		sensitivity looks about right.
	- Also worth noting that outputs are not consistent when I feel they should be;
		also also worth noting that they are a hell of a lot more consisent than they
		were before. Maybe offsets of 1 as opposed to 2?
	- Output signals are still inconsistent. I think that the only point of change
		is the inputs matrix. It is created based on the combiner minus the tb1
		signal.
		- Tried to remove, no reaction is generated whatsoever. Makes sense as I
			think this is the comparison with the current direction. So this potential
			source of inconsistency is going to have to stay in. Worth trying a couple
			of behavioural tests.
	- Generate consistent turns in one direction. Right turns reasonably consistent.
		So are left. So, I can generate reasonably consistent turns by shifting the
		offset. They vary but they're large enough to be CA saccades. They could be
		capped at 30 perhaps.
	- Now worth including ecx.collisionDetected in the check. Now guaranteed that
		all turns in outbound are from CA.
	- Just got one of those sadly promising runs.

	- Okay, it seems to be the collision avoidance system only that's breaking.
		Turns are generated in the wrong direction, and it isn't the inconsistency
	

14 / 03 / 2019:
Todo:
- Tune CACX system. Collision avoidance in the CX seems to work!
  But, it needs tuned.
	- Reaction saccade
	- Reaction sensitivity
	- Balance with CX

Notes;
- Trying to remove the whole shift index thing from the process of building the
  sinusoid; maybe this plus the dot product with the current heading are causing
	issues but I wouldn't have thought so.

- Turns seem erratic; though the erratacism is seemingly consistent.
- The TB1 headings are relative to the direction in which the robot started, so
	the mod operation could be the issue.
- I knew it couldn't be that easy. The reaction isn't due to sensitivity, it's
	due to odd CX outputs. The weighted CA is outputting that we should change
	course even if no collision has been detected by the accumulators.  Why is
	this?

- As per, stuff from last year that I thought was wrong is actually right...
  Zhaoyu's expansion matrix was correct.



13 / 03 / 2019:
Todo:
- Further debugging

Notes:
- Performance improved by combining with memory circuit, but this doesn't make
  sense.
	- Okay, plan: re-insert the flag-based method. The model will signal if a
		collision has been detected. Then we get output from the model.
	- Now, the model is producing collision avoidance output, however, the
	  response is too small and the detection is too sensitive.
	- Wondering if there's something wrong with the extension matrix
	  I'm now ending up with just a really long sin curve which isn't ideal.
	- Got some weirdly promising results back from an initial test.

- CA seems to work surprisingly. About as sensitive as it was before; but seems
  close could maybe do with slightly higher sensitivity. Leak factor instead of
	hard reset may be causing issues; but all in all it seems okay.
- Reaction still isn't really big enough and I've run out of space.
  If I were to add another cell's worth of offset I'd be telling the robot just
	to go backwards.
- Also I'd like to try just injecting the CA output : DOESN'T WORK
	- I don't get this. It has to be combined with the weighted CPU4, however, the
	  numbers are almost identical. When combined with weighted CPU4 it seems to
		work. I don't want to know if this is just luck haha
- Index out of bound for SimpleMatrix caused by remainder instead of mod but I
  don't know why it only happens on inbound

- I think CA behaviour so far has been mostly luck. I think the matrix that
	extends to 16 neurons is incorrect. We therefore end up with replication
	and the robot is pulled equally in both directions rather than a significant
	direction.
11 / 03 / 2019:
Todo:
- Check accumulation; make sure reaction is being triggered.
- Check reaction; make sure shape is correct
Notes:
- First thing, accumulation leak now ensures that a reaction will be
  triggered eventually as data builds up.
- Can now trigger turns coursly, however, the generated sinusoid is completely
  wrong. I have no idea how to fix this... (I'm dumb, yet again)
- Is there any way I can make the original method more accurate???
  The response is pretty much guaranteed to be course.
- The sinusoid response is being correctly computed and shifted. Stop trying to
  write "efficient" code.
- Next step is the incorporation into the output. But that's a story for another
  day.

Done:
- Made neural CA model (last commit)
07 / 02 / 2019:
Todo:
- Check CXCA
Notes:
- CXCA works, kinda... The CA is too strong on the inbound run. Every turn is
	triggered by CA, not CX.
	- One avenue is to damp the CA response, it is still hypersensitive.
	- Another, or maybe combined avenue would be to have the CA add to the
	  CX response rather than just overwriting it? How? Adding them
		may help, but the problem remains. The idea is clearly, boost it in the
		direction of avoidance. The other problem is that my reactions are
		binary, not in any way scalable; I don't think there is a way to solve this
		for this year.
	- I think dampening + adding proposed CX reaction to the CA reaction

- Translating into an eight neuron thing. I can use the same W_TB1_CPU1 weight
  matrix to describe the connections between the CA layer (TB1) and CPU1.
	I have my left and right accumulation. So I have a 1/0 output for each neuron.

  So, run the collision avoidance accumulations. Only when a threshold is reached
	Set the neurons to have 1 at the desired turning heading, then set the others
	to the sinusoid. Two neurons to the left or right?
	

		
Done:
- (Done) Collision avoidance works but needs tuned


28 / 02 / 2019:
Wrap up:
- Hacky way of doing CXE is implemented. Collision avoidance sets a flag which is
	then checked by the main thread. Accumulators and reaction are computed in the
	CXE model. Accumulation isn't happening and I'm not sure why. Log statements
	in place in accumulateFlow. If i can solve this, I'll have CXCA.

Todo:
- Test isHome mechanism on standard CX

Blocked:
Notes:
- All steering is being governed by CPU1. So, add the flowsums as input to the
	cpu1Output function. Know this function is called once per loop iteration
	so we can 

- Desired usage of CXE
	I'd like to run a thread that just uses the CX framework with a couple of extra
	inputs. The most difficult bit isn't the obstacle avoidance. It's the
	localisation. The isHome mechanism does not work - and I'm not sure it can work.
	Deciding whether or not we're home is stupidly precise. I'm not sure the
	measured odometry is actually that precise. Maybe an interesting test would be
	to let the robot loose in the conference room to see if it ever does stop; or
	if it tries to change its homing behaviour at all. Do this; it may provide the
	yes/no to if I can use it at all.

	Assuming the worst; how can I test my ideas without localisation?
	I can get a single outbound/inbound run with CACX. That in itself	is useful. To
	incorporate the mushroom body, I need to be able to navigate back out again,
	which means	need to know when the robot is home. How?

	- Hit a button on the robot? : Would cause interferance with saved images
	  potentially and requires a lot of intervention.
		
	- Geomagnetic field sensor? I have no idea how accurate or feasible this is.
	  This sensor should just be used to say home or food; it will not be used to
		actually follow the vector. So define a home point then define a circle
		with some radius around this point and class that as home.

	- So what more am I doing tonight? Test geomagnetic idea; write CA into CXE
	  (just set MB weighting to 0 for just now). Test; then go home.
		- If the geomagnetic sensor on the phone doesn't work, don't sweat it.
		  make the CXE so that the capability is there even if the technology
			isn't
	
	
	

- Code formatting as I go
- Clear that I need to think about the experiment scenario first.
	- Robot starts from "home", navigates outward for some time using obstacle
		avoidance. Store image with direction throughout.
	- Navigates home using CX+CA. Store image with direction throughout.
	- Need a way of determining home and food locations.
		- A network state makes most sense (copied memory). Then we can check atFood,
			or atNest or whatever. Can we use the network state to compute a new goal?
			I.e. could I tell the network to zero on the food location?

			Current Util.isHome function checks a SimpleMatrix representing memory to
			see if enough neurons are sufficiently close to zero in value.

			Works for isHome, but being at the nest presents more challenges; we could
			just invert the instruction? Always navigate on the anti-home vector. That
			should work! Then isFood or equivalent will check that the memory argument
			is sufficiently close to the stored state. Plan.

			Only problem is that the current isHome function does not work. It seems
			designed to work with the memory structure rather than cpu4; though I'm not
			sure why this is.
			
			

Done:
- (Done) Create a copy of the CXMB model
- (Done) Recall the CXMB model


26 / 02 / 2019:
Todo:
- Fix framerate issues when comparing filters
	- Framerate issues resolved and flow filtering checked. Seems to be working
	  reasonably though additional tuning may be required.
		
- Build integration

Notes:
	Integration:
	- How to do this...
	- An idea may be to have the CX constantly be in control of movement.
	- In previous iterations, the CX does not control movement on the outbound
	  run. In my case, I could include this with some input, unless the memory
		layer would start screwing with things??? In my model I could simply say
		that memory gets a weighting of zero on an outbound journey.
	- The weighting is then modified to give control to memory only if collision
	  avoidance has nothing to give. I.e. we navigate only when we are not in
		immediate danger; same could be done including the MB.

  - So first attempt; a CX model with some input from CA sums; or even the
	  flow difference.
				 - Could inject either: The sums, the difference, or the fact we need to
				   turn follwed by a direction (a binary input; 1 turn right, 0 turn left)
					 - If I do the sums, the accumulation and decision making is all
					   contained in the model
					 - If I do the difference, the decision making is in the model
					 - If I do the binary input, the model is just told to make a turn
					 - The first option easily makes the most sense.
---
- Framerate tanks as soon as filtering is included. Framerate approximately
	halves

- Sticking consistently around 15fps
- Image rotation snippit drops about 5 frames. ~10fps
- Downsampling drops a couple of frames too. ~11fps
- Gaussian blur and histogram equalization drops a couple of frames. ~11
- Copying doesn't affect it too badly; maybe 1 fps dropped.
- With all copying in place, approx. 13/14fps

- Bottlenecks found: Goal baseline, approx 13fps.
	- Rotation; slightly improved performance by using the opencv copyTo method
		(approx 13fps)
	- Downsampling
	- Gaussian blur and histogram equalization (may be better once the image is
		smaller).
		

18 / 02 / 2019:
Wrap-up:
- Framerate issue: Found the sources after 5hrs
	- Tried disabling all flow related things. Framerate rose to ~10fps though with
	  wide variation.
	- Re-wrote the visual processing to run in a separate thread so priority could
		be set and processing could run asynchronously. This took approx 5hrs to get
		everything working (and even then it was buggy).
		- The thread was set to highest priority; the framerate was not affected in
			any noticable way. I want to say it was negligably higher, but I'm just
			desperate I think.
	- Reverted to my backup commit and added flow functionality back bit-by-bit.
	- Framerate crashes when trying to work with the matched filters so I may
		need to look at a way of optimising these bits specifically.
	- It may also be worth adding a timer to check how long the frame actually takes
		to process.
	- Only modification to retain from today's adventure is to move the menu stuff
	  into its own function.
	- I find it hard to believe we can't get better than 10fps.
		
	
Todo:
- Check framerate when recording isn't running.
	- Roughly the same, maybe negligably higher.
	- On average, getting 2/3 frames per second with occasional spikes and dips
	- Speaking to Jan, he suggested looking at the framerate without the farneback
		computation; also, just checking out the framerate through the stages of
		image processing.
	- Raising the framerate may help with the optical flow.
	- All of the delays sprinkled through the code are also abnormal. Why do we
	  have to put sleep statements in each while loop? Unless this is a really
		terrible way of providing synchronisation.
	- When dealing with all of this, we have to acknowledge that matched filters
	  worked last year.
	- So, an avenue for exploration; relegate the image preprocessing to its own
		thread, then set this to be a max priority thread.
		- Same frame buffering method as VideoRecorder???
		- Probably not, we always need to know what the current frame is, but
			processing likely wont be complete by the time the next frame is passed
			to the application.
		- Could just treat the newest frame off the pile as the current one?
		- The same frames still need to be made global so they are visible to
			the rest of the system. Safety push, then get started...

	  - I'm using the same frame buffer system after all; I'd like to be able to
		  put the thread to sleep, but I don't think it'll be an issue.

		- Really want images to be available as much as possible. So, do we only
		  update the current image after all processing is complete? Supposedly
			slow, but I'm not so sure.

    - Basically; start moving code across and see what happens; don't get too
		  bogged down with details, but keep notes.

		- I'm genuinely furious. The amount of unused, stupid code here is ridiculous.
		- Aaaand, now I'm lost. I have no idea where I am in the process...

		- I may have stumbled on a much easier way to extract the blue channel but I
		  am loath to mess with it.


		Okay, so, having moved all of the processing code into another file; it works.
		However, the display issue is a problem. I could do with having a display on
		the robot. Getting this image back to MainActivity is the problem.

- Play about with new sensors to see if you can get anything to work
- Look at TinkerCAD to model a piece for the front of the robot which
	can be used to mount the sensors.
- Seriously; have a think as to whether this is the best course of action.
	This is pretty extreme and it could easily have its own problems.

Notes:
- Thanks to the magic of emacs, I have this day's todo across the branches.


13 / 02 / 2019:
Notes:
- Plotting the flow field seems to show that the flow detected is miniscule and
	erratic.
- I've emailed Garry to look for alternatives as I don't think anything flow
	based	will do particularly well
- That said, I'm not sure the recordings are the best. Even as a human its hard
	to tell whats going on until the last minute.
- Comfortingly, Luca's code seems to give a similar reading for the flow
	magnitude.
- The AntBot gets about 2 frames per second. Seriously?


12 / 02 / 2019:
Wrap-up:
- Video recording utilities work; can plot the FOE on the video frame
- As expected there isn't really a reliable relationship between a looming
 	obstacle and the FOE position, although in the recording it does jump
	right
- I need to redo the recordings anyway, so I'll check again, but as a preliminary
  this is good, and I can start to look at alternatives.
07 / 02 / 2019:
Todo:
- Compile recordings into video
	(Should be doable with an ffmpeg command)
	(actually worked it into a python script so its reusable)
	- Works nicely, but highlighted a problem with the recorder, i.e. if a
		recording is shorter than a previous one, frames are left over.
		Plan is to fix this, write the analysis, then re-record.

	- Fixed overwrite issue, on recording start, delete the directory if it exists
	  and recreate.
	- Would be nice to automatically pull video off of the phone, but this seems
		more hassle than its worth

- Write offline video processing (tried Python, didn't work for god knows what
	reason)
	- Compute optical flow field
	- Compute FOE
	- Draw FOE and flow field
	- Upres to 900x100
	- Store resulting video

06 / 02 / 2019
Todo:
- Last-ditch effort to get libraries working
- Write buffered version instead


04 / 02 / 2019:
Wrap-up:
- First three chapters of dissertation complete! As always, could use a proof,
	but the legwork is done.
Todo:


Done:
- (Done) Finish dissertation redraft
	- Cell descriptions for the AntBot if required.
	- TN:
		- TN Input as described in Stone2017 does exist on the robot, however
		  it is never used (according to Android Studio). Instead we take the
			raw speeds computed from Optical Flow (See Scimeca for details)
			and pass them as input.

- (Done) Finish platform summary
	- Added a potential section on Video Recording, but this isn't pressing as it
		currently doesn't work.
	- Just about done; just need to add figures.
		- All figures from last year can likely be used. Double check that they're
			relevant.
		- Grab the images of the centre detection.
		- Maybe a picture of the directory structure? I don't think this is
			necessary.

03 / 01 / 2019:
Wrap up:
- Fixed most of the points raised by Barbara, only haven't mentioned the AntBot
	implementations of the AntBot CX.

- Haven't done anything further on the platform summary.
Todo:
- Fix points raised by Barbara in feedback
- Finish the platform summary; this should complete milestone 3

Notes:
- In 1.1 the phrase "synaptic plasticity" is used, which is underlined in red,
	and I'm not sure why.

- How FOE will work needs some more work from me.
	- Have a look for some papers that have done this. It seemed to be a fairly
		large body of work when researched.

- (Fixed) GA stuff moved to Appendix (I'm keeping it, because I've written it and
	an appendix is out of the way).

- (non-issue) 2.3.1 paragraph break highlighted on \textit{Haferlach et al.}
	present the model shown in figure... I don't know why this is highlighted.

- (non-issue Comment about CPU4L/R being identical seemed to cast doubt when
	Barbara mentioned it; I've left it unchanged but it would make sense for these
	to be the same in the absence of Holonomic motion.

- (Fixed) Pontine rate in Equation 14 is highlighted, but it was there in Stone
	et al. Need to ask about this.

	Changed a plus to a minus.

- Need to mention AntBot implementations for each neuronal type. I don't think
	this will be too bad. AAAAAAAAAAAAAAAAAAGGGGGGGHHHHHHHHHH! So, in looking into
	how the TN neurons work on AntBot, its clear that this will need a lot more
	time. May not be worth going into too much detail, but where to stop???
	In most cases it just looks like a function to get the output from the
	neuron using the rate model from Stone et al. which means they're mostly the
	same


Blocked:
- FOE Testing
Done:

01 / 01 / 2019:
Todo:
- Try to get the video recording software working, or at least figure out the
	issue
- Check out "The Senses: A comprehensive reference" in the library on optical
  flow sensitive cells (Horizontal and vertical); Jan suggested chapter 6 or 7,
	paper by Krapp. Ideally this will be relevant to the ant.



Notes:
- bytedeco/javacv1.4.4
- Can now confirm, reoving the bytedeco library has removed the duplicate entry.
	Fuck.
- Okay, there are no resources on how to use multiple versions of opencv in the
	same project.
- Going to see if I can rebuild the recorder using the utilities available in
	opencv 3.0.0
	- I tried to exclude the javacpp subpackage but no joy
	- javacv seems to wrap OpenCV, FFMPEG and more. So, there should be
	  equivalent opencv 3.0.0 functions; turning out now that javacv 1.4.4
		has recently been updated to use OpenCV 4.0.0
	- What do I actually need, and how can I get them?
		- org.bytedeco.javacpp.opencv_core : org.opencv.core.Core (I think?)
		- org.bytedeco.javacv.AndroidFrameConverter :
		- org.bytedeco.javacv.FFmpegFrameRecorder :
		- org.bytedeco.javacv.Frame :
		- org.bytedeco.javacpp.opencv_core.IplImage : (Supposedly IplImage is
																									 deprecated but the library
																									 version in javacv isn't
																									 clear; advice is to use
																									 Mat)
		- Another potential solution is to extract exactly the code for the
		  ffmpeg stuff from the library

- Attempted using the bytedeco javacpp library (does not include OpenCV)
  then tearing out the parts of the javaCV library that I needed for recordings.
	Compilation is successful, however there is a runtime error,
	ClassNotFoundException. THis is a massive issue as I have no idea how to fix it.

- Another potential is to include the javacv library and exclude the opencv parts.
  This seems to be doable via the compile command in the gradle file.

Blocked:
- FOE testing
Done:

31 / 01 / 2019:
Todo:
- Continue writing video recording utility for the AntBot

Notes:
- Video recording utility mostly finished (just need to add stop calls), however
	a major issue has arisen in that the project won't compile. Gradle sync and
	build seems to complete successfully, however, compiling for a device does not
	work.

- The problem appears to be duplicate code (i.e. two things with the same name
	that the compiler cannot resolve); why this is ocurring I have no idea.

- It appears to be a problem with conflicting android SDK versions being
	referenced from the Imgproc class, however, I think it is likely related to the
	dual OpenCV imports. OpenCV is included from the OpenCV300 library, but I
	needed functionality from the bytedeco/javacv version and I think there may be
	some overlap. Android Studio is none too helpful in fixing this.

Blocked:
- Test video recording

Done:
  ;_;

30 / 01 / 2019:
Todo:
- Further debugging on FOE computation

Notes:
- Problems with FOE:
	- FOE x and y are almost always small; even after multiplication by 10 and
	  modulo 90. What could cause this? Working theory is that the motion detected
		is actually very little; i.e. the sum computed will work out to a very
		small value. There is also the fact that we consider all motion vectors,
		so negative flow vectors will draw the FOE to the back, while positive will
		draw the FOE to the front at (hopefully) an equal rate. In theory, we should
		also see FOE x-values around 90 if this is the case, but we do not.

	- FOE is negative. As commented in last entry, this is likely to do with
		negative flow being received from the rear of the robot. Backward motion
		seems to give similar FOE values so I am hopeful.

	- Even when the computation is limited to the central 30 pixels, the FOE is
		always computed on the left-hand side. At this point, the idiot notices that,
		if the origin were centred in the image, then the FOE readings might make
		more sense

  - It's looking like I'm going to have to record some video from the robot, then
	  then use opencv on it to see if I can find a solution.


Blocked:
Done:
28 / 01 / 2019:
-
Time since last entry spent writing the interim report and catching up on
course material.
-

Todo:
- Test and debug the FOE computation; be mindful, this will likely not be finished
  today.

Blocked:
Notes:
- FOE calculation will now compute something, let's see what...
	- AntEye Stopped: CvException; Hooray, first invocation of Core.gemm
		- Error: (-215) a_size.width == len
		- Not a solution, but worth noting; Mat.size() gives a string in the form
		  COLSxROWS rather than the expected ROWSxCOLS
		- Okay, so gemm docs claim that the first matrix is automatically transposed,
		  which does not seem to be the case.
		- Don't reinitialise Mats inside a function; the Mat Header will only last
		  for the scope of the function.

- I'm now computing, well, something. Reminding me a tad of last year. The FOE
  is coming out as 0.0...; 0.0.. in most cases and at some points it is negative;
	I have no idea how the negative values work...
- Try using a subset of pixels to clear out some noise.
- Taking every 10th column provides a much more consistent measurement.
- Still behaviour isn't great.
- I'm dumb, you can account for negative flow readings because of the 360 camera;
	going to try reading only from the front facing pixels.

- On closing; FOE still very small (less than 1 without amplification factor).
  I really could do with figuring out what causes this. Given that the FOE is
	arrived at by summing flow vectors, it could be to do with the 360 camera.
	Token attempt made to work with only the frontal pixels but this isn't working
	any better so far as I can tell. Re-visit this idea and perhaps break down the
	calculation. Yes this is horrible but it could provide your solution.

Done:
- (Done) FOE is being computed, but I'm not convinced as to its accuracy. See
				 notes

22 / 01 / 2019:
Todo:
- Write the background for Stone et al's model
- Add a section header for Haferlach et al's model
	- This and a little physiological background should be given as part of the
	  background though I fear they may not make it in for the interim report.
	- I am significantly behind here (due to more of a break over christmas).
	  Ideally, by the end of this week I should have a complete background and
		platform summary but this is likely to extend to next week (this should
		really be a hard deadline). If I can have the first three chapters
		completed by the end of Week 3 I'll feel a bit better about shifting
		focus.
Notes:
- It would be excellent to have an example of the tb1 layer output
- What do the Pontine neurons actually do?
	- Identical activity to CPU4 neurons, but they provide a normalisation
	  effect by subtracive inhibition of neurons with opposite directional
		sensitivities. (Isn't this also a ring attractor then?)
- What is the difference between CPU1a and CPU1b?
	- Distinct connection patterns between the two types from the PB to
		the CBU. This is preserved across insect species.

- For TL/CL1 neurons have a look at Pfeiffer and Homberg; I'd assume its
	some kind of polarized light sensitivity cell followed by some kind of
	processing step.
21 / 01 / 2019:
Todo:
- Make a start on the CX background
Notes:
Blocked:
Done:
-(Done) Re-read CX paper


20 / 01 / 2019:
Todo:
- Restructure MB section in Dissertation

Notes:
- Light work day. The MB background section needs restructured
  as currently its pretty bad. Also need to look at the review
	from last year; I don't want to repeat too much between the sections

- Moved review to the end of the background as it worked better, allowing
  me to actually explain the concepts in the relevant sections.

Blocked:
Done:
- (Done) MB section restructured and background re-ordered. Happier now but
  its not great. I think Webb will be able to give more succinct feedback and
	I can move onto the CX>

18 / 01 / 2019:
Todo:
- Write an implementation for estimating the focus of expansion.
- Devise a method to move towards the focus of expansion
- Start trying to split the image into the eight regions.
	- This literally just needs to be an X value check on the FOE

Notes:
- The FOE estimation will go in the Util class; pass in the MainActivity object
  which contains the currentFrame and previousFrame (whatever they're actually
	called).
	- Method: Compute matrix A = [ u v ] where V = (u,v) is the flow vector
		associated with pixel P = (x,y). I'm going to need to figure this out again...
		Hooray!
	- Idea; instead of returning the FOE explicitly, you could simply return the
	  zone in which it resides. (image split into eight even zones -
		90/8 = 11.25). How do we split into 11.25px windows??? Likely approximate to
		11px intervals.

- The FOE implementation is written but no testing has been done. The testing
  will likely take a while. Do the old hope test to see if it magically works
	first time (it won't), but the matrix ops are going to be a pain. Really
	triple, quadruple check them at each stage. Key to testing is the matrix size.
	OpenCV types should be correct.

Blocked:
- Test FOE computation
- Devise a method to move towards the focus of expansion
- Start trying to split the image into the eight regions.

Done:
- (Done) FOE implementation written, UNTESTED

17 / 01 / 2019:
Todo:
- Run CX baseline tests (see CXE.txt in this directory)

Notes:
Blocked:
Done:
- (Done) Recordings for CX completed
- (Done) Recordings exported to csv
- (Done) Recordings added to the Repo

#
# Notes from 20/11/18 to 17/01/2019 were kept in a physical journal which is in
# IF G.17; on 17/01/2019 it was decided to revert to the text file format.
#

20 / 11 / 2018:
Notes:
- Code refactor finished
- Auto-calibration finished
- New sensor researched and added.
- Sequential thread... In progress.

The robot functions as it did last year when running the VN thread.
Optic flow mostly works and mushroom body works well, replicating the
learned route. OF implemented on the sequential thread doesn't work,
why? I think I'm dumb, I hope I'm dumb because it'll save a lot of work.
Okay, not dumb in the way I thought but in a different way.

I had originally left out the accumulator for the other side as I was
having issues with the turning (caused by the image problems). I also
had no accumulation resets. The loop counter wasn't ever incremented
and the accumulators weren't reset on each turn. Now seems to be working
but inconsistency still observed. Watch for this. It should be
remembered that the optical flow would occassionally bug out.

We'll immediately switch to using time + speed to measure distance.
Less time faffing. The robot moves one unit each second that it is mobile.
Getting this precise will be tricky, pay attention to where the timers
are started/stopped

On finishing:
Distance computation has been fixed. One unit per second, though for some
very short intervals no time can be recorded (this could be fixed by
allocating 10 units per second perhaps).

Optical flow playing up on final test and generally being a pain.
This may have been to do with the large objects I was using to try and induce
CA responses; I think it more likely that it just isn't working.

Todo:
- Test the older PI threads; make sure the existing CX actually works
  as described in Zhaoyu and Luca's papers (maybe alter the speeds).
- Figure out how to make the CXtheta value useful. Currently it is
  4 or 5 or zero. This is useless considering the robot doesn't turn to
	reset its orientation to be facing homeward. Make sure you're using
	this in the same way that Scimeca and Zhang did. CXtheta is meant
	to be in degrees so it should be directly passable to the turnAround
	function.
- Robot needs charged. 
